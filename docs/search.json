[
  {
    "objectID": "ComplexSurveyNotes.html",
    "href": "ComplexSurveyNotes.html",
    "title": "Complex Survey Notes",
    "section": "",
    "text": "What follows is a coded work through of Thomas Lumley’s “Complex Surveys: A Guide to Analysis in R” (Lumley 2011) and some of the methods underlying design based statistics more broadly. This write-up reflects my understanding of the material with additions made to try and clarify ideas further. These include simulations, derivations or references that I found helpful in working through Lumley’s material. Most of the data sets that I use throughout these notes are from Lumley’s website for the book. Some data sets are not available, or at least I was not able to find them, or the the documentation for the data was not available. I remark on this when relevant in my notes below.\n\nI’d imagine there are two general ways one might use these notes:\n\nAs a quick reference on any of the code or topics.\nIf you are working through the book, you may find these notes useful to double check Lumley’s code or your understanding of the principles tested in the exercises.\n\n\nNote that Lumley’s code was written at the time of publishing and much has changed, both in the survey package and R more generally. I try to keep this as close to his original code as possible while updating when necessary. I also often show how to produce the same code using the srvyr package, but this is less relevant for the later chapters.\n\nI would not always use the same method if I was writing this kind of code but I try to replicate his to make it easy for any reader with the book to follow along.\n\n\nMy answers to the exercises are not guaranteed to be correct. I would strongly encourage you to avoid copying them if you are, for example, working through this textbook for a class.\n\nThese notes are no substitute for buying the book itself. I would encourage you to buy the book if you’re intent on working through these notes.\n\nIt wasn’t very long into reading this book that I found that weighted sampling in R is, unfortunately, not well set-up for complex designs. It would not be possible, for example, to simply use the base R sample or popular tidyverse package dplyr’s function slice_sample() to draw a weighted sample from a population for example with appropriate inclusion probabilities.Further details are in this stats exchange post.\nInstead a function from the sampling package would have to be used. I use this function below in any setting where a non-uniform sample with inclusion probabilities is needed.\nIts worth further pointing out that the topic of how samples themselves are drawn is a complicated one its own right and that the functions in the sampling package each have pros and cons according to the target estimand or question of interest. Drawing samples with replicate weights – discussed in Chapter 2 – is a similarly complex question which I haven’t yet resolved to my satisfaction."
  },
  {
    "objectID": "ComplexSurveyNotes.html#overview",
    "href": "ComplexSurveyNotes.html#overview",
    "title": "Complex Survey Notes",
    "section": "",
    "text": "What follows is a coded work through of Thomas Lumley’s “Complex Surveys: A Guide to Analysis in R” (Lumley 2011) and some of the methods underlying design based statistics more broadly. This write-up reflects my understanding of the material with additions made to try and clarify ideas further. These include simulations, derivations or references that I found helpful in working through Lumley’s material. Most of the data sets that I use throughout these notes are from Lumley’s website for the book. Some data sets are not available, or at least I was not able to find them, or the the documentation for the data was not available. I remark on this when relevant in my notes below.\n\nI’d imagine there are two general ways one might use these notes:\n\nAs a quick reference on any of the code or topics.\nIf you are working through the book, you may find these notes useful to double check Lumley’s code or your understanding of the principles tested in the exercises.\n\n\nNote that Lumley’s code was written at the time of publishing and much has changed, both in the survey package and R more generally. I try to keep this as close to his original code as possible while updating when necessary. I also often show how to produce the same code using the srvyr package, but this is less relevant for the later chapters.\n\nI would not always use the same method if I was writing this kind of code but I try to replicate his to make it easy for any reader with the book to follow along.\n\n\nMy answers to the exercises are not guaranteed to be correct. I would strongly encourage you to avoid copying them if you are, for example, working through this textbook for a class.\n\nThese notes are no substitute for buying the book itself. I would encourage you to buy the book if you’re intent on working through these notes."
  },
  {
    "objectID": "ComplexSurveyNotes.html#drawing-samples-in-r",
    "href": "ComplexSurveyNotes.html#drawing-samples-in-r",
    "title": "Complex Survey Notes",
    "section": "",
    "text": "It wasn’t very long into reading this book that I found that weighted sampling in R is, unfortunately, not well set-up for complex designs. It would not be possible, for example, to simply use the base R sample or popular tidyverse package dplyr’s function slice_sample() to draw a weighted sample from a population for example with appropriate inclusion probabilities.Further details are in this stats exchange post.\nInstead a function from the sampling package would have to be used. I use this function below in any setting where a non-uniform sample with inclusion probabilities is needed.\nIts worth further pointing out that the topic of how samples themselves are drawn is a complicated one its own right and that the functions in the sampling package each have pros and cons according to the target estimand or question of interest. Drawing samples with replicate weights – discussed in Chapter 2 – is a similarly complex question which I haven’t yet resolved to my satisfaction."
  },
  {
    "objectID": "ComplexSurveyNotes.html#design-vs.-model",
    "href": "ComplexSurveyNotes.html#design-vs.-model",
    "title": "Complex Survey Notes",
    "section": "Design vs. Model",
    "text": "Design vs. Model\nThis book focuses on “Design-based” Inference. That is the methods in this book focus on the design from which the data are constructed, rather than the data itself. In a traditional survey setting the data are assumed to be fixed and the probabilities of sampling different entities are used to derive the desired estimate. These inclusion probabilities or their inverse, “sampling weights”, are used to re-balance the data so that they more accurately reflect the target population distribution. Different sampling techniques — clustering, 2-phase, etc. — are used to either decrease the variance of the resulting estimate, the cost associated with the design or both.\nHorvitz Thompson Estimation\nThe Horvitz Thompson Estimator (HTE) is the starting point for non-uniform random estimates. If we observe measure X_i on subject i drawn with probability \\pi_i from a population of N total subjects the HTE is formulated as follows:\n\nHTE(X) = \\sum_{i=1}^{N} \\frac{1}{\\pi_i}X_i, \\tag{1.1}\n\nwhich is an unbiased estimator as shown in the Chapter 1 Appendix.\nThe variance of this estimate is:\n\nV[HTE(X)] = \\sum_{i,j} \\left ( \\frac{X_i X_j}{\\pi_{ij}} -\n\\frac{X_i}{\\pi_i} \\frac{X_j}{\\pi_j} \\right ), \\tag{1.2}\n\nwhich follows from the Bernoulli covariance using indicator variables R_i=1 if individual i is in the sample, R_i=0 otherwise. A proof is provided in the [Questions From Chapter 1].\nDesign And Misspecification Effects\n(Kish 1965) defined the notion of a design effect as the ratio of a variance of an estimate in a complex sample to the variance of the same estimate in a simple random sample (SRS). The motivation for this entity being that it can guide researchers in terms of how much sample size they may need; If the sample size for a given level of precision is known for a simple random sample, the sample size for a complex design can be obtained by multiplying by the design effect.\nWhile larger sample sizes may be necessary to maintain the same level of variance as a SRS, the more complex may still be more justified because of the lower cost associated. See (Meng 2018) for an example of where design effects are used in a modern statistical setting by comparing competing estimators.\nOther preliminary items\nFrom this point Lumley works through an introduction to the datasets used in the book and the idea that we’ll often be taking samples from datasets where we know the “true” population and computing estimates from there. This isn’t always the case and there may some subtlety worth discussing how to interpret results once we get into topics like regression, but for the most part his description makes sense.\nOne thing I found lacking in this introductory section is the motivation for why we might take non-uniform samples. It isn’t until Chapter 3 that Lumley discusses probability proportional to size (PPS) sampling, but this is very often the reason why a non-uniform sample is used.\nIf we have some measure that is right skewed in our population of interest and we’d like to estimate the mean, we could take a SRS to estimate the mean but the variance on that item would be lower than if we sampled proportional to the right skew measure itself. I’ll demonstrate with the following quick example, suppose we want to measure the income of a population. Incomes are often right skewed, but we can get a lower variance estimate if we take a weighted sample.\nI generate a right skewed population and visualize the distribution.\n\nShow the codepopulation &lt;- tibble(\n  id = 1:1E3,\n  income = 5E4 * rlnorm(1E3, meanlog = 0, sdlog = 0.5)\n)\npopulation %&gt;%\n  ggplot(aes(x = income)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = mean(income)), linetype = 2, color = \"red\") +\n  ggtitle(\"Simulated Income Distribution\") +\n  xlab(\"Income (USD)\")\n\n\n\n\nHere I’ll take a uniform and weighted sample of size 50. Note that the differences in the samples are subtle. They might not look all that different on visual inspection.\n\nShow the codeuniform_sample &lt;- population %&gt;%\n  slice_sample(n = 50) %&gt;%\n  transmute(\n    income = income,\n    method = \"uniform\",\n    pi = 50 / 1E3\n  )\n\nweighted_sample &lt;- population %&gt;%\n  mutate(\n    pi = sampling::inclusionprobabilities(floor(population$income), 50),\n    in_weighted_sample = sampling::UPbrewer(pi) == 1\n  ) %&gt;%\n  filter(in_weighted_sample) %&gt;%\n  transmute(\n    income = income,\n    pi = pi,\n    method = \"weighted\"\n  )\n\nrbind(\n  uniform_sample,\n  weighted_sample\n) %&gt;%\n  ggplot(aes(x = income, fill = method)) +\n  geom_histogram() +\n  ggtitle(\"Sample Comparisons\") +\n  xlab(\"Income (USD)\") +\n  theme(legend.title = element_blank(), legend.position = \"top\")\n\n\n\n\nFinally I’ll estimate the population mean from both samples and include the design effect calculation in the weighted sample estimate.\n\nShow the codeuniform_sample %&gt;%\n  as_survey_design() %&gt;%\n  summarize(\n    mean_income = survey_mean(income)\n  )\n\n# A tibble: 1 × 2\n  mean_income mean_income_se\n        &lt;dbl&gt;          &lt;dbl&gt;\n1      50661.          3734.\n\n\n\nShow the codeweighted_sample %&gt;%\n  as_survey_design(probs = pi) %&gt;%\n  summarize(mean_income = survey_mean(income, deff = TRUE))\n\n# A tibble: 1 × 3\n  mean_income mean_income_se mean_income_deff\n        &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1      59001.          4153.            0.882\n\n\nWe see that the weighted estimate standard error is not quite half the uniform estimate. Accordingly the design effect for the weighted sample is less than 1."
  },
  {
    "objectID": "ComplexSurveyNotes.html#exercises",
    "href": "ComplexSurveyNotes.html#exercises",
    "title": "Complex Survey Notes",
    "section": "Exercises",
    "text": "Exercises\n\nDoesn’t make sense to reproduce here.\nDon’t make sense to reproduce here.\nEach visit to the front page of a newspaper’s website has (independently) a 1/1000 chance of resulting in a questionnaire on voting intentions in a forthcoming election. Assuming that everyone who is given the questionnaire responds, why are the results not a probability sample of:\n\n\nVoters?\nReaders of the newspaper?\nReaders of the newspaper’s online version?\n\nLumley lists 4 properties needed for a sample to be considered a probability sample.\n\n\nEvery individual (unit of analysis) in the population must have a non-zero probability of ending up in the sample (\\pi_i&gt;0 \\forall i)\n\n\n\n\\pi_i must be known for every individual who does end up in the sample.\n\n\nEvery pair of individuals in the sample must have a non-zero probability of both ending up in the sample (\\pi_{i,j} \\forall i, j)\n\n\nThe probability \\pi_{i,j} must be known for every pair that does end up in the sample.\n\n\n\nis not guaranteed when considering voters — there are voters who don’t read the paper who have will have \\pi_i = 0 — or the broader heading of “readers” of the newspaper - since those who only read the physical paper will have a $_i = 0 $. For “readers of the newspaper’s online version” the sample would only be a probability sample if the time window was further specified, as there could be online readers who do not visit during the survey window, and would thus be assigned a \\pi_i=0.\n\n\nYou are conducting a survey that will estimate the proportion of women who used anti-malarial insecticide-treated bed nets every night during their last pregnancy. With a simple random sample you would need to recruit 50 women in any sub-population where you wanted a standard error of less than 5 percentage points in the estimate. You are using a sampling design that has given design effects of 2-3 for proportions in previous studies in similar areas.\n\n\nWill you need a larger or smaller sample size than 50 for a sub-population to get the desired precision?\n\nLarger, a design effect &gt;1 indicates that the variance is larger in the complex design with the same sample size - consequently the sample size will need to be increased to maintain the same level of precision.\n\nApproximately what sample size will you need to get the desired precision?\n\n100 - 150. Derived from multiplying 50 by 2 and 3.\n\nSystematic sampling involves taking a list of the population and choosing, for example, every 100th entry in the list.\n\n\nWhich of the necessary properties of a probability sample does this have?\n\nItems ii-iv from the list enumerated above. The only condition that is not satisfied is that not every item has a nonzero probability of being chosen.\n\nFor systematic sampling with a random start, the procedure would be to choose a random starting point from 1, 2, …, 1000 and then take every 100th entry starting at the random point. Which of the necessary properties of a probability sample does this procedure have?\n\nThis satisfies all items from the above list.\n\nFor systematic sampling with multiple random starts we might choose 5 random starting points in 1, 2, ….., 5000 and then take every 500th entry starting from each of the 5 random points. Which of the necessary properties of a probability sample does this procedure have?\n\nAgain, this satisfies all items from the above list.\n\nIf the list were shuffled into a random order before a systematic sample was taken, which of the properties would the procedure have.\n\nAgain, all of them. The key is adding the known randomness and not excluding any items from selection.\n\nTreating a systematic sample as if it were a simple random sample often gives good results. Why would this be true?\n\nThis would be because the items are not ordered in any particular fashion prior to taking the “systematic sampling”. In this setting a systematic sample is equivalent to a simple random sample.\n\nWhy must all the sampling probabilities be non-zero to get a valid population estimate?\n\nIf any of the sampling probabilities are zero, that would introduce bias in shifting the estimate away from the portion of the population that would always be unobserved under repeated sampling.\n\nWhy must all the pairwise probabilities be non-zero to get a valid uncertainty estimate.\n\nThis is basically a second order statement equivalent to the previous. If any pair is unable to be observed together that is a form of selection bias that would shift the sample estimate away from the true population value.\n\nA probability design assumes that people who are sampled will actually be included in the sample, rather than refusing. Look up the response rates for the most recent year of BRFSS and NHANES.\n\nLumley is highlighting the fact that even though we set up samples thinking that every sample will be observed that is rarely the case. Looking at just the most recent NHANES data I see response rates at ~ 78% for the un-weighted, 6-year household survey.\n\nIn a telephone study using random digit dialing, telephone numbers are sampled with equal probability from a list. When a household is recruited, why is it necessary to ask how many telephones are in the household, and what should be done with this information in computing the weights.\n\nIt is necessary to ask how many telephones are in the household to down weight the a priori sampling probability accordingly because every additional telephone line increases the odds that a given house is sampled. For example in a simple population with two houses, where house one has 5 telephones and house two has 2 telephones, and we’re looking to take a n=1 sample, but we don’t know the number of telephones a priori, house one has a \\frac{5}{7} probability of being sampled. If that is the house that is chosen its weight needs to go from 2 to \\frac{5}{7} to better reflect its sampling probability. In a real sample this would be corrected relative to all the other households number of telephones or perhaps a population average of the number of telephones.\n\n\n\nDerive the Horvitz Thompson variance estimator for the total as follows.\n\nWrite R_i = 1 if individual i is in the sample, R_i=0 otherwise. Show that V[R_i] = \\pi_i(1-\\pi_i) and that Cov[R_i,R_j]=\\pi_{ij} - \\pi_i\\pi_j.\n\nThis follows in a straightforward fashion from the assumption that R_i is distributed according to the Bernoulli distribution and R_i \\perp R_j. This is an accurate model for sampling with replacement, or sampling from large populations with small sample sizes without replacement, but less true for small sample sizes without replacement.\n\nShow that the variance of the Horvitz Thompson estimator is:\n\n\nV[\\hat{T}_{HT}] = \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\check{x}_i\\check{x}_j(\\pi_{ij} - \\pi_i \\pi_j)\n We have, \n\\hat{T}_{HT} := \\sum_{i=1}^{N} \\frac{X_i I(X_i \\in {S})}{\\pi_i} \\\\\nV[\\hat{T}_{HT}] = V\\left[\\sum_{i=1}^{N}\\frac{X_i I(X_i \\in {S})}{\\pi_i} \\right] \\\\\n=\\sum_{i=1}^{N}\\sum_{j=1}^{N} Cov\\left[\\frac{X_i I(i \\in {S})}{\\pi_i},\n\\frac{X_j I(j \\in {S})}{\\pi_j}\\right]\\\\\n= \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\frac{X_i}{\\pi_i}\\frac{X_j}{\\pi_j}Cov(I(i \\in\n{S}),I(j \\in {S})) \\\\\n= \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\frac{X_i}{\\pi_i}\\frac{X_j}{\\pi_j} (\\pi_{ij} -\n\\pi_i \\pi_j)\n\nwhich is equivalent to the above, where \\check{x_i} = \\frac{X_i}{\\pi_i}.\n\nShow that an unbiased estimator of the variance is \n\\hat{V}[\\hat{T}_{HT}] = \\sum_{i=1}^{N}\\sum_{j=1}^{N}\n\\frac{R_i R_j}{\\pi_{ij}}\\check{x_i}\\check{x_j}(\\pi_{ij} - \\pi_i \\pi_j)\n\n\n\nTo show the expression above is unbiased for \\hat{V}[\\hat{T}_{HT}] we must show that E\\left [\\hat{V}[\\hat{T}_{HT}] \\right] = V[\\hat{T}_{HT}] \nE \\left [\\hat{V}[\\hat{T}_{HT} ] \\right] = E \\left[\\sum_{i=1}^{N}\\sum_{j=1}^{N} \\frac{R_iR_j}{\\pi_{ij}} \\check{x}_i \\check{x}_j(\\pi_{ij} - \\pi_i\\pi_j) \\right] \\\\\n= \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\frac{E[R_iR_j]}{\\pi_{ij}} \\check{x}_i \\check{x}_j (\\pi_{ij} - \\pi_i \\pi_j) \\\\\n= \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\frac{\\pi_{ij}}{\\pi_{ij}} \\check{x}_i\\check{x}_j(\\pi_{ij} - \\pi_i \\pi_j)  \\\\\n= \\sum_{i=1}^{N} \\sum_{j=1}^{N}  \\check{x}_i\\check{x}_j(\\pi_{ij} - \\pi_i \\pi_j)  \\\\\n\\blacksquare\n\n\nShow that the previous expression simplifies to equation 1.2\n\n\n\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\frac{R_iR_jx_ix_j}{\\pi_{ij}\\pi_i\\pi_j}(\\pi_{ij} - \\pi_i \\pi_j) \\\\\n=\n\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\frac{x_ix_j}{\\pi_{ij}\\pi_i\\pi_j}(\\pi_{ij} - \\pi_i \\pi_j) \\\\\n=\n\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i x_j(\\frac{1}{\\pi_i \\pi_j} - \\frac{1}{\\pi_{ij}}) \\\\\n=\n\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\frac{x_ix_j}{\\pi_i \\pi_j}  - \\frac{x_i x_j}{\\pi_{ij}}\\\n\nI’m not sure how the signs switch on the last line to reproduce expression 1.2\n\nAnother popular way to write the Horvitz-Thompson variance estimator is\n\n\n\\hat{V}[\\hat{T}_{HT}] = \\sum_{i=1}^{n} x_i^{2} \\frac{1-\\pi_i}{\\pi_i^2} +\n\\sum_{i\\neq j}x_ix_j\\frac{\\pi_{ij} - \\pi_i \\pi_j}{\\pi_i\\pi_j\\pi_{ij}}\n\nShow that this is equivalent to equation 1.2\nWe need to show that the above is equivalent to \n\\sum_{i,j} \\frac{X_iX_j}{\\pi_{ij}} - \\frac{X_i}{\\pi_i}\\frac{X_j}{\\pi_j}\n\nFirst we fix i \\neq j in the above expression and we find \n\\sum_{i\\neq j} \\frac{X_iX_j}{\\pi_{ij}} - \\frac{X_i}{\\pi_i}\\frac{X_j}{\\pi_j} =\n\\sum_{i\\neq j} X_iX_j(\\frac{1}{\\pi_{ij}} - \\frac{1}{\\pi_i} \\frac{1}{\\pi_j}) \\\\\n= \\sum_{i \\neq j} X_iX_j(\\frac{\\pi_i \\pi_j - \\pi_{ij}}{\\pi_{ij}\\pi_i\\pi_j})\n\nwhich is the latter part in the desired expression save for a sign, which again I must be missing somehow or is an error in the book.\nNow we take i=j and return to expression 1.2 in which we have,\n\n\\sum_{i=j} \\frac{X_i X_j}{\\pi_{ij}} - \\frac{X_i}{\\pi_i} \\frac{X_j}{\\pi_j} \\\\\n= \\sum_{i=j} \\frac{X_i^2}{\\pi_{ii}} - \\frac{X_i}{\\pi_i} \\frac{X_i}{\\pi_i} \\\\\n= \\sum_{i=j} X_i^2 (\\frac{1}{\\pi_{ii}} - \\frac{1}{\\pi_i^2} ) \\\\\n= \\sum_{i=j} X_i^2 (\\frac{\\pi_i^2 - \\pi_{ii}}{\\pi_i^2 \\pi_{ii}})\n Clearly we have to formulate \\pi_{ii} in terms of \\pi_i but isn’t immediately clear to me how to do so. We know that for the two terms to be equal we must have\n\n\\frac{\\pi_i^2 - \\pi_{ii}}{\\pi_i^2 \\pi_{ii}} = \\frac{1 - \\pi_i^2}{\\pi_i^2}  \\\\\n\\iff \\\\\n\\pi_{ii} = \\frac{\\pi_i^2}{2- \\pi_i^2}\n Which I suppose we’ll take to be the expression of a co-inclusion probability of an entity sampled with itself (this must assume sampling with replacement) for this expression to be true."
  },
  {
    "objectID": "ComplexSurveyNotes.html#chapter-1-appendix",
    "href": "ComplexSurveyNotes.html#chapter-1-appendix",
    "title": "Complex Survey Notes",
    "section": "Chapter 1 Appendix",
    "text": "Chapter 1 Appendix\nThe HTE is an unbiased estimator of the population total - I reproduce the expression from above, but now make explicit the indicator variables that express which observations are included in our sample, S.\n\nHTE := \\sum_{i=1}^{N} \\frac{X_i I(X_i \\in S)}{\\pi_i} \\\\\nE[HTE] = E\\left [\\sum_n \\frac{X_i I(X_i \\in S)}{\\pi_i} \\right ] \\\\\n= \\sum_n E \\left [\\frac{X_iI(X_i \\in S)}{\\pi_i} \\right ] \\\\\n= \\sum_n \\frac{X_iE[I(X_i \\in S)]}{\\pi_i} \\\\\n= \\sum_n \\frac{X_i \\pi_i}{\\pi_i} = \\sum_n X_i"
  },
  {
    "objectID": "ComplexSurveyNotes.html#starting-from-simple-random-samples",
    "href": "ComplexSurveyNotes.html#starting-from-simple-random-samples",
    "title": "Complex Survey Notes",
    "section": "Starting from Simple Random Samples",
    "text": "Starting from Simple Random Samples\nWhen dealing with a sample of size n from a population of size N the HTE of the total value of X_i in the population can be written as\n\n\\begin{equation}\nHTE(X) = \\hat{T_X} =  \\sum_{i=1}^{n} \\frac{X_i}{\\pi_i}.\n\\end{equation}\n\nFor a simple random sample, the variance can be more explicitly written as\n\n\\begin{equation}\nV[\\hat{T_X}] = \\frac{N-n}{N} \\times N^{2} \\times \\frac{V[X]}{n},\n\\end{equation}\n\nwhere \\frac{N-n}{N} is the finite population correction factor. This factor is derived from the hypergeometric distribution and explains the reduction in uncertainty that follows from sampling a large portion of the population. Consequently, if the sample is taken with replacement — the same individual or unit has the possibility to be sampled twice — this term is no longer relevant. It should be noted that sampling with replacement is not usually used however, but sometimes this language is used to refer to the fact that the finite correction factor may not be used.\nThe second term, N^2, rescales the estimate from the mean to the total, while the final term is simply the scaled variance of X.\nA point worth deliberating on, that Lumley notes as well, is that while the above equations suggest that a larger sample size is always better that is not always the case in reality. Non-response bias or the cost of surveys can dramatically diminish the quality of the dataset, even if the size is large. I state this is worth deliberating on because it is a matter of increasing importance in the world of “Big Data” - where it can be easy to delude oneself with confidence in their estimates because their sample is large, even when the sample is not well designed. See (Meng 2018) for a larger discussion of this topic.\nIt follows from the above that the HTE for the population size is defined as \\hat{N} = \\sum_{i=1}^{n} \\frac{1}{\\pi_i}. This holds true in the case where, as here \\pi_i = \\frac{n}{N}, a bit trivial, but also in those where \\pi_i may be defined differently."
  },
  {
    "objectID": "ComplexSurveyNotes.html#confidence-intervals",
    "href": "ComplexSurveyNotes.html#confidence-intervals",
    "title": "Complex Survey Notes",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nThe sampling distribution for the estimates — typically sample means and sums — across “repeated surveys” is Normal by the Central Limit Theorem, so the typical \\bar{x} \\pm 1.96 \\sqrt{\\frac{\\sigma^2_X}{n}}, expression is used to calculate a 95% confidence interval. Lumley offers the following example from the California Academic Performance Index (API) dataset to illustrate this idea.\n\nShow the codedata(api)\nmn_enroll &lt;- mean(apipop$enroll, na.rm = TRUE)\np1 &lt;- apipop %&gt;%\n  ggplot(aes(x = enroll)) +\n  geom_histogram() +\n  xlab(\"Student Enrollment\") +\n  geom_vline(xintercept = mn_enroll, linetype = 2, color = \"red\") +\n  ggtitle(\"Distribution of School Enrollment\")\np2 &lt;- replicate(n = 1000, {\n  apipop %&gt;%\n    sample_n(200) %&gt;%\n    pull(enroll) %&gt;%\n    mean(., na.rm = TRUE)\n})\nmn_sample_mn &lt;- mean(p2)\np2 &lt;- tibble(sample_ix = 1:1000, sample_mean = p2) %&gt;%\n  ggplot(aes(x = sample_mean)) +\n  geom_histogram() +\n  xlab(\"Student Enrollment Averages\") +\n  geom_vline(\n    xintercept = mn_sample_mn,\n    linetype = 2, color = \"red\"\n  ) +\n  ggtitle(\"Distribution of Sample Means\")\np1 + p2"
  },
  {
    "objectID": "ComplexSurveyNotes.html#complex-sample-data-in-r",
    "href": "ComplexSurveyNotes.html#complex-sample-data-in-r",
    "title": "Complex Survey Notes",
    "section": "Complex Sample Data in R",
    "text": "Complex Sample Data in R\nWhat follows is a work-up of basic survey estimates using the California API data set composed of student standardized test scores. I’ll work through the code once using the survey package and a second time using the srvyr package, which has a tidyverse friendly API. These are all demonstrated alongside Lumley’s commentary in the book and I’d encourage you to read the section for more details on this part.\nMuch of the computational work in this book begins with creating a design object, from which weights and other information can then be drawn on for any number/type of estimates.\nFor example, we create a basic design object below, where we look at a classic simple random sample (SRS) of the schools in the API dataset. Let’s take a look at the dataset first.\n\nShow the codedplyr::as_tibble(apisrs)\n\n# A tibble: 200 × 39\n   cds       stype name  sname  snum dname  dnum cname  cnum  flag pcttest api00\n   &lt;chr&gt;     &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n 1 15739081… H     \"McF… McFa…  1039 McFa…   432 Kern     14    NA      98   462\n 2 19642126… E     \"Sto… Stow…  1124 ABC …     1 Los …    18    NA     100   878\n 3 30664493… H     \"Bre… Brea…  2868 Brea…    79 Oran…    29    NA      98   734\n 4 19644516… E     \"Ala… Alam…  1273 Down…   187 Los …    18    NA      99   772\n 5 40688096… E     \"Sun… Sunn…  4926 San …   640 San …    39    NA      99   739\n 6 19734456… E     \"Los… Los …  2463 Haci…   284 Los …    18    NA      93   835\n 7 19647336… M     \"Nor… Nort…  2031 Los …   401 Los …    18    NA      98   456\n 8 19647336… E     \"Gla… Glas…  1736 Los …   401 Los …    18    NA      99   506\n 9 19648166… E     \"Max… Maxs…  2142 Moun…   470 Los …    18    NA     100   543\n10 38684786… E     \"Tre… Trea…  4754 San …   632 San …    37    NA      90   649\n# ℹ 190 more rows\n# ℹ 27 more variables: api99 &lt;int&gt;, target &lt;int&gt;, growth &lt;int&gt;, sch.wide &lt;fct&gt;,\n#   comp.imp &lt;fct&gt;, both &lt;fct&gt;, awards &lt;fct&gt;, meals &lt;int&gt;, ell &lt;int&gt;,\n#   yr.rnd &lt;fct&gt;, mobility &lt;int&gt;, acs.k3 &lt;int&gt;, acs.46 &lt;int&gt;, acs.core &lt;int&gt;,\n#   pct.resp &lt;int&gt;, not.hsg &lt;int&gt;, hsg &lt;int&gt;, some.col &lt;int&gt;, col.grad &lt;int&gt;,\n#   grad.sch &lt;int&gt;, avg.ed &lt;dbl&gt;, full &lt;int&gt;, emer &lt;int&gt;, enroll &lt;int&gt;,\n#   api.stu &lt;int&gt;, pw &lt;dbl&gt;, fpc &lt;dbl&gt;\n\n\nIn the code below fpc stands for the aforementioned finite population correction factor and id=~1 designates the unit of analysis as each individual row in the dataset.\n\nShow the codesrs_design &lt;- svydesign(id = ~1, fpc = ~fpc, data = apisrs)\nsrs_design\n\nIndependent Sampling design\nsvydesign(id = ~1, fpc = ~fpc, data = apisrs)\n\n\nIn order to calculate the mean enrollment based on this sample the, appropriately named, svymean function can be used.\n\nShow the codesvymean(~enroll, srs_design)\n\n         mean     SE\nenroll 584.61 27.368\n\n\nThis is the same as the typical computation - which makes sense, this is a SRS!\n\nShow the codec(\n  \"Mean\" = mean(apisrs$enroll),\n  \"SE\" = sqrt(var(apisrs$enroll) / nrow(apisrs))\n)\n\n     Mean        SE \n584.61000  27.82121 \n\n\nInstead of specifying the finite population correction factor, the sampling weights could be used - since this is a SRS, all the weights should be the same.\n\nShow the codeas_tibble(apisrs) %&gt;% distinct(pw)\n\n# A tibble: 1 × 1\n     pw\n  &lt;dbl&gt;\n1  31.0\n\n\n\nShow the codenofpc &lt;- svydesign(id = ~1, weights = ~pw, data = apisrs)\nnofpc\n\nIndependent Sampling design (with replacement)\nsvydesign(id = ~1, weights = ~pw, data = apisrs)\n\n\nUse svytotal to calculate the estimate of the total across all schools, note that the standard error will be different between the two designs because of the lack of fpc.\n\nShow the codesvytotal(~enroll, nofpc)\n\n         total     SE\nenroll 3621074 172325\n\n\n\nShow the codesvytotal(~enroll, srs_design)\n\n         total     SE\nenroll 3621074 169520\n\n\nTotals across groups can be calculated using the ~ notation with a categorical variable.\n\nShow the codesvytotal(~stype, srs_design)\n\n         total     SE\nstypeE 4397.74 196.00\nstypeH  774.25 142.85\nstypeM 1022.01 160.33\n\n\nsvycontrast can be used to calculate the difference or addition of two different estimates - below we estimate the difference in the 2000 and 1999 scores based on the SRS design.\n\nShow the codesvycontrast(svymean(~ api00 + api99, srs_design), quote(api00 - api99))\n\n         nlcon     SE\ncontrast  31.9 2.0905\n\n\nNow again with the srvyr package\n\nShow the codedstrata &lt;- apisrs %&gt;%\n  as_survey_design(fpc = fpc)\ndstrata %&gt;%\n  mutate(api_diff = api00 - api99) %&gt;%\n  summarise(\n    enroll_total = survey_total(enroll, vartype = \"ci\"),\n    api_diff = survey_mean(api_diff, vartype = \"ci\")\n  ) %&gt;%\n  gt()\n\n\n\n\n\n\nenroll_total\n      enroll_total_low\n      enroll_total_upp\n      api_diff\n      api_diff_low\n      api_diff_upp\n    \n\n3621074\n3286789\n3955360\n31.9\n27.77764\n36.02236"
  },
  {
    "objectID": "ComplexSurveyNotes.html#stratified-sampling",
    "href": "ComplexSurveyNotes.html#stratified-sampling",
    "title": "Complex Survey Notes",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\nSimple random samples are not often used in complex surveys because there is a justified concern that some strata (e.g. racial ethnic group, age group, etc.) may be underrepresented in the sample if a simple random sample were used. Similarly, complex designs can give the same precision at a lower cost. Consequently, a sample may be constructed so that some units are guaranteed to be included within a given strata - improving the resulting variance. When this is a simple random sample, the HTE and variance of the total population is simply the sum of the strata specific estimates; \\hat{T}_{HT} = \\sum_{s=1}^{S} \\hat{T}^{s}_X, where there are S strata within the population.\nFor example, in the apistrat data set a stratified random sample of 200 schools is recorded such that schools are sampled randomly within school type ( elementary, middle school or high school).\nIn the code below we can designate the strata using the categorical variable stype, which denotes each of the school type as strata.\n\nShow the codestrat_design &lt;- svydesign(\n  id = ~1,\n  strata = ~stype,\n  fpc = ~fpc,\n  data = apistrat\n)\nstrat_design\n\nStratified Independent Sampling design\nsvydesign(id = ~1, strata = ~stype, fpc = ~fpc, data = apistrat)\n\n\n\nShow the codesvytotal(~enroll, strat_design)\n\n         total     SE\nenroll 3687178 114642\n\n\n\nShow the codesvymean(~enroll, strat_design)\n\n         mean     SE\nenroll 595.28 18.509\n\n\n\nShow the codesvytotal(~stype, strat_design)\n\n       total SE\nstypeE  4421  0\nstypeH   755  0\nstypeM  1018  0\n\n\nNote that are standard errors are 0 for the within strata estimates because if we have strata information on each member of the population, then we know the strata counts without any uncertainty.\nSeveral points worth noting about stratified samples before moving on.\n\nStratified samples get their power from “conditioning” on the strata information that explain some of the variability in the measure — analogous to regression estimators improved variance when conditioning on informative covariates.\nWhereas a SRS might have a chance of leaving out an elementary or middle school, and leaving a higher estimate of enrollment, because of a higher number of high schools in the sample, keeping a fixed number of samples within each strata removes this problem.\nStratified analysis may also refer to something entirely different from what we’re discussing here — where a subgroup has some model or estimate fit only on that subgroup’s data exclusively.\n\nNow again with the srvyr package\n\nShow the codesrvyr_strat_design &lt;- apistrat %&gt;%\n  as_survey_design(\n    strata = stype,\n    fpc = fpc\n  )\nsrvyr_strat_design\n\nStratified Independent Sampling design\nCalled via srvyr\nSampling variables:\n  - ids: `1` \n  - strata: stype \n  - fpc: fpc \nData variables: \n  - cds (chr), stype (fct), name (chr), sname (chr), snum (dbl), dname (chr),\n    dnum (int), cname (chr), cnum (int), flag (int), pcttest (int), api00\n    (int), api99 (int), target (int), growth (int), sch.wide (fct), comp.imp\n    (fct), both (fct), awards (fct), meals (int), ell (int), yr.rnd (fct),\n    mobility (int), acs.k3 (int), acs.46 (int), acs.core (int), pct.resp (int),\n    not.hsg (int), hsg (int), some.col (int), col.grad (int), grad.sch (int),\n    avg.ed (dbl), full (int), emer (int), enroll (int), api.stu (int), pw\n    (dbl), fpc (dbl)\n\n\n\nShow the codesrvyr_strat_design %&gt;%\n  summarise(\n    enroll_total = survey_total(enroll),\n    enroll_mean = survey_mean(enroll)\n  ) %&gt;%\n  gt()\n\n\n\n\n\n\nenroll_total\n      enroll_total_se\n      enroll_mean\n      enroll_mean_se\n    \n\n3687178\n114641.7\n595.2821\n18.50851\n\n\n\n\n\n\nShow the codesrvyr_strat_design %&gt;%\n  group_by(stype) %&gt;%\n  survey_count()\n\n# A tibble: 3 × 3\n# Groups:   stype [3]\n  stype     n  n_se\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 E     4421.     0\n2 H      755.     0\n3 M     1018.     0"
  },
  {
    "objectID": "ComplexSurveyNotes.html#replicate-weights",
    "href": "ComplexSurveyNotes.html#replicate-weights",
    "title": "Complex Survey Notes",
    "section": "Replicate Weights",
    "text": "Replicate Weights\nReplicate weights exploit sub-sampling to derive more generalizable statistics than sampling weights. This is particularly useful when estimating a “nonparametric” statistic like the median or a quantile which doesn’t have an easily derived variance.\nFor a basic idea of why this works, Lumley notes that one could estimate the variance of a total by using two independent half samples estimating the same total, i.e. if \\hat{T}_A and \\hat{T}_B are both from two independent half samples estimating \\hat{T} then the variance of the difference of the two half samples is proportional to the variance of the original total:\n\nE\\left[ (\\hat{T}_A - \\hat{T}_B)^2 \\right] = 2 V[\\hat{T}_A] = 4 V[\\hat{T}].\n\nThere are multiple ways one might set up these splits that are more efficient than the straightforward “half” sample described above - Lumley discusses 3 variants briefly:\n\n\nBalanced Repeated Replication (BRR) Based on the work of (McCarthy 1966).\n\n\n\n(Judkins 1990), extends BRR to handle issues with sparse signals and small samples.\n\n\nJackknife\n\n\nBecause BRR and Fay’s method is difficult with other designs using overlapping subsamples, Jackknife and the bootstrap are intended to be more flexible.\n\n\nBootstrap\n\n\nThis is the method I’m most familiar with, outside of complex designs.\nLumley states that using the Bootstrap in this setting involves taking a sample (with replacement) of observations or clusters and multiplying the sampling weight by the number of times the observation appears in the sample.\n\nEach of these ideas relies on the fundamental idea that we can calculate the variance of our statistic of interest by using — sometimes carefully chosen — subsamples of our original sample to calculate our statistic of interest and more importantly, the variance of that statistic. Lumley’s use of the equation above gives the basic idea but I believe the more rigorous justification appeals to theory involving empiricial distribution functions, as much of the theory underlying these ideas relies on getting a good estimate of the empirical distribution.\nIt isn’t explicitly clear which of these techniques is most popular currently, but my guess would be that the bootstrap is the most used. This also happens to be the method that Lumley has provided the most citations for in the text. I’ve also run into cases where the US Census IPUMS data uses successive difference weights.\nAll this to say that replicate weights are powerful for producing “non-parametric” estimates, like quantiles and so on, and different weighting techniques may be more or less appropriate depending on the design and data involved.\nReplicate Weights in R\nLumley first demonstrates how to setup a survey design object when the weights are already provided. I’ve had trouble accessing the 2005 California Health Interview Survey data on my own but he thankfully provides a link to the data on his website.\n\nShow the codechis_adult &lt;- as.data.frame(read_dta(\"Data/ADULT.dta\")) %&gt;%\n  # have to convert labeled numerics to regular numerics for\n  # computation in survey package.\n  mutate(\n    bmi_p = as.numeric(bmi_p),\n    srsex = factor(srsex, labels = c(\"MALE\", \"FEMALE\")),\n    racehpr = factor(racehpr, labels = c(\n      \"LATINO\", \"PACIFIC ISLANDER\",\n      \"AMERICAN INDIAN/ALASKAN NATIVE\",\n      \"ASIAN\", \"AFRICAN AMERICAN\",\n      \"WHITE\",\n      \"OTHER SINGLE/MULTIPLE RACE\"\n    ))\n  )\nchis &lt;- svrepdesign(\n  variables = chis_adult[, 1:418],\n  repweights = chis_adult[, 420:499],\n  weights = chis_adult[, 419, drop = TRUE],\n  ## combined.weights specifies that the replicate weights\n  ## include the sampling weights\n  combined.weights = TRUE,\n  type = \"other\", scale = 1, rscales = 1\n)\nchis\n\nCall: svrepdesign.default(variables = chis_adult[, 1:418], repweights = chis_adult[, \n    420:499], weights = chis_adult[, 419, drop = TRUE], combined.weights = TRUE, \n    type = \"other\", scale = 1, rscales = 1)\nwith 80 replicates.\n\n\nWhen creating replicate weights in R one specifies a replicate type to the type argument.\n\nShow the codeboot_design &lt;- as.svrepdesign(strat_design,\n  type = \"bootstrap\",\n  replicates = 100\n)\nboot_design\n\nCall: as.svrepdesign.default(strat_design, type = \"bootstrap\", replicates = 100)\nSurvey bootstrap with 100 replicates.\n\n\nBy default, the as.svrepdesign() function assumes the replicate weights are jackknife replicates.\n\nShow the code## jackknife is the default\njk_design &lt;- as.svrepdesign(strat_design)\njk_design\n\nCall: as.svrepdesign.default(strat_design)\nStratified cluster jackknife (JKn) with 200 replicates.\n\n\nOnce the design object is created the mean of a variable can computed equivalently as before using the svymean() function. We’ll compare the bootstrap and jackknife estimates, noting that the bootstrap has a higher standard error than the jackknife.\n\nShow the codesvymean(~enroll, boot_design)\n\n         mean     SE\nenroll 595.28 21.113\n\n\n\nShow the codesvymean(~enroll, jk_design)\n\n         mean     SE\nenroll 595.28 18.509\n\n\nOf course, part of the motivation in using replicate weights is that you’re able to estimate standard errors for non-trivial estimands, especially those that may not be implemented in the survey package. Lumley demonstrates this using a sample from the National Wilms Tumor Study Cohort, in order to estimate the five year survival probability via a Kaplan-Meier Estimator.\n\nShow the codelibrary(addhazard)\nnwtsco &lt;- as_tibble(nwtsco)\nhead(nwtsco)\n\n# A tibble: 6 × 12\n   trel  tsur relaps  dead study stage histol instit   age    yr specwgt tumdiam\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;dbl&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n1 21.9  21.9       0     0     3     1      1      1  2.08  1979     750      14\n2 11.3  11.3       0     0     3     2      0      0  4.17  1979     590       9\n3 22.1  22.1       0     0     3     1      1      1  0.75  1979     356      13\n4  8.02  8.02      0     0     3     2      0      0  2.67  1979     325       9\n5 20.5  20.5       0     0     3     2      0      0  3.67  1979     970      17\n6 14.4  14.4       1     1     3     2      0      1  2.58  1979     730      15\n\n\n\nShow the codecases &lt;- nwtsco %&gt;% filter(relaps == 1)\ncases &lt;- cases %&gt;% mutate(wt = 1)\nctrls &lt;- nwtsco %&gt;% filter(relaps == 0)\nctrls &lt;- ctrls %&gt;%\n  mutate(wt = 10) %&gt;%\n  sample_n(325)\nntw_sample &lt;- rbind(cases, ctrls)\n\nfivesurv &lt;- function(time, status, w) {\n  scurve &lt;- survfit(Surv(time, status) ~ 1, weights = w)\n  ## minimum probability that corresponds to a survival time &gt; 5 years\n  return(scurve$surv[min(which(scurve$time &gt; 5))])\n}\n\ndes &lt;- svydesign(id = ~1, strata = ~relaps, weights = ~wt, data = ntw_sample)\njkdes &lt;- as.svrepdesign(des)\nwithReplicates(jkdes, quote(fivesurv(trel, relaps, .weights)))\n\n       theta     SE\n[1,] 0.83669 0.0016\n\n\nThe estimated five year survival probability of 84% (95% CI: 84%,85%) uses the fivesurv function which computes the kaplan meier estimate of five year survival probability fora given time status and weight. The withReplicates function then re-estimates this statistic using each set of replicates and calculates the standard error from the variability of these estimates.\nIts worth noting that this is the standard error for estimating the five year survival in the NWTS cohort, not the hypothetical superpopulation of all children with Wilms’ tumor.\nNow again with the srvyr package\n\nShow the codeboot_design &lt;- as_survey_rep(strat_design,\n  type = \"bootstrap\",\n  replicates = 100\n)\nboot_design\n\nCall: Called via srvyr\nSurvey bootstrap with 100 replicates.\nData variables: \n  - cds (chr), stype (fct), name (chr), sname (chr), snum (dbl), dname (chr),\n    dnum (int), cname (chr), cnum (int), flag (int), pcttest (int), api00\n    (int), api99 (int), target (int), growth (int), sch.wide (fct), comp.imp\n    (fct), both (fct), awards (fct), meals (int), ell (int), yr.rnd (fct),\n    mobility (int), acs.k3 (int), acs.46 (int), acs.core (int), pct.resp (int),\n    not.hsg (int), hsg (int), some.col (int), col.grad (int), grad.sch (int),\n    avg.ed (dbl), full (int), emer (int), enroll (int), api.stu (int), pw\n    (dbl), fpc (dbl)\n\n\n\nShow the codeboot_design %&gt;% summarise(Mean = survey_mean(enroll))\n\n# A tibble: 1 × 2\n   Mean Mean_se\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  595.    22.1\n\n\nIt’s not clear or straightforward to me from reading the srvyr docs how to estimate the weighted survival function probability — I may return to this later.\nFinal Notes on Replicate Weights\nLumley finishes this section by noting that the bootstrap typically works better when all the strata are large. While a strata correction is available it is likely not correct for small or unequal strata.\nSeparately, Lumley note that both the jackknife and bootstrap can incorporate finite population correction factors.\nFinally, the BRR designs implemented in the survey package will use at most excess 4 replicate splits for K &lt; 180 and at most 5% when K &gt; 100. It is not clear to me from the reading, which is more likely to be used for 100 &lt; K &lt; 180."
  },
  {
    "objectID": "ComplexSurveyNotes.html#other-population-summaries",
    "href": "ComplexSurveyNotes.html#other-population-summaries",
    "title": "Complex Survey Notes",
    "section": "Other population summaries",
    "text": "Other population summaries\nWhile population means, totals, and differences are typically easy to estimate via the horvitz thompson estimator there are other population statistics such as the median or regression estimates that are more complex. These require using the replicate weights described in the previous section or making certain linearization / interpolation assumptions which may or may not influence the resulting estimate.\nQuantiles\nEstimation of quantiles involves estimating arbitary points along the cumulative distribution function(cdf). For example, the 90th percentile has 90% of the estimated population size below it and 10% above. In this case, for cdf F_X(x), we want to estimate x: F_X(x) = 0.9. However, estimating the cdf presents some technical difficulties in that the empirical cumulative distribution function (ecdf), is not typically a “smooth” estimate for any given x — as the estimate is highly dependent upon the sample. Consequently, Lumley’s function, svyquantile() interpolates linearly between two adjacent observations when the quantile is not uniquely defined.\n\nShow the codesamp &lt;- rnorm(20)\nplot(ecdf(samp))\n\n\n\nEmpirical Cumulative Distribution Function - note the jumps at distinctive points along the x-axis.\n\n\n\nConfidence intervals are constructed similarly, using the ecdf, though it should be noted that estimating extreme quantiles poses difficulties because of the low density values in the area.\nA first calculation to demonstrate this using replicate weights with the CA health interview study, estimating different quantiles of BMI.\n\nShow the codesvyquantile(~bmi_p, design = chis, quantiles = c(0.25, 0.5, 0.75))\n\n$bmi_p\n     quantile ci.2.5 ci.97.5         se\n0.25    22.68  22.66   22.81 0.03767982\n0.5     25.75  25.69   25.80 0.02763161\n0.75    29.18  29.12   29.29 0.04270393\n\nattr(,\"hasci\")\n[1] TRUE\nattr(,\"class\")\n[1] \"newsvyquantile\"\n\n\nThe same thing can be done with the stratified design. Here the uncertainty is computed via the estimates of the ecdf and finding the pointwise confidence interval for different points along the curve.\n\nShow the codesvyquantile(~api00,\n  design = strat_design, quantiles = c(0.25, 0.5, 0.75),\n  ci = TRUE\n)\n\n$api00\n     quantile ci.2.5 ci.97.5       se\n0.25      565    535     597 15.71945\n0.5       668    642     694 13.18406\n0.75      756    726     778 13.18406\n\nattr(,\"hasci\")\n[1] TRUE\nattr(,\"class\")\n[1] \"newsvyquantile\"\n\n\nYou can see how to construct the same estimate below using the srvyr package.\n\nShow the codesrvyr_strat_design %&gt;%\n  summarize(quantiles = survey_quantile(api00, quantiles = c(0.25, 0.5, 0.75)))\n\n# A tibble: 1 × 6\n  quantiles_q25 quantiles_q50 quantiles_q75 quantiles_q25_se quantiles_q50_se\n          &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1           565           668           756             15.7             13.2\n# ℹ 1 more variable: quantiles_q75_se &lt;dbl&gt;\n\n\nContingency Tables\nLumley’s main points in this section focus on the complications in interpretation of typical contingency table tests of association in a design based setting. Specifically, he points out that it is not obvious how the null distribution should be generated without making some kind of modeling assumptions. Quoting from the book (text in parentheses from me):\n\nFor example, if there are 56,181,887 women and 62,710,561 men in a population it is not possible for the proportions of men and women who are unemployed to be the same, since these population sizes have no common factors. We would know without collecting any employment data that the finite-population null hypothesis (of equal proportions) was false. A more interesting question is whether the finite population could have arisen from a process that had no association between the variables: is the difference at the population level small enough to have arisen by chance…. A simpler approach is to treat the sample as if it came from an infinite superpopulation and simply ignore the finite-population corrections in inference.\n\nThe super-population approach offers the more interesting approach philosophically and thus is implemented in the survey package. The svychisq function implements a test for no association as the null using a chi-squared distribution with a correction for the mean and variance. Lumley discusses various methods for computing the \\chi^2 statistic in this setting and their implementations in svycontrast(). I’d suggest looking at the function documentation if that level of detail is needed.\nLumley demonstrates how to call these functions estimating the proportion of smokers in each insurance status group from the California Health Interview Survey.\n\nShow the codetab &lt;- svymean(~ interaction(ins, smoking, drop = TRUE), chis)\ntab\n\n                                              mean     SE\ninteraction(ins, smoking, drop = TRUE)1.1 0.112159 0.0021\ninteraction(ins, smoking, drop = TRUE)2.1 0.039402 0.0015\ninteraction(ins, smoking, drop = TRUE)1.2 0.218909 0.0026\ninteraction(ins, smoking, drop = TRUE)2.2 0.026470 0.0012\ninteraction(ins, smoking, drop = TRUE)1.3 0.507728 0.0036\ninteraction(ins, smoking, drop = TRUE)2.3 0.095332 0.0022\n\n\n\nShow the codeftab &lt;- ftable(tab, rownames = list(\n  ins = c(\"Insured\", \"Uninsured\"),\n  smoking = c(\"Current\", \"Former\", \"Never\")\n))\nround(ftab * 100, 1)\n\n             ins Insured Uninsured\nsmoking                           \nCurrent mean        11.2       3.9\n        SE           0.2       0.1\nFormer  mean        21.9       2.6\n        SE           0.3       0.1\nNever   mean        50.8       9.5\n        SE           0.4       0.2\n\n\nIn the output below we see a very small p-value indicating that the data were unlikely to be generated from a process in which smoking and insurance status were independent.\n\nShow the codesvychisq(~ smoking + ins, chis)\n\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  svychisq(~smoking + ins, chis)\nF = 130.11, ndf = 1.9923, ddf = 157.3884, p-value &lt; 2.2e-16\n\n\nEstimates in Subpopulations\nEstimation within subpopulations (also called domain estimation) that are sampled strata is easy since a stratified sample is composed of random samples within strata by definition; simply compute the desired statistic within the given strata using the strata-specific random sample.\nWhen the subpopulation of interest is not a strata, things are more difficult. While the sampling weights would still be correct for representing any given observation to the total population — resulting in an unbiased mean point estimate — the co-occurrence probabilities \\pi_{i,j} would be incorrect, because the co-occurrence probabilities are now unknown/random and not fixed by design. The two (usual) approaches for trying to estimate subpopulation variance estimates in-spite of these difficulties are linearization and replicate weighting. The computation is straightforward for replicate weighting since the non-subpopulation entities can simply be discarded in the computation. For linearization the computation is less straightforward as the extra entities still have to be included as “0”s in the final computation – this is according to Lumley’s arguments in his Appendix.\nExamples below demonstrating this idea estimate the number of teachers with emergency, emer, training amongst California schools using the api dataset.\n\nShow the codeemerg_high &lt;- subset(strat_design, emer &gt; 20)\nemerg_low &lt;- subset(strat_design, emer == 0)\nsvymean(~ api00 + api99, emerg_high)\n\n        mean     SE\napi00 558.52 21.708\napi99 523.99 21.584\n\n\n\nShow the codesvymean(~ api00 + api00, emerg_low)\n\n        mean     SE\napi00 749.09 17.516\n\n\n\nShow the codesvytotal(~enroll, emerg_high)\n\n        total     SE\nenroll 762132 128674\n\n\n\nShow the codesvytotal(~enroll, emerg_low)\n\n        total    SE\nenroll 461690 75813\n\n\nIn general, if replicate weights are available, domain estimation is much easier.\n\nShow the codebys &lt;- svyby(~bmi_p, ~ srsex + racehpr, svymean,\n  design = chis,\n  keep.names = FALSE\n)\nprint(bys, digits = 3)\n\n    srsex                        racehpr bmi_p     se\n1    MALE                         LATINO  28.2 0.1447\n2  FEMALE                         LATINO  27.5 0.1443\n3    MALE               PACIFIC ISLANDER  29.7 0.7055\n4  FEMALE               PACIFIC ISLANDER  27.8 0.9746\n5    MALE AMERICAN INDIAN/ALASKAN NATIVE  28.8 0.5461\n6  FEMALE AMERICAN INDIAN/ALASKAN NATIVE  27.0 0.4212\n7    MALE                          ASIAN  24.9 0.1406\n8  FEMALE                          ASIAN  23.0 0.1112\n9    MALE               AFRICAN AMERICAN  28.0 0.2663\n10 FEMALE               AFRICAN AMERICAN  28.4 0.2417\n11   MALE                          WHITE  27.0 0.0598\n12 FEMALE                          WHITE  25.6 0.0680\n13   MALE     OTHER SINGLE/MULTIPLE RACE  26.9 0.3742\n14 FEMALE     OTHER SINGLE/MULTIPLE RACE  26.7 0.3158\n\n\n\nShow the code# This is the code from the book but it didn't work for me\n# because of issues in the survey R package, I reproduce the\n# first result using the srvyr package below\n# medians &lt;- svyby(~bmi_p, ~ srsex + racehpr, svyquantile,\n#   design = chis,\n#   covmat = TRUE,\n#   quantiles = 0.5\n# )\n# svycontrast(medians, quote(MALE.LATINO/FEMALE.LATINO))\n\nmedians &lt;- chis %&gt;%\n  as_survey() %&gt;%\n  group_by(srsex, racehpr) %&gt;%\n  summarize(Median_BMI = survey_median(bmi_p, vartype = \"ci\"))\n\nmedians\n\n# A tibble: 14 × 5\n# Groups:   srsex [2]\n   srsex  racehpr                       Median_BMI Median_BMI_low Median_BMI_upp\n   &lt;fct&gt;  &lt;fct&gt;                              &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n 1 MALE   LATINO                              27.4           27.3           27.8\n 2 MALE   PACIFIC ISLANDER                    28.9           27.8           30.7\n 3 MALE   AMERICAN INDIAN/ALASKAN NATI…       28.4           26.9           29.3\n 4 MALE   ASIAN                               24.4           24.2           24.8\n 5 MALE   AFRICAN AMERICAN                    27.4           26.6           28.1\n 6 MALE   WHITE                               26.4           26.3           26.5\n 7 MALE   OTHER SINGLE/MULTIPLE RACE          26.6           25.9           27.5\n 8 FEMALE LATINO                              26.3           25.8           26.5\n 9 FEMALE PACIFIC ISLANDER                    27.3           25.6           28.3\n10 FEMALE AMERICAN INDIAN/ALASKAN NATI…       25.1           24.5           26.5\n11 FEMALE ASIAN                               22.1           22.0           22.4\n12 FEMALE AFRICAN AMERICAN                    27.2           26.6           27.5\n13 FEMALE WHITE                               24.3           24.2           24.4\n14 FEMALE OTHER SINGLE/MULTIPLE RACE          25.7           25.1           26.5"
  },
  {
    "objectID": "ComplexSurveyNotes.html#design-of-stratified-samples",
    "href": "ComplexSurveyNotes.html#design-of-stratified-samples",
    "title": "Complex Survey Notes",
    "section": "Design of Stratified Samples",
    "text": "Design of Stratified Samples\nHow to pick the sample size for each strata? Well it depends on the goals of the analysis. If the goal is to estimate a total across the whole population, the formula for the variance of a total can be used to gain insights about optimal allocation. Since the variance of the total is dependent (via sum) of the strata specific variances, more sample size would want to be dedicated to more heterogeneous and/or larger strata.\nThis general approach means that the sample size for strata k, n_k should be proportional to the population strata size N_k and strata variance \\sigma^{2}_k, n_k \\propto \\sqrt{N^2_k \\sigma^2_k} = N_k \\sigma_k. Lumley notes that while this expression satisfies some theoretical optimality criteria, it is often the case that different strata have different costs associated with their sampling and so the expression can be modified in order to take into account this cost as follows:\n\nn_k \\propto \\frac{ N_k \\sigma_k}{\\sqrt{\\text{cost}_k}},\n\nwhere cost_k is the cost of sampling for strata k."
  },
  {
    "objectID": "ComplexSurveyNotes.html#exercises-1",
    "href": "ComplexSurveyNotes.html#exercises-1",
    "title": "Complex Survey Notes",
    "section": "Exercises",
    "text": "Exercises\n1.You are conducting a survey of emergency preparedness at a large HMO, where you want to estimate what proportion of medical staff would be able to get to work after an earthquake.\n\nYou can either send out a single questionnaire to all staff, or send out a questionnaire to about 10% of the staff and make follow-up phone calls for those that don’t respond. What are the disadvantages of each approach?\n\nThis comes down to a discussion of cost for sampling and what missing data mechanism may be at play. As a simple starting point, if we were to assume the resulting data were MCAR and the non response rate was equivalent between both sampling strategies, the single questionnaire would be preferred because it would result in a higher overall sample size. These assumptions are probably not likely however, and we may expect that non-response is associated with other meaningful factors, by choosing a the follow-up phone call we might minimize non-response to both reduce bias and improve precision.\nAdditional relevant concerns would be the possible response or lack of response of certain strata — certain physicians, technicians or other kinds of staff’s response would likely be worth knowing yet these groups may be less well represented in a 10% simple random sample of the population.\n\nYou choose to survey just a sample. What would be useful variables to stratify the sampling, and why?\n\nThe aforementioned job title would be useful to stratify on. This would likely be most useful to conduct within each department. Further, if the HMO has more than one site or clinic, that would be worth stratifying on as well for substantive reasons just as much as statistical reasons.\n\nThe survey was conducted with just two strata: physicians and other staff. The HMO has 900 physicians and 9000 other staff. You sample 450 physicians and 450 other staff. What are the sampling probabilities in each stratum?\n\nPhysician strata sampling probabilities are \\frac{n}{N_k} = \\frac{450}{900} = \\frac{1}{2}, while the “other staff” probabilities are \\frac{450}{9000} = \\frac{1}{20}.\n\n300 physicians and 150 other staff say they would be able to get to work after an earthquake. Give unbiased estimates of the proportion in each stratum and the total proportion.\n\nThe physician strata estimate would be \\frac{300}{450} = \\frac{2}{3}. The staff strata would be \\frac{150}{450} = \\frac{1}{3} The total proportion would be \\frac{2 \\times 300 + 20 \\times 150}{9900}. This value can be recreated below with the survey package as follows.\n\nShow the codedf &lt;- tibble(\n  id = 1:900,\n  job = c(rep(\"MD\", 450), rep(\"staff\", 450)),\n  prep = c(rep(1, 300), rep(0, 150), rep(1, 150), rep(0, 300)),\n  weights = c(rep(2, 450), rep(20, 450))\n)\nhmo_design &lt;- svydesign(strata = ~job, ids = ~0, weights = ~weights, data = df)\nhmo_design\n\nStratified Independent Sampling design (with replacement)\nsvydesign(strata = ~job, ids = ~0, weights = ~weights, data = df)\n\n\n\nShow the codesvymean(~prep, hmo_design)\n\n        mean     SE\nprep 0.36364 0.0203\n\n\n\nHow would you explain to the managers that commissioned the study how the estimate was computed and why it wasn’t just the number who said “yes” divided by the total number surveyed?\n\nWe sampled from the total population using the strata because we though these two groups would respond differently and indeed, they did. Physicians have are twice as likely to be able to make it to the hospital in the event of an emergency as general staff. However, physicians make up a much smaller proportion of the overall hospital workforce and so we need to down weight their responses, relative to general staff in order to ensure their response reflects their distribution in the total population, thus the total estimate of the HMO’s emergency preparedness is much closer to the “general” staff’s strata estimate of \\frac{1}{3}.\n2.You are conducting a survey of commuting time and means of transport for a large university. What variables are likely to be available and useful for stratifying the sampling?\nProbably worth stratifying on “role” at university — student vs. staff vs. professor. Each of these have varying amounts of income available and would likely determine their different means and, consequently, commute time of getting to campus. It might also be worth stratifying on the department of employment for staff and professors, as there can be a wide variability in these measures, again, by department.\n3.-4. Skip because of CHIS data issues\n\nIn the Academic Performance Index data we saw large gains in precision from stratification on school type when estimating mean or total school size, and no gain when estimating mean Academic performance Index. Would you expect a large or small gain from the following variables: mobility, emer, meals, pcttest? Compare your expectations with the actual results.\n\nThe general principle here, is which of these variables do we expect to have some association with the school type. The greater association the more the benefit from stratifying.\n\nFor estimating total school enrollment in the Academic Performance Index population, what is the optimal allocation of a total sample size of 200 stratified by school size? Draw a sample with this optimal allocation and compare the standard errors to the stratified sample in Figure 2.5 for: total enrollment, mean 2000 API, mean meals, mean ell.\n\nA first point worth noting is that school size is an integer valued variable and so some grouping will have to be created to define the strata from which schools are then drawn. One possible option is to define the strata as above and below the median school enrollment. Since this divides the population exactly in half the strata are equally sized and the only differentiating factor is the variability of the enrollment.\n\nShow the codeas_tibble(apipop) %&gt;%\n  transmute(\n    enroll = enroll,\n    strat = if_else(enroll &gt; median(enroll, na.rm = TRUE), \"Above\", \"Below\")\n  ) %&gt;%\n  group_by(strat) %&gt;%\n  summarize(sd_enroll = sd(enroll))\n\n# A tibble: 3 × 2\n  strat sd_enroll\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Above     504. \n2 Below      89.8\n3 &lt;NA&gt;       NA  \n\n\nSince the variability of the school enrollment sizes in the above median size schools is roughly five times that in the below median size schools, we’d sample from the two strata at a 5:1 ratio, respectively. Or, explicitly, we’d sample 160 schools from the above median school size and 40 schools from the below median school size.\n\nShow the codeapi_opt_strat &lt;- as_tibble(apipop) %&gt;%\n  filter(!is.na(enroll)) %&gt;%\n  transmute(\n    enroll = enroll,\n    strat = if_else(enroll &gt; median(enroll, na.rm = TRUE), \"Above\", \"Below\")\n  )\n\nabove &lt;- api_opt_strat %&gt;%\n  filter(strat == \"Above\") %&gt;%\n  mutate(fpc = n()) %&gt;%\n  slice_sample(n = 160)\n\nbelow &lt;- api_opt_strat %&gt;%\n  filter(strat == \"Below\") %&gt;%\n  mutate(fpc = n()) %&gt;%\n  slice_sample(n = 40)\n\nopt_strat_design &lt;- svydesign(\n  ids = ~1, strata = ~strat, fpc = ~fpc,\n  data = rbind(above, below),\n)\n\nsvytotal(~enroll, opt_strat_design)\n\n         total     SE\nenroll 3780158 113922\n\n\nThe mean estimate is slightly closer to the true value, 3811472 but the standard error is slightly larger (119K vs 114K) compared to the previous stratified design. However, the other design sampled 1000s of schools per strata so this is remarkably efficient.\nI won’t perform the other comparisons, but one would expect this design to be much less efficient at estimating the other variables if they are not well correlated with enrollment size.\n\nFigure 2.1 shows that the mean school size (enroll) in simple random samples of size 200 from the Academic Performance Index population has close to a Normal distribution.\n\n\nConstruct similar graphs for SRS of size 200, 50, 25, 10.\n\nI’ve already done something like this at the start of the chapter. We’d expect the central limit theorem (the normality of the sample means) to be better for the larger sample sizes listed. Or rather, we might not trust the standard error for the sample size of 10 to really represent the variability of the sample mean.\n\nRepeat for median school size.\n\n\nShow the codepop_median &lt;- median(apipop$enroll)\np1 &lt;- apipop %&gt;%\n  ggplot(aes(x = enroll)) +\n  geom_histogram() +\n  xlab(\"Student Enrollment\") +\n  geom_vline(xintercept = pop_median, linetype = 2, color = \"red\") +\n  ggtitle(\"Distribution of School Enrollment\",\n    subtitle = \"Median Enrollment Identified\"\n  )\n\np &lt;- replicate(n = 500, {\n  apipop %&gt;%\n    sample_n(200) %&gt;%\n    pull(enroll) %&gt;%\n    median(., na.rm = TRUE)\n})\n\np &lt;- tibble(sample_ix = 1:500, sample_median = p) %&gt;%\n  ggplot(aes(x = sample_median)) +\n  geom_histogram() +\n  xlab(\"Student Enrollment Medians\") +\n  geom_vline(\n    xintercept = pop_median,\n    linetype = 2, color = \"red\"\n  ) +\n  ggtitle(\"Distribution of Sample Medians\")\np1 + p\n\n\n\n\n\nRepeat for mean school size in stratified samples of size 100, 52, 24, 12 using the same stratification proportions (50% elementary, 25% middle schools, 25% high schools) as in the built-in stratified sample.\n\nThe same result holds since the samples are independent within and across strata as well.\n\nShow the codens &lt;- c(100, 52, 24, 12)\nsimdf &lt;- map_dfr(1:200, function(sim_ix) {\n  map_dfr(ns, function(n) {\n    apipop %&gt;%\n      filter(stype == \"E\") %&gt;%\n      mutate(fpc = n()) %&gt;%\n      slice_sample(n = n * .5) %&gt;%\n      rbind(\n        .,\n        apipop %&gt;%\n          filter(stype != \"E\") %&gt;%\n          group_by(stype) %&gt;%\n          mutate(fpc = n()) %&gt;%\n          slice_sample(n = n * .25)\n      ) %&gt;%\n      as_survey_design(strata = stype, fpc = fpc) %&gt;%\n      summarize(mean_enroll = survey_mean(enroll, na.rm = TRUE)) %&gt;%\n      mutate(sim_ix = sim_ix, n = n)\n  })\n})\nsimdf %&gt;%\n  ggplot(aes(x = mean_enroll, fill = factor(n))) +\n  geom_histogram() +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"CLT for stratified samples of different sizes.\")\n\n\n\n\n\nIn a design with just two strata write the sample sizes as n_1 and n-n_1 so that there is only one quantity that can be varied. Differentiate the variance of the total with respect to n_1 to find the optimal allocation for two strata. Extend this to any number of strata by using the fact that an optimal allocation cannot be improved by moving samples from one stratum to another stratum.\n\n\n\\hat{V}[\\hat{T}] = V_1 + V_2 \\\\\nV_1 = \\frac{N_1 - n_1}{N_1} N_1^2 \\frac{\\sigma^2_1}{n_1} \\\\\nV_2 = \\frac{N_2 - (n - n_1)}{N_2} N_2^2 \\frac{\\sigma^2_2}{n_2}\n taking the derivative …\n\n\\frac{d\\hat{V}[\\hat{T}]}{dn_1} = \\frac{dV_1}{dn_1} + \\frac{dV_2}{dn_1} \\\\\n= \\frac{-N_1^2 \\sigma^2_1}{n_1^2} + \\frac{N_2^2 \\sigma_2^2}{(n - n_1)^2}\n setting to zero and and solving for n_1 we get \n\\frac{-N_1^2 \\sigma^2_1}{n_1^2} + \\frac{N_2^2 \\sigma_2^2}{(n - n_1)^2} = 0 \\\\\n\\iff \\\\\n\\frac{N_2^2\\sigma^2_2}{(n-n_1)^2} = \\frac{N_1^2\\sigma_1^2}{n_1^2}\\\\\n\\iff \\\\\n\\frac{n_1^2}{(n-n_1)^2} = \\frac{N_1^2\\sigma_1^2}{N_2^2\\sigma^2_2} \\\\\n\\iff \\\\\n\\frac{n_1}{(n-n_1)} = \\frac{N_1\\sigma_1}{N_2\\sigma}  \\\\\n\\iff \\\\\nn_1 = \\frac{nN_1\\sigma_1}{N_2\\sigma_2(1 + \\frac{N_1\\sigma_1}{N_2\\sigma_2})} \\\\\n= \\frac{nN_1\\sigma_1}{N_2\\sigma_2 + N_1\\sigma_1}\n\nWhere the square root is taken over variables constrained to be positive so we only have the positive values as the solution to the equation.\nAlso, N_1, N_2 are taken to be the population strata sizes and \\sigma_1, \\sigma_2 are the strata standard deviations.\nWe can check the second derivative to ensure this is a global optima or we can use the fact that the variance is a quadratic function and is therefore convex. Consequently, there is only one global optima.\nExamining the expression - we see the optimal strata size is the variance of each strata weighted by the size of the strata as a fraction of the same value added across all strata, i.e. the population variance.\nHere we’ve derived the solution for the “first” strata, but this is arbitrary and there would be a similar value for any strata, for a design with any number of strata.\n\nWrite an R function that takes inputs n_1, n_2, N_1, N_2, \\sigma^2_1, \\sigma^2_2 and computes the variance of the population total in a stratified sample. Choose some reasonable values of the population sizes and variances, and graph this function as n_1 and n_2 change, to find the optimum and to examine how sensitive the variance is the precise values of n_1 and n_2.\n\n\nShow the codestrat_var_sample &lt;- function(n_1, n_2, N_1, N_2, sigma_1, sigma_2) {\n  one_strat_var &lt;- function(n, N, sigma) {\n    (N - n) / N * N^2 * sigma^2 / n\n  }\n  return(one_strat_var(n_1, N_1, sigma_1) + one_strat_var(n_2, N_2, sigma_2))\n}\nexpand.grid(\n  n_1 = seq(2, 100, 10), n_2 = seq(2, 100, 10),\n  N_1 = 1E3, N_2 = 1E3, sigma_1 = 1, sigma_2 = 1\n) %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    var = map2_dbl(n_1, n_2, strat_var_sample, unique(N_1), unique(N_2), unique(sigma_1), unique(sigma_2)),\n    n_2 = factor(n_2)\n  ) %&gt;%\n  ggplot(aes(x = n_1, y = var, color = n_2)) +\n  geom_point() +\n  geom_line() +\n  ylab(\"Variance\") +\n  ggtitle(\"Two Strata Design Variance\", subtitle = \"For Varying Strata Sizes\")\n\n\n\n\nI think the point Lumley is after is that the variance doesn’t change dramatically after each sample size is roughly more than ten. This is the quadratic behavior of the variance at play. Obviously the “optimal” variance will be found at the highest n_1 and n_2 as those values will provide the lowest variance. In terms of where the greatest efficiency lies, however, we see the greatest change in the variance estimates after the two sample sizes are greater than 10, as stated previously.\n\nVerify that equation 2.2 gives the HTE of variance for a SRS\n\n\nShow that when i \\neq j\n\n\n\n\\pi_{ij} = \\frac{n}{N}n - 1N - 1\n It isn’t completely clear from the above, but I think Lumley means for there to be parentheses around the n-1 and N-1 terms, that is: \n\\pi_{ij} = \\frac{n}{N}(n-1)(N-1)\n\nThis follows from combinatorics. We want the number of combinations in which two (different) items are chosen from a sample of n together, divided by the total number of combinations of n samples from a population of size N.\n\n\\frac{n \\choose 2}{N\\choose n} = \\frac{n!}{(n-2)!2!} \\frac{(N-n)!n!}{N!} \\\\\n=\\frac{n(n-1)}{2} \\times \\frac{(N-n)!n!}{N!}\n I don’t see yet how this reduces to the intended form but may return to this later.\n\nCompute \\pi_{ij} - \\pi_i \\pi_j\n\n\nUsing the expression Lumley gives us for \\pi_{ij}, and assuming a similar combinatoric form for \\pi_i, \\pi_j,\n\n\\pi_{ij} - \\pi_i \\pi_j =  \\frac{n}{N}(n-1)(N-1) - \\left(\\frac{n (N-n)!n!}{N!}\\right)^2.\n\n\nShow that the equation in exercise 1.10 (c) reduces to equation 2.2\n\nI’m clearly on the wrong track here. My formulation of the combinatorics must be throwing me off. Feel free to file an issue in the repo for these notes if you see what I’m missing.\n\nSuppose instead each individual in the population is independently sampled with probability \\frac{n}{N}, so that the sample size n is not fixed. Show that the finite population correction disappears from equation 2.2 for this Bernoulli sampling design.\n\n\nV[\\hat{T}_{HTE,Bern}] = V[\\sum_{i=1}^{N} R_i \\frac{Y_i}{\\pi}] \\\\\n\\stackrel{ind}{=} \\sum_{i=1}^{N} V[R_i \\frac{Y_i}{\\pi}] \\\\\n= \\sum_{i=1}^{N} \\pi(1-\\pi)\\frac{Y_i^2}{\\pi^2}\\\\\n= \\frac{(1-\\pi)}{\\pi}\\sum_{i=1}^{N}Y_i^2 \\\\\n= (\\frac{1}{\\pi} - 1) \\sum_{i=1}^{N}Y_i^2 \\\\\n= \\frac{N -n}{n} \\sum_{i=1}^{N} Y_i^2, \\quad  \\pi := \\frac{n}{N}\n which can be understand as equivalent to equation 2.2 where — assume without loss of generality — that the variable Y has E[Y]=0 then it is easy to see that the equation reduces to \n(N-n) \\times N \\times \\frac{V[Y]}{n} = (N^2 - Nn) \\frac{V[Y]}{n}\n\nwhich replaces the finite population correction factor with the N^2 - Nn term."
  },
  {
    "objectID": "ComplexSurveyNotes.html#why-clusters-the-nhanes-design",
    "href": "ComplexSurveyNotes.html#why-clusters-the-nhanes-design",
    "title": "Complex Survey Notes",
    "section": "Why Clusters? The NHANES design",
    "text": "Why Clusters? The NHANES design\nWhy sample clusters? Because sometimes it’s easier than sampling individuals. Specifically, in cases where the cost of sampling individuals can be quite high, sampling clusters can be more efficient. This is in spite of the fact that within cluster correlation tends to be positive, reducing the information in the sample. Lumley uses the NHANES survey to motivate this idea: moving mobile examination centers all across the country to sample individuals is extremely expensive. By sampling a large number of individuals within a census tract aggregation area the NHANES survey is able to reduce the cost of their effort at a reasonable expense in precision.\nSingle-stage and multistage designs\nDepending on the type of clusters involved it can be easy to sample the entire cluster as classrooms, medical practice and workplaces are, however it is more likely that some sub-sampling within clusters will be performed for the sake of efficiency. As Lumley notes, clusters in the first stage are called Primary Sampling Units or PSUs. “Stages” refer to the different levels at which sampling occurs. E.g. Sampling individuals within sampled census tracts within a state would involve sampling census tracts in the first stage and then individuals in the second stage. The diagram below communicates this idea graphically.\n\n\n\n\n\n\nSampling weights are determined assuming independence across stages — e.g. if a cluster of houses is sampled with probability \\pi_1 and a household is sampled within that cluster with probability \\pi_2 then the sampling probability for that house is \\pi = \\pi_1 \\times \\pi_2 and it’s weight is the inverse of that probability. Note that this requires that clusters be mutually exclusive - a sampled unit can belong only to one cluster and no others. Further, note that we can still have biased sampling within a stage, as independence is only required across stages to use to find probabilities via their product.\nLumley goes on to describe how cluster sampling and individual sampling can be mixed since each stratum of a survey can be thought of as a separate and independent sample it is trivial to combine single stage sampling in one stratum and multistage sampling in another; a stratified random sample can be used in high density regions where measurement of multiple units is less costly and a cluster sample can be taken in low density regions where the cost of each additional unit is more costly.\nThe statistical rationale behind this strategy is fairly straightforward — since the variance of the sum is the sum of the variances of each stage (assuming independence) each sampled cluster in a multistage sample can be considered as a population for further sampling. Lumley uses the example of a simplified NHANES design, where 64 regions are grouped into 32 strata. A simple random sample of 440 individuals are then measured in each region. In Lumley’s words,\n\nThe variance of an estimated total from this design can be partitioned across two sources: the variance of each estimated regional total around the true total of the region and the variance that would result if the true total for each of the 64 sampled regions were known exactly.\n\nIn my own words and understanding, I understand there to be variance that comes from grouping the 64 regions into 32 strata — so there is uncertainty across region and then the uncertainty within that region that results from the sample of only a subset of the population."
  },
  {
    "objectID": "ComplexSurveyNotes.html#describing-multi-stage-designs-to-r",
    "href": "ComplexSurveyNotes.html#describing-multi-stage-designs-to-r",
    "title": "Complex Survey Notes",
    "section": "Describing Multi Stage Designs to R",
    "text": "Describing Multi Stage Designs to R\nIn order to specify a single stage cluster sample or a multistage sample treated as a single stage sample with replacement, the main difference is that the PSU identifier needs to be supplied to the id argument, as follows.\n\nShow the code# Data originally found at\n# \"https://github.com/cran/LogisticDx/blob/master/data/nhanes3.rda\"\n\n\n\nShow the codenames3 &lt;- load(\"Data/nhanes/nhanes3.rda\")\nas_tibble(nhanes3)\n\n# A tibble: 17,030 × 16\n    SEQN SDPPSU6 SDPSTRA6 WTPFHX6 HSAGEIR HSSEX  DMARACER BMPWTLBS BMPHTIN\n   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1     3       1       44   1735.      21 male   white        180.    70.4\n 2     4       1       43   1725.      32 female white        136.    63.9\n 3     9       2       43  19452.      48 female white        150.    61.8\n 4    10       1        6  27770.      35 male   white        204.    69.8\n 5    11       2       40   1246.      48 male   white        155.    66.2\n 6    19       1       35   3861.      44 male   black        190.    70.2\n 7    34       1       13   5032.      42 female black        126.    62.6\n 8    44       1        8  28149.      24 female white        123.    64.4\n 9    45       1       22   4582.      67 female black        150.    64.3\n10    48       1       24  26919.      56 female white        240.    67.6\n# ℹ 17,020 more rows\n# ℹ 7 more variables: PEPMNK1R &lt;dbl&gt;, PEPMNK5R &lt;dbl&gt;, HAR1 &lt;fct&gt;, HAR3 &lt;fct&gt;,\n#   SMOKE &lt;fct&gt;, TCP &lt;dbl&gt;, HBP &lt;fct&gt;\n\n\n\nShow the codesvydesign(\n  id = ~SDPPSU6, strat = ~SDPSTRA6,\n  weight = ~WTPFHX6,\n  ## nest = TRUE indicates the PSU identifier is nested\n  ## within stratum - repeated across strata\n  nest = TRUE,\n  data = nhanes3\n)\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (98) clusters.\nsvydesign(id = ~SDPPSU6, strat = ~SDPSTRA6, weight = ~WTPFHX6, \n    nest = TRUE, data = nhanes3)\n\n\nSDPPSU6 is the pseudo PSU variable, and SDPSTRA6 is the stratum identifier defined for the single stage analysis.\nFor example, a two stage design for the API population that samples 40 school districts, then five schools within each district , the design has population size 757 at the first stage for the number of school districts in CA and the number of schools within each district for the second stage. The weights need not be supplied if they can be worked out from the other arguments.\n\nShow the codedata(api)\nas_tibble(apiclus2)\n\n# A tibble: 126 × 40\n   cds       stype name  sname  snum dname  dnum cname  cnum  flag pcttest api00\n   &lt;chr&gt;     &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n 1 31667796… E     Alta… Alta…  3269 Alta…    15 Plac…    30    NA     100   821\n 2 55751846… E     Tena… Tena…  5979 Big …    63 Tuol…    54    NA     100   773\n 3 41688746… E     Pano… Pano…  4958 Bris…    83 San …    40    NA      98   600\n 4 41688746… M     Lipm… Lipm…  4957 Bris…    83 San …    40    NA     100   740\n 5 41688746… E     Bris… Bris…  4956 Bris…    83 San …    40    NA      98   716\n 6 40687266… E     Cayu… Cayu…  4915 Cayu…   117 San …    39    NA     100   811\n 7 20651936… E     Full… Full…  2548 Chow…   132 Made…    19    NA     100   472\n 8 20651936… E     Fair… Fair…  2550 Chow…   132 Made…    19    NA     100   520\n 9 20651936… M     Wils… Wils…  2549 Chow…   132 Made…    19    NA     100   568\n10 06615980… H     Colu… Colu…   348 Colu…   152 Colu…     5    NA      96   591\n# ℹ 116 more rows\n# ℹ 28 more variables: api99 &lt;int&gt;, target &lt;int&gt;, growth &lt;int&gt;, sch.wide &lt;fct&gt;,\n#   comp.imp &lt;fct&gt;, both &lt;fct&gt;, awards &lt;fct&gt;, meals &lt;int&gt;, ell &lt;int&gt;,\n#   yr.rnd &lt;fct&gt;, mobility &lt;int&gt;, acs.k3 &lt;int&gt;, acs.46 &lt;int&gt;, acs.core &lt;int&gt;,\n#   pct.resp &lt;int&gt;, not.hsg &lt;int&gt;, hsg &lt;int&gt;, some.col &lt;int&gt;, col.grad &lt;int&gt;,\n#   grad.sch &lt;int&gt;, avg.ed &lt;dbl&gt;, full &lt;int&gt;, emer &lt;int&gt;, enroll &lt;int&gt;,\n#   api.stu &lt;int&gt;, pw &lt;dbl&gt;, fpc1 &lt;dbl&gt;, fpc2 &lt;int[1d]&gt;\n\n\n\nShow the code## dnum = district id\n## snum = school id\n## fpc1 = school id number\nclus1_design &lt;- svydesign(id = ~dnum, fpc = ~fpc, data = apiclus1)\nclus2_design &lt;- svydesign(\n  id = ~ dnum + snum, fpc = ~ fpc1 + fpc2,\n  data = apiclus2\n)\nclus2_design\n\n2 - level Cluster Sampling design\nWith (40, 126) clusters.\nsvydesign(id = ~dnum + snum, fpc = ~fpc1 + fpc2, data = apiclus2)"
  },
  {
    "objectID": "ComplexSurveyNotes.html#strata-with-only-one-psu",
    "href": "ComplexSurveyNotes.html#strata-with-only-one-psu",
    "title": "Complex Survey Notes",
    "section": "Strata with only one PSU",
    "text": "Strata with only one PSU\nWhen only one PSU exists within a population stratum, the sampling fraction must be 100%, since otherwise it would be 0%. In this case, the stratum does not contribute to the first stage variance and it should be ignored in calculating the first stage variance. Lumley argues that the best way to handle a stratum with only one PSU is to combine it with another stratum, one that is chosen to be similar based on population data available before the study was done. The survey package has two different methods implemented to handle “lonely” PSU’s. Lumley has written further on this topic here."
  },
  {
    "objectID": "ComplexSurveyNotes.html#how-good-is-the-single-stage-approximation",
    "href": "ComplexSurveyNotes.html#how-good-is-the-single-stage-approximation",
    "title": "Complex Survey Notes",
    "section": "How good is the single-stage approximation?",
    "text": "How good is the single-stage approximation?\nHere Lumley walks through an example detailing the trade-offs involved in using the single stage approximation. I’ll try to come up with a simulated example later as the data is not listed on the book’s website nor is it clear how to reassemble his dataset from the files at the NHIS site."
  },
  {
    "objectID": "ComplexSurveyNotes.html#sampling-by-size",
    "href": "ComplexSurveyNotes.html#sampling-by-size",
    "title": "Complex Survey Notes",
    "section": "Sampling by Size",
    "text": "Sampling by Size\n\nWhy do white sheep eat more than black sheep? There are more white sheep than black sheep\n\nA specific design theory, Probability-proportional-to-size (PPS), cluster sampling is a sampling strategy that exploits the fact that for a simple random sample of an unstratified population \\pi_i can be chosen such that it is approximately proportional to X_i, the variable of interest, the resulting variance of the estimate of the total V[\\hat{T}] = \\frac{N-n}{N} N^{2} \\frac{V[X]}{n} can then be controlled to be quite small. These are the same ideas I discussed at the start of the notes but more discussion on this topic can be found in (Tillé 2006; Hanif and Brewer 1980).\n\nShow the codedata(election)\nelection &lt;- as_tibble(election) %&gt;%\n  mutate(\n    votes = Bush + Kerry + Nader,\n    p = 40 * votes / sum(votes)\n  )\nelection %&gt;%\n  ggplot(aes(x = Kerry, y = Bush)) +\n  geom_point() +\n  scale_y_log10() +\n  scale_x_log10() +\n  ggtitle(\"Correlation in Voting Totals from US 2004 Presidential Election\",\n    subtitle = \"Both x and y axes are on log 10 scales.\"\n  )\n\n\n\n\nWhen Lumley’s book was written, only the single stage approximation of PPS could be analyzed using the survey package. A demo is shown below using the voting data, where a PPS sample is constructed and then analyzed.\n\nShow the codedata(election)\nelection &lt;- as_tibble(election) %&gt;%\n  mutate(\n    votes = Bush + Kerry + Nader,\n    p = 40 * votes / sum(votes)\n  )\nelection\n\n# A tibble: 4,600 × 8\n   County   TotPrecincts PrecinctsReporting   Bush Kerry Nader  votes       p\n   &lt;fct&gt;           &lt;int&gt;              &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt;\n 1 Alaska            439                439 151876 86064  3890 241830 0.0832 \n 2 Autauga            22                 22  15212  4774    74  20060 0.00691\n 3 Baldwin            50                 50  52910 15579   371  68860 0.0237 \n 4 Barbour            24                 24   5893  4826    26  10745 0.00370\n 5 Bibb               16                 16   5471  2089    12   7572 0.00261\n 6 Blount             26                 26  17364  3932    92  21388 0.00736\n 7 Bullock            27                 27   1494  3210     3   4707 0.00162\n 8 Butler             27                 27   4978  3409    13   8400 0.00289\n 9 Calhoun            53                 53  29806 15076   182  45064 0.0155 \n10 Chambers           24                 24   7618  5346    42  13006 0.00448\n# ℹ 4,590 more rows\n\nShow the codeinsample &lt;- sampling::UPtille(election$p)\nppsample &lt;- election[insample == 1, ]\nppsample$wt &lt;- 1 / ppsample$p\npps_design &lt;- svydesign(id = ~1, weight = ~wt, data = ppsample)\npps_design\n\nIndependent Sampling design (with replacement)\nsvydesign(id = ~1, weight = ~wt, data = ppsample)\n\n\n\nShow the codesvytotal(~ Bush + Kerry + Nader, pps_design, deff = TRUE)\n\n         total       SE   DEff\nBush  60203714  2711502 0.0118\nKerry 55617937  2695188 0.0052\nNader   377454    91396 0.1035"
  },
  {
    "objectID": "ComplexSurveyNotes.html#loss-of-information-from-sampling-clusters",
    "href": "ComplexSurveyNotes.html#loss-of-information-from-sampling-clusters",
    "title": "Complex Survey Notes",
    "section": "Loss of information from sampling clusters",
    "text": "Loss of information from sampling clusters\nThe loss of precision per observation from cluster sampling is given by the design effect.\n\n“For a single-stage cluster sample with all clusters having the same number of individuals, m, the design effect is\n\n\nD_{eff} = 1 + (m-1)\\rho,\n\n\nwhere \\rho is the within-cluster correlation.\n\nLumley illustrates how design effects can illustrate the impact on inference using the California school data set from before as well as the Behavioral Risk Factor Surveillance System from 2007.\n\nShow the codesvymean(~ api00 + meals + ell + enroll, clus1_design, deff = TRUE)\n\n           mean       SE    DEff\napi00  644.1694  23.5422  9.2531\nmeals   50.5355   6.2690 10.3479\nell     27.6120   2.0193  2.6711\nenroll 549.7158  45.1914  2.7949\n\n\nIn the above, the variance is up to 10 times higher in the cluster sample as compared to a simple random sample.\n\nShow the code## Lumley renames clus2_design to dclus2 from before. I maintain the same names.\nsvymean(~ api00 + meals + ell + enroll, clus2_design, deff = TRUE, na.rm = TRUE)\n\n           mean       SE    DEff\napi00  673.0943  31.0574  6.2833\nmeals   52.1547  10.8368 11.8585\nell     26.0128   5.9533  9.4751\nenroll 526.2626  80.3410  6.1427\n\n\nThese values increase slightly for all measures except api00 in the two stage cluster sampling design. Lumley points out that these large design effects demonstrate how variable the measures of interest are between cluster, suggesting that the sampling of clusters, while efficient economically are not as efficient statistically.\nSimilarly, when computing the proportion of individuals who have more than 5 servings of fruits and vegetables a day (X_FV5SRV = 2), as well as how often individuals received a cholesterol test in the past 5 years (X_CHOLCHK = 1) from the 2007 Behavioral Risk Factor Surveillance System data set, we see design effects that reflect the geographic variability across the blocks of telephone numbers that were sampled for the survey.\n\nShow the codebrfss &lt;- svydesign(\n  id = ~X_PSU, strata = ~X_STATE, weight = ~X_FINALWT,\n  data = \"brfss\", dbtype = \"SQLite\",\n  dbname = \"data/BRFSS/brfss07.db\", nest = TRUE\n)\nbrfss\n\nDB-backed Stratified Independent Sampling design (with replacement)\nsvydesign(id = ~X_PSU, strata = ~X_STATE, weight = ~X_FINALWT, \n    data = \"brfss\", dbtype = \"SQLite\", dbname = \"data/BRFSS/brfss07.db\", \n    nest = TRUE)\n\n\n\nShow the codefood_labels &lt;- c(\"Yes Veg\", \"No Veg\")\nchol_labels &lt;- c(\"within 5 years\", \"&gt;5 years\", \"never\")\nsvymean(\n  ~ factor(X_FV5SRV) +\n    factor(X_CHOLCHK),\n  brfss,\n  deff = TRUE\n)\n\n                         mean         SE   DEff\nfactor(X_FV5SRV)1  0.73096960 0.00153359 5.1632\nfactor(X_FV5SRV)2  0.23844253 0.00145234 5.0147\nfactor(X_FV5SRV)9  0.03058787 0.00069991 7.1323\nfactor(X_CHOLCHK)1 0.73870300 0.00168562 6.3550\nfactor(X_CHOLCHK)2 0.03230828 0.00058759 4.7676\nfactor(X_CHOLCHK)3 0.19989559 0.00162088 7.0918\nfactor(X_CHOLCHK)9 0.02909313 0.00055471 4.7029"
  },
  {
    "objectID": "ComplexSurveyNotes.html#repeated-measurements",
    "href": "ComplexSurveyNotes.html#repeated-measurements",
    "title": "Complex Survey Notes",
    "section": "Repeated Measurements",
    "text": "Repeated Measurements\nLumley notes that design based inference continues to differ from model based in its analysis of repeated measurements. Where model based inference is careful to account for modeling the – for example – within person or within household correlation in a cohort study, no such adjustment is required in a designed survey other than adjusting and using the appropriate weights - treating the repeated measurement like another stage of clustering in the sampling.\nLumley illustrates this with the Survey of Income and Program Participation (SIPP) panel survey.\n\nEach panel is followed for multiple years, with subsets of the panel participating in four month waves of follow-up… wave 1 of the 1996 panel, which followed 36,730 households with interviews every four months, starting in late 1995 or early 1996… The households were recruited in a two-stage sample. The first stage sampled 322 counties or groups of counties as PSUs; the second stage sampled households within these PSUs.\n\nLumley demonstrates how to estimate repeated measures with panel data using the survey package via the code below. Five quantiles are estimated across the population and across the 8 months. When Lumley mentions that there is no need for adjusting for correlation in the block quote above, I believe he is referring to the within-month point estimates. If we were to try and estimate the change in, say, income as a function of other covariates I believe we would want to adjust for correlation in order to get the appropriate standard errors. For the point estimates Lumley points out that the weights are exactly as required for the per-month estimate, but would need to be divided by the number of samples when totaling across the number of measurements. Proportions or regressions are invariant to this scaling factor so no adjustment is needed there.\n\nShow the codesipp_hh &lt;- svydesign(\n  id = ~ghlfsam, strata = ~gvarstr, nest = TRUE,\n  weight = ~whfnwgt, data = \"household\", dbtype = \"SQLite\",\n  dbname = \"Data/SIPP/sipp.db\"\n)\nsipp_hh &lt;- update(sipp_hh,\n  month = factor(rhcalmn,\n    levels = c(12, 1, 2, 3, 4, 5, 6),\n    labels = c(\n      \"Dec\", \"Jan\", \"Feb\", \"Mar\",\n      \"Apr\", \"May\", \"Jun\"\n    )\n  )\n)\nqinc &lt;- svyby(~thtotinc, ~month, svyquantile,\n  design = sipp_hh,\n  quantiles = c(0.25, 0.5, 0.75, 0.9, 0.95), se = TRUE\n)\npltdf &lt;- as_tibble(qinc) %&gt;%\n  select(month, contains(\"thtotinc\"), -contains(\"se\")) %&gt;%\n  gather(everything(), -month, key = \"quantile\", value = \"Total Income\") %&gt;%\n  mutate(quantile = as.numeric(str_extract(quantile, \"[0-9].[0-9]?[0-9]\")) * 100)\n\nse &lt;- as_tibble(qinc) %&gt;%\n  select(month, contains(\"se\")) %&gt;%\n  gather(everything(), -month, key = \"quantile\", value = \"SE\") %&gt;%\n  mutate(quantile = as.numeric(str_extract(quantile, \"[0-9].[0-9]?[0-9]\")) * 100)\n\npltdf &lt;- pltdf %&gt;%\n  left_join(se) %&gt;%\n  mutate(\n    lower = `Total Income` - 2 * SE,\n    upper = `Total Income` + 2 * SE\n  )\n\n\n\nShow the codepltdf %&gt;%\n  mutate(quantile = factor(quantile)) %&gt;%\n  ggplot(aes(x = month, y = `Total Income`, color = quantile)) +\n  geom_pointrange(aes(ymin = lower, ymax = upper)) +\n  xlab(\"Month in 1995/1996\") +\n  ylab(\"Total Income (USD)\") +\n  ggtitle(\"Total Income Quantiles\",\n    subtitle = \"Survey of Income and Program Participation\"\n  )\n\n\n\n\nLumley notes here that we’re only talking about correlation at all here because the same individuals are being measured across time. In a study like NHANES that recruits different individuals each year, we can effectively assume the samples are independent since the overall population is so big.\nEstimation would be much more complicated if the samples overlapped in some way."
  },
  {
    "objectID": "ComplexSurveyNotes.html#exercises-2",
    "href": "ComplexSurveyNotes.html#exercises-2",
    "title": "Complex Survey Notes",
    "section": "Exercises",
    "text": "Exercises\n\nThe web site has data files demo_x.xpt of demographic data and bpx_c.xpt of blood pressure data from NHANES 2003-2004. Code to load and merge these data sets is in Appendix B, in Figure B.1\n\n\nConstruct a survey design object with these data.\n\n\nShow the code## data are from the book website:\n# https://r-survey.r-forge.r-project.org/svybook/\n# demographic data\nddf &lt;- haven::read_xpt(\"data/nhanesxpt/demo_c.xpt\")\n# blood pressure data\nbpdf &lt;- haven::read_xpt(\"data/nhanesxpt/bpx_c.xpt\")\n\nbpdf &lt;- merge(ddf, bpdf, by = \"SEQN\", all = FALSE) %&gt;%\n  mutate(\n    sys_over_140 = (BPXSAR &gt; 140) * 1,\n    dia_over_90 = (BPXDAR &gt; 90) * 1,\n    hbp = (sys_over_140 + dia_over_90 == 2) * 1,\n    age_group = cut(RIDAGEYR, c(0, 21, 65, Inf)),\n    sex = factor(RIAGENDR, levels = 1:2, labels = c(\"Male\", \"Female\"))\n  )\n\nbpdf_design &lt;- bpdf %&gt;%\n  select(\n    sys_over_140, dia_over_90, hbp, WTMEC2YR, SDMVPSU, SDMVSTRA,\n    age_group, sex, RIDAGEYR, BPXSAR, BPXDAR, RIAGENDR, RIDAGEMN,\n  ) %&gt;%\n  as_survey_design(\n    weights = WTMEC2YR,\n    id = SDMVPSU,\n    strata = SDMVSTRA,\n    nest = TRUE\n  )\nbpdf_design\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (30) clusters.\nCalled via srvyr\nSampling variables:\n  - ids: SDMVPSU \n  - strata: SDMVSTRA \n  - weights: WTMEC2YR \nData variables: \n  - sys_over_140 (dbl), dia_over_90 (dbl), hbp (dbl), WTMEC2YR (dbl), SDMVPSU\n    (dbl), SDMVSTRA (dbl), age_group (fct), sex (fct), RIDAGEYR (dbl), BPXSAR\n    (dbl), BPXDAR (dbl), RIAGENDR (dbl), RIDAGEMN (dbl)\n\n\n\nEstimate the proportion of the US population with systolic blood pressure over 140 mm HG, with diastolic blood pressure over 90 mm Hg, and with both.\n\nLumley doesn’t tell us which variable name corresponds to the measurements indicated and I didn’t see any documentation in the files included so I just guess here.\n\nShow the codebpdf_design %&gt;%\n  summarize(\n    prop_hbp_one =\n      survey_ratio(sys_over_140, n(), na.rm = TRUE, vartype = \"ci\", deff = TRUE),\n    prop_hbp_two =\n      survey_ratio(sys_over_140, n(), na.rm = TRUE, vartype = \"ci\", deff = TRUE),\n    prob_hbp = survey_ratio(hbp, n(), na.rm = TRUE, vartype = \"ci\", deff = TRUE)\n  ) %&gt;%\n  gather(everything(), key = \"metric\", value = \"value\")\n\n# A tibble: 12 × 2\n   metric                 value\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 prop_hbp_one      0.0000119 \n 2 prop_hbp_one_low  0.0000104 \n 3 prop_hbp_one_upp  0.0000135 \n 4 prop_hbp_one_deff 3.60      \n 5 prop_hbp_two      0.0000119 \n 6 prop_hbp_two_low  0.0000104 \n 7 prop_hbp_two_upp  0.0000135 \n 8 prop_hbp_two_deff 3.60      \n 9 prob_hbp          0.00000217\n10 prob_hbp_low      0.00000141\n11 prob_hbp_upp      0.00000293\n12 prob_hbp_deff     4.15      \n\n\n\nEstimate the design effects for these proportions.\n\nIncluded in the output above.\n\nHow do these proportions vary by age (RIDAGEYR) and gender (RIAGENDR)\n\n\nShow the codenhanes_ests &lt;- bpdf_design %&gt;%\n  group_by(sex, age_group) %&gt;%\n  summarize(\n    prop_hbp_one =\n      survey_ratio(sys_over_140, n(), na.rm = TRUE, vartype = \"ci\", deff = TRUE),\n    prop_hbp_two =\n      survey_ratio(sys_over_140, n(), na.rm = TRUE, vartype = \"ci\", deff = TRUE),\n    prob_hbp = survey_ratio(hbp, n(), na.rm = TRUE, vartype = \"ci\", deff = TRUE)\n  ) %&gt;%\n  ungroup()\nnhanes_ests\n\n# A tibble: 8 × 14\n  sex    age_group  prop_hbp_one prop_hbp_one_low prop_hbp_one_upp\n  &lt;fct&gt;  &lt;fct&gt;             &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Male   (0,21]      0.000000953     -0.000000551       0.00000246\n2 Male   (21,65]     0.0000647        0.0000486         0.0000809 \n3 Male   (65,Inf]    0.000478         0.000402          0.000554  \n4 Male   &lt;NA&gt;      NaN              NaN               NaN         \n5 Female (0,21]      0.000000660     -0.000000521       0.00000184\n6 Female (21,65]     0.0000597        0.0000518         0.0000677 \n7 Female (65,Inf]    0.000738         0.000655          0.000820  \n8 Female &lt;NA&gt;      NaN              NaN               NaN         \n# ℹ 9 more variables: prop_hbp_one_deff &lt;dbl&gt;, prop_hbp_two &lt;dbl&gt;,\n#   prop_hbp_two_low &lt;dbl&gt;, prop_hbp_two_upp &lt;dbl&gt;, prop_hbp_two_deff &lt;dbl&gt;,\n#   prob_hbp &lt;dbl&gt;, prob_hbp_low &lt;dbl&gt;, prob_hbp_upp &lt;dbl&gt;, prob_hbp_deff &lt;dbl&gt;\n\n\nThis produces a bunch of output, so I plot the high blood pressure result — both high systolic and diastolic blood pressure – below.\n\nShow the codenhanes_ests %&gt;%\n  select(age_group, sex, prob_hbp, prob_hbp_low, prob_hbp_upp) %&gt;%\n  filter(!is.na(age_group)) %&gt;%\n  ggplot(aes(x = age_group, y = prob_hbp, color = sex)) +\n  geom_pointrange(aes(ymin = prob_hbp_low, ymax = prob_hbp_upp),\n    position = position_dodge(width = .25)\n  ) +\n  ylab(\"% US Population with High Blood Pressure\") +\n  xlab(\"Age Group\") +\n  ggtitle(\"Prevalence of High Blood Pressure in U.S. Across Age and Sex\",\n    subtitle = \"Data from NHANES 2003-2004\"\n  ) +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\nRepeat the sampling and estimation in Figure 3.4 1000 times.\n\nFYI the sampling::UPtille function takes a while so running this simulation 1000 times takes a while.\n\nShow the codeOneSimulation &lt;- function() {\n  insample &lt;- sampling::UPtille(election$p)\n  ppsample &lt;- election[insample == 1, ]\n  ppsample$wt &lt;- 1 / ppsample$p\n  pps_design &lt;- svydesign(id = ~1, weight = ~wt, data = ppsample, pps = \"brewer\")\n  total &lt;- svytotal(~ Bush + Kerry + Nader, pps_design)\n  std_err &lt;- sqrt(diag(attr(total, \"var\")))\n  ci &lt;- confint(total)\n  out &lt;- cbind(total, std_err, ci)\n}\ntotal_sims &lt;- replicate(10, OneSimulation())\n\n\n\nCheck that the mean of the estimated totals is close to the true population totals.\n\n\nShow the codeestimated_totals &lt;- as_tibble(t(rowMeans(total_sims[, 1, ]))) %&gt;%\n  mutate(label = \"Estimate\")\n\ntrue_totals &lt;- as_tibble(t(colSums(election[, c(\"Bush\", \"Kerry\", \"Nader\")]))) %&gt;%\n  mutate(label = \"Truth\")\n\nrbind(estimated_totals, true_totals) %&gt;%\n  gather(everything(), -label, key = \"Candidate\", value = \"Vote Count\") %&gt;%\n  spread(label, `Vote Count`) %&gt;%\n  gt::gt() %&gt;%\n  gt::fmt_scientific() %&gt;%\n  gt::tab_header(\"Simulated Total Comparison\")\n\n\n\n\n\n\n\nSimulated Total Comparison\n    \n\nCandidate\n      Estimate\n      Truth\n    \n\n\n\nBush\n5.98 × 107\n\n5.96 × 107\n\n\n\nKerry\n5.61 × 107\n\n5.61 × 107\n\n\n\nNader\n3.82 × 105\n\n4.04 × 105\n\n\n\n\n\n\n\nThese are quite close.\n\nCompute the mean of the estimated standard errors and compare it to the true simulation standard error, that is, the standard deviation of the estimated totals.\n\n\nShow the codeestimated_stderrs &lt;- as_tibble(t(rowMeans(total_sims[, 2, ]))) %&gt;%\n  mutate(label = \"Estimate\")\n\ntrue_stderr &lt;- as_tibble(apply(total_sims, 1, sd)) %&gt;%\n  mutate(label = \"Truth\", Candidate = c(\"Bush\", \"Kerry\", \"Nader\")) %&gt;%\n  spread(Candidate, value)\n\nrbind(estimated_stderrs, true_stderr) %&gt;%\n  gather(everything(), -label, key = \"Candidate\", value = \"Vote Std.Err\") %&gt;%\n  spread(label, `Vote Std.Err`) %&gt;%\n  gt::gt() %&gt;%\n  gt::fmt_scientific() %&gt;%\n  gt::tab_header(\"Simulated Standard Error Comparison\")\n\n\nSame as above.\n\nEstimate 95% confidence intervals for the population totals and compute the proportion of intervals that contain the true population value.\n\n\nShow the codetotals &lt;- unlist(true_totals[, 1:3])\nprop_contained &lt;- rowMeans((total_sims[, 3, ] &lt; totals &\n  total_sims[, 4, ] &gt; totals) * 1)\nprop_contained\n\n\nWe see that the proportion contained by the interval is near the 95% nominal value.\n\nThe National Longitudinal Study of Youth is documented at www.nlsinfo.org/nlsy97/nlsdocs/nlsy97/maintoc.html\n\n\nWhat are the stages of sampling and the sampling units at each stage?\n\nFrom the website the sampling occurred in 2 phases (see image below).\n\nThe first phase screened households and the second identified eligible respondents. I think this would technically be called a two phase sample (discussed in chapter 8) but given the population sizes involved it may be that there was no need to account for the potential dependence here.\nThe sampling unit in the first stage came from NORC’s 1990 sampling design which used Standard Metropolitan Statistical Areas or non-metropolitan counties. That is, according to the general social survey documentation\nSubsequent sampling units are segments, households, and then members of households.\n\nWhat would the strata and PSUs be for the single stage approximation to the design?\n\nI don’t know. It isn’t clear how to reconstruct Lumley’s example given that the data is not available. Looking at the NHIS website it looks like the strata and PSU information for a single stage approximation are given. When looking for the same information for the NLYS data I don’t see equivalent instructions on how to construct a single stage approximation. His description in the book here isn’t very forthcoming either but I’d guess that one could just concatenate the PSU and SSU labels and the same for the strata and then use those in a one stage design. This doesn’t look exactly like what Lumley does in his example.\n\nThe British Household Panel Survey (BHPS) is documented at http://www.iser.essex.ac.uk/survey/bhps (this website is no longer accurate) .\n\n\nWhat are the stages of sampling, strata and the sampling units at each stage?\n\nThe website given is no longer accurate. Clicking through the documentation of the “Understanding Society” website it looks like the BHPS has been combined with other surveys. When I look at more documentation, (1, 2, 3), the last of these being the most pertinent, I see a variety of designs by wave. It looks like the design was a two stage stratified sample where the sampling frame was the Postcode Address File for Great Britian, excluding Northern Ireland. 250 postcodes were chosen as the primary sampling unit, in the second stage, “delivery points” which are roughly equivalent to addresses were then selected.\n\nWhat would the strata and PSUs be for the single stage approximation to the design?\n\nAgain, my best guess is to concatenate the postcodes and delivery addresses to construct the single stage identifiers, and the same for the strata.\n\nStatistics New Zealand lists survey topics at http://www.stats.govt.nz/datasets/a-z-list.htm . Find the Household Labour Force Survey.\n\n\nWhat are the stages of sampling and the sampling units at each stage?\n\nAgain, that site is no longer valid. The current help page for the household labor survey simply says,\n\nApproximately fifteen thousand (15,000) households take part in this survey every three months. A house is selected using a fair statistical method to ensure the sample is an accurate representation of New Zealand. Every person aged 15 years or over living in a selected house needs to take part.\n\nA different doc I found on Google listed the 2016 survey design as a 2 stage design where samples are drawn from strata in the first stage. The PSU’s are “meshblocks” which appear to be the NZ equivalent of a U.S. census block / tract.\n\nWhat would the strata and PSUs be for the single stage approximation to the design?\n\nSame answer here as previously.\n\nThis exercise uses the Washington State Crime data for 2004 as the population. The data consists of crime rates and population size for the police districts (in cities/towns) and sheriffs’ offices (in unincorporated areas), grouped by county.\n\nI took the data from this website. Specifically, this excel sheet. One of the tricky things about using this data was the fact that several police districts are reported as having zero associated population. I removed these from the data set to make things simpler.\n\nTake a simple random sample of 10 counties from the state and use all the data from the sampled counties. Estimate the total number of murders and burglaries in the state.\n\n\nShow the code# data from https://www.waspc.org/cjis-statistics---reports)\nwa_crime_df &lt;- readxl::read_xlsx(\"data/WA_crime/1984-2011.xlsx\", skip = 4) %&gt;%\n  filter(Year == \"2004\", Population &gt; 0) %&gt;%\n  mutate(\n    murder = `Murder Total`,\n    murder_and_crime = `Murder Total` + `Burglary Total`,\n    violent_crime = `Violent Crime Total`,\n    burglaries = `Burglary Total`,\n    property_crime = `Property Crime Total`,\n    state_pop = sum(Population),\n    County = stringr::str_to_lower(County),\n    num_counties = n_distinct(County),\n  ) %&gt;%\n  group_by(County) %&gt;%\n  mutate(num_agencies = n_distinct(Agency)) %&gt;%\n  ungroup() %&gt;%\n  select(\n    County, Agency, Population, murder_and_crime, murder, violent_crime,\n    property_crime, burglaries, num_counties, num_agencies\n  )\n\ntrue_count &lt;- sum(wa_crime_df$murder_and_crime)\ncounty_list &lt;- unique(wa_crime_df$County)\ncounty_sample &lt;- sample(county_list, 10)\npart_a &lt;- wa_crime_df %&gt;%\n  filter(County %in% county_sample) %&gt;%\n  as_survey_design(\n    ids = c(County, Agency),\n    fpc = c(num_counties, num_agencies)\n  ) %&gt;%\n  summarize(total = survey_total(murder_and_crime)) %&gt;%\n  mutate(Q = \"a\")\npart_a\n\n# A tibble: 1 × 3\n   total total_se Q    \n   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n1 52982.   26039. a    \n\n\n\nStratify the sampling so that King County is sampled with 100% probability together with a simple random sample of five other counties. Estimate the total number of of murders and burglaries in the state and compare to the previous estimates.\n\n\nShow the codecounty_sample &lt;- sample(county_list[-which(county_list == \"king\")], 5)\ncounty_sample &lt;- c(county_sample, \"king\")\npart_b &lt;- wa_crime_df %&gt;%\n  filter(County %in% county_sample) %&gt;%\n  mutate(\n    strata_label = if_else(County == \"king\", \"strata 1\", \"strata 2\"),\n    num_counties = if_else(County == \"king\", 1, length(county_list) - 1)\n  ) %&gt;%\n  as_survey_design(\n    ids = c(County, Agency),\n    fpc = c(num_counties, num_agencies),\n    strata = strata_label\n  ) %&gt;%\n  summarize(\n    total = survey_total(murder_and_crime)\n  ) %&gt;%\n  mutate(Q = \"b\")\npart_b\n\n# A tibble: 1 × 3\n   total total_se Q    \n   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n1 92233.   55064. b    \n\n\n\nTake simple random samples of five police districts from King County and five counties from the rest of the state. Estimate the total number of murders and burglaries in the state and compare to the previous estimates.\n\nThis is a stratified two-stage sample design with no uncertainty in the first stage in one (king county) strata, and no uncertainty in the second stage in the other (non-king counties) strata.\n\nShow the codeking_districts &lt;- wa_crime_df %&gt;%\n  filter(County == \"king\") %&gt;%\n  pull(Agency)\nsampled_king_districts &lt;- sample(king_districts, 5)\nsampled_counties &lt;- sample(county_list, 5)\n\npart_c &lt;- wa_crime_df %&gt;%\n  filter(County %in% sampled_counties | Agency %in% sampled_king_districts) %&gt;%\n  mutate(\n    strata_label = if_else(County == \"king\", \"King County\", \"WA Counties\"),\n    num_counties = if_else(County == \"king\", 1, length(county_list) - 1),\n  ) %&gt;%\n  as_survey_design(\n    id = c(County, Agency),\n    fpc = c(num_counties, num_agencies),\n    strata = strata_label\n  ) %&gt;%\n  summarize(total = survey_total(murder_and_crime, vartype = \"se\")) %&gt;%\n  mutate(Q = \"c\")\npart_c\n\n# A tibble: 1 × 3\n  total total_se Q    \n  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n1 24160    2530. c    \n\n\n\nTake a probability proportional to size (PPS) sample of 10 police districts. Estimate the total number of murders and burglaries in the state and compare to the previous estimates.\n\n\nShow the codepi &lt;- sampling::inclusionprobabilities(a = wa_crime_df$Population, n = 10)\npart_d &lt;- wa_crime_df %&gt;%\n  mutate(\n    pi = pi,\n    in_sample = sampling::UPbrewer(pi)\n  ) %&gt;%\n  filter(in_sample == 1) %&gt;%\n  as_survey_design(probs = pi) %&gt;%\n  summarize(total = survey_total(murder_and_crime)) %&gt;%\n  mutate(Q = \"d\")\npart_d\n\n# A tibble: 1 × 3\n   total total_se Q    \n   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n1 47256.    5853. d    \n\n\nThis has the lowest standard error yet, likely since the sampling was specifically designed to capture high density districts.\n\nTake a simple random sample of counties and include all the police districts. Estimate the total number of murders and burglaries in the state and compare to the previous estimates.\n\n\nShow the codecounty_sample &lt;- sample(county_list, 10)\npart_e &lt;- wa_crime_df %&gt;%\n  filter(County %in% county_sample) %&gt;%\n  as_survey_design(\n    ids = c(County, Agency),\n    fpc = c(num_counties, num_agencies)\n  ) %&gt;%\n  summarize(total = survey_total(murder_and_crime)) %&gt;%\n  mutate(Q = \"e\")\npart_e\n\n# A tibble: 1 × 3\n   total total_se Q    \n   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n1 38594.   16828. e    \n\n\nI’ll compare the standard error of all the estimates now.\n\nShow the coderbind(part_a, part_b, part_c, part_d, part_e) %&gt;%\n  select(Q, total_se) %&gt;%\n  gt::gt()\n\n\n\n\n\n\nQ\n      total_se\n    \n\n\na\n26038.557\n\n\nb\n55064.174\n\n\nc\n2530.232\n\n\nd\n5852.756\n\n\ne\n16828.106\n\n\n\n\n\n\nWe see the highest variance in estimates from the simple random samples - Q’s a) and e). We see the lowest variance from part d) which used the proportional to size sampling scheme. Finally, we see the two stratified multi-stage samples have variances in between, all of which makes sense.\n\nUse the household data from the 1966 SIP panel to estimate the 25th, 50th, 75th, 90th and 95th percentiles of income for households of different sizes (ehhnumpp) averaged over the fourth months. You will want to re-code the large values of ehhnumpp to a a single category. Describe the patterns you see.\n\nFor this question we effectively copy the code from the “Repeated Measures” section, looking at quantile by a re-coded household size now, instead of month.\n\nShow the codesipp_hh &lt;- update(sipp_hh,\n  household_size = factor(\n    case_when(\n      ehhnumpp &lt;= 8 ~ as.character(ehhnumpp),\n      TRUE ~ \"&gt;=9\"\n    ),\n    levels = c(as.character(1:8), \"&gt;=9\")\n  )\n)\nqinc &lt;- svyby(~thtotinc, ~household_size, svyquantile,\n  design = sipp_hh,\n  quantiles = c(0.25, 0.5, 0.75, 0.9, 0.95), se = TRUE\n)\n\n\n\nShow the codepltdf &lt;- as_tibble(qinc) %&gt;%\n  select(household_size, contains(\"thtotinc\"), -contains(\"se.\")) %&gt;%\n  gather(everything(), -household_size, key = \"quantile\", value = \"Total Income\") %&gt;%\n  mutate(quantile = as.numeric(str_extract(quantile, \"[0-9].[0-9]?[0-9]\")) * 100)\n\nse &lt;- as_tibble(qinc) %&gt;%\n  select(household_size, contains(\"se.\")) %&gt;%\n  gather(everything(), -household_size, key = \"quantile\", value = \"SE\") %&gt;%\n  mutate(quantile = as.numeric(str_extract(quantile, \"[0-9].[0-9]?[0-9]\")) * 100)\n\npltdf &lt;- pltdf %&gt;%\n  left_join(se) %&gt;%\n  mutate(\n    lower = `Total Income` - 2 * SE,\n    upper = `Total Income` + 2 * SE\n  )\n\npltdf %&gt;%\n  mutate(quantile = factor(quantile)) %&gt;%\n  ggplot(aes(x = household_size, y = `Total Income`, color = quantile)) +\n  geom_pointrange(aes(ymin = lower, ymax = upper)) +\n  xlab(\"Household Size\") +\n  ylab(\"Total Income (USD)\") +\n  ggtitle(\"Total Income Quantiles\",\n    subtitle = \"Survey of Income and Program Participation\"\n  )\n\n\n\n\nIn the plot above we can see that the income variability gets wider as household size increases but then plateaus at around ~ 5. Additionally, there are few samples with very large households so estimating the quantiles for those groups is increasingly noisy.\n\nIn the data from the 1996 SIPP panel\n\n\nWhat proportion of households received any “means-tested cash benefits” (thtrninc)? For those households that did receive benefits what mean proportion of their total income came from these benefits?\n\n\nShow the codedb &lt;- DBI::dbConnect(RSQLite::SQLite(), \"Data/SIPP/sipp.db\")\nsipp_df &lt;- tbl(db, sql(\"SELECT * FROM household\")) %&gt;%\n  dplyr::collect() %&gt;%\n  select(ghlfsam, gvarstr, whfnwgt, thtotinc, thtrninc, tmthrnt) %&gt;%\n  mutate(\n    benefit_recipient = I(thtrninc &gt; 0) * 1,\n    thtrninc = as.numeric(thtrninc),\n    tmthrnt = as.numeric(tmthrnt)\n  )\nDBI::dbDisconnect(db)\n\nsipp_hh_sub &lt;- sipp_df %&gt;%\n  as_survey_design(\n    id = ghlfsam, strata = gvarstr, nest = TRUE,\n    weight = whfnwgt\n  )\nsipp_hh_sub %&gt;%\n  summarize(prop_benefit_recipients = survey_mean(benefit_recipient))\n\n# A tibble: 1 × 2\n  prop_benefit_recipients prop_benefit_recipients_se\n                    &lt;dbl&gt;                      &lt;dbl&gt;\n1                  0.0829                    0.00174\n\n\n\nShow the codesipp_hh_sub %&gt;%\n  filter(benefit_recipient == 1) %&gt;%\n  mutate(prop_benefit_income = thtrninc / thtotinc) %&gt;%\n  summarize(\n    mn_prop_benefit_income = survey_mean(prop_benefit_income)\n  )\n\n# A tibble: 1 × 2\n  mn_prop_benefit_income mn_prop_benefit_income_se\n                   &lt;dbl&gt;                     &lt;dbl&gt;\n1                  0.497                   0.00710\n\n\n\nWhat proportion of households paid rent tmthrnt? What were the mean and the 75th and 95th percentiles of the proportion of monthly income paid in rent? What proportion paid more than one third of their income in rent?\n\n\nShow the codesipp_hh_sub %&gt;%\n  mutate(\n    pays_rent = I(as.numeric(tmthrnt) &gt; 0) * 1,\n    pct_income_rent = (tmthrnt / thtotinc) * 100,\n    rent_pct_gt_thrd = (pct_income_rent &gt; 0.33) * 1\n  ) %&gt;%\n  # avoid divide by zero error\n  filter(thtotinc &gt; 0) %&gt;%\n  summarize(\n    prop_pays_rent = survey_mean(pays_rent),\n    mean_pct_income_rent = survey_mean(pct_income_rent, na.rm = TRUE),\n    quantile_pct_income_rent = survey_quantile(pct_income_rent,\n      na.rm = TRUE,\n      quantiles = c(.75, .95)\n    ),\n    prop_rent_gt_thrd = survey_mean(rent_pct_gt_thrd, na.rm = TRUE)\n  ) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Rent\", values_to = \"Estimates\")\n\n# A tibble: 10 × 2\n   Rent                            Estimates\n   &lt;chr&gt;                               &lt;dbl&gt;\n 1 prop_pays_rent                    0.0401 \n 2 prop_pays_rent_se                 0.00111\n 3 mean_pct_income_rent              2.24   \n 4 mean_pct_income_rent_se           0.633  \n 5 quantile_pct_income_rent_q75      0      \n 6 quantile_pct_income_rent_q95      0      \n 7 quantile_pct_income_rent_q75_se   1.42   \n 8 quantile_pct_income_rent_q95_se   1.42   \n 9 prop_rent_gt_thrd                 0.0400 \n10 prop_rent_gt_thrd_se              0.00111\n\n\n\nBy the time you read this, the survey package is likely to provide some approximations for PPS designs that use the finite population size information. Repeat 3.2 using these.\n\n\nShow the codeOneSimulation &lt;- function() {\n  insample &lt;- sampling::UPtille(election$p)\n  ppsample &lt;- election[insample == 1, ]\n  ppsample$wt &lt;- 1 / ppsample$p\n  pps_design &lt;- svydesign(id = ~1, weight = ~wt, data = ppsample, pps = \"brewer\")\n  total &lt;- svytotal(~ Bush + Kerry + Nader, pps_design)\n  std_err &lt;- sqrt(diag(attr(total, \"var\")))\n  ci &lt;- confint(total)\n  out &lt;- cbind(total, std_err, ci)\n}\ntotal_sims &lt;- replicate(100, OneSimulation())\n\n\n\nCheck that the mean of the estimated totals is close to the true population totals.\n\n\nShow the codeOneSimulation &lt;- function() {\n  insample &lt;- sampling::UPtille(election$p)\n  ppsample &lt;- election[insample == 1, ]\n  ppsample$wt &lt;- 1 / ppsample$p\n  ppsample$fpc &lt;- 40 / sum(election$votes)\n  pps_design &lt;- svydesign(id = ~1, weight = ~wt, fpc = ~fpc, data = ppsample, pps = \"brewer\")\n  total &lt;- svytotal(~ Bush + Kerry + Nader, pps_design)\n  std_err &lt;- sqrt(diag(attr(total, \"var\")))\n  ci &lt;- confint(total)\n  out &lt;- cbind(total, std_err, ci)\n}\ntotal_sims &lt;- replicate(100, OneSimulation())\n\n\n\nShow the codeestimated_totals &lt;- as_tibble(t(rowMeans(total_sims[, 1, ]))) %&gt;%\n  mutate(label = \"Estimate\")\n\ntrue_totals &lt;- as_tibble(t(colSums(election[, c(\"Bush\", \"Kerry\", \"Nader\")]))) %&gt;%\n  mutate(label = \"Truth\")\n\nrbind(estimated_totals, true_totals) %&gt;%\n  gather(everything(), -label, key = \"Candidate\", value = \"Vote Count\") %&gt;%\n  spread(label, `Vote Count`) %&gt;%\n  gt::gt() %&gt;%\n  gt::fmt_scientific() %&gt;%\n  gt::tab_header(\"Simulated Total Comparison\")\n\n\n\n\n\n\n\nSimulated Total Comparison\n    \n\nCandidate\n      Estimate\n      Truth\n    \n\n\n\nBush\n5.97 × 107\n\n5.96 × 107\n\n\n\nKerry\n5.61 × 107\n\n5.61 × 107\n\n\n\nNader\n4.06 × 105\n\n4.04 × 105\n\n\n\n\n\n\n\nThese are quite close.\n\nCompute the mean of the estimated standard errors and compare it to the true simulation standard error, that is, the standard deviation of the estimated totals.\n\n\nShow the codeestimated_stderrs &lt;- as_tibble(t(rowMeans(total_sims[, 2, ]))) %&gt;%\n  mutate(label = \"Estimate\")\n\ntrue_stderr &lt;- as_tibble(apply(total_sims, 1, sd)) %&gt;%\n  mutate(label = \"Truth\", Candidate = c(\"Bush\", \"Kerry\", \"Nader\")) %&gt;%\n  spread(Candidate, value)\n\nrbind(estimated_stderrs, true_stderr) %&gt;%\n  gather(everything(), -label, key = \"Candidate\", value = \"Vote Std.Err\") %&gt;%\n  spread(label, `Vote Std.Err`) %&gt;%\n  gt::gt() %&gt;%\n  gt::fmt_scientific() %&gt;%\n  gt::tab_header(\"Simulated Standard Error Comparison\")\n\n\n\n\n\n\n\nSimulated Standard Error Comparison\n    \n\nCandidate\n      Estimate\n      Truth\n    \n\n\n\nBush\n2.49 × 106\n\n2.51 × 107\n\n\n\nKerry\n2.48 × 106\n\n2.36 × 107\n\n\n\nNader\n8.37 × 104\n\n1.96 × 105\n\n\n\n\n\n\n\n\nEstimate 95% confidence intervals for the population totals and compute the proportion of intervals that contain the true population value.\n\n\nShow the codetotals &lt;- unlist(true_totals[, 1:3])\nprop_contained &lt;- rowMeans((total_sims[, 3, ] &lt; totals &\n  total_sims[, 4, ] &gt; totals) * 1)\nprop_contained\n\n Bush Kerry Nader \n 0.97  0.97  0.97 \n\n\n\nRepeat 3.2 computing the Hartley-Rao approximation and the full Horvitz-Thompson estimate using the code and joint-probability data on the web site. Compare the standard errors from these two approximations to the standard errors from the single-stage with replacement approximation and to the true simulation standard error.\n\nNot clear to me where this code is when looking on his website.\n\nSince 1999, NHANES has been conducting surveys continuously with data released on a 2 year cycle. Each data set includes a weight variable for analyzing the two-year data; the weights add to the size of the US adult, civilian, non-institutionalized population.\n\n\nWhat weight would be appropriate for estimating the number of diabetics in the population combining data from two two-year data sets?\n\nAssuming these are non-overlapping samples — which seems reasonable — and considering the target population we’re generalizing to be roughly constant over the 4 years of interest, we could divide each of the previous weights in half. This would give us twice the sample size to estimate the any target estimand considering the four year population as roughly homogeneous.\n\nWhat weight would be appropriate if three two-year data sets were used?\n\nDivide the weights by 3.\n\nWhat weights would be appropriate when estimating changes, comparing the combined 1999-2000 and 2001-2002 data with the combined 2003-2004 and 2005-2006 data?\n\nYou’d need to use the original weights and set these up as different strata. Or at least, that’s the most intuitive answer to me.\n\nHow would the answers differ if the goal was to estimate a population proportion rather than a total?\n\nIf estimating a population total the weights would need to be adjusted further such that the population total of each sample remained the same and the two samples added up to the combined population from each sample – this would be more complicated than just dividing by two as suggested above. For proportions this would matter less when using the simpler method above, assuming no dramatic change in the total population, but could still differ slightly if using the total adjusted weights."
  },
  {
    "objectID": "ComplexSurveyNotes.html#plotting-a-table",
    "href": "ComplexSurveyNotes.html#plotting-a-table",
    "title": "Complex Survey Notes",
    "section": "Plotting a Table",
    "text": "Plotting a Table\nLumley recommends using bar charts, forest plots and fourfold plots to visualize the data from a table. I would agree that all of these are “fine” in a certain context but strongly prefer the forest plot and variations on it personally, since it does more than all the other to include representations of uncertainty about the tabulated estimates.\n\n\nBarplot\nForest Plot\n\n\n\n\nShow the codemedians %&gt;%\n  ggplot(aes(x = racehpr, y = Median_BMI, fill = srsex)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  ylab(\"Median BMI (kg/m^2)\") +\n  xlab(\"\") +\n  theme(\n    legend.title = element_blank(),\n    axis.text.x = element_text(angle = 45, vjust = 0.25, hjust = .25)\n  ) +\n  ggtitle(\"Median BMI Across Race, Sex\")\n\n\n\n\n\n\n\nShow the codemedians %&gt;%\n  ggplot(aes(x = Median_BMI, y = racehpr, color = srsex)) +\n  geom_pointrange(aes(xmin = Median_BMI_low, xmax = Median_BMI_upp),\n    position = position_dodge(width = .25)\n  ) +\n  geom_vline(aes(xintercept = 25), linetype = 2, color = \"red\") +\n  xlab(\"BMI\") +\n  ylab(\"\") +\n  theme(legend.title = element_blank(), legend.position = \"top\") +\n  ggtitle(\"Median BMI by Race/Ethnicity and gender, from CHIS\") +\n  labs(caption = str_c(\n    \"Line indicates CDC border between healthy and \",\n    \"overweight BMI.\"\n  ))"
  },
  {
    "objectID": "ComplexSurveyNotes.html#one-continuous-variable",
    "href": "ComplexSurveyNotes.html#one-continuous-variable",
    "title": "Complex Survey Notes",
    "section": "One Continuous Variable",
    "text": "One Continuous Variable\nLumley argues that the most straightforward graphical displays for a single continuous variable are boxplots, cumulative distribution function (cdf) plots and survival curves. I think histograms are also useful but we’ll get to that shortly.\nBelow I reproduce Figure 4.6 from the text in which Lumley demonstrates the difference between the weighted and unweighted CDF’s of the California school data demonstrating that, indeed, the design based estimate is closer to the population value than the unweighted, naive estimate of the stratified design. This should all intuitively make sense. My only complaint would be that the uncertainty of the cdf isn’t visualized when that should be just as easily accessible. I made a cursory investigation to see if these data were available in the svycdf object but they were not.\n\nShow the codecdf.est &lt;- svycdf(~ enroll + api00 + api99, dstrata)\n\ncdf.pop &lt;- ecdf(apipop$enroll)\n\ncdf.samp &lt;- ecdf(apistrat$enroll)\n\npar(mar = c(4.1, 4.1, 1.1, 2.1))\n\nplot(cdf.pop,\n  do.points = FALSE, xlab = \"enrollment\",\n  ylab = \"Cumulative probability\",\n  main = \"Cumulative Distribution of California School Size\",\n  sub = \"Reproduces Lumley Fig 4.6\", lwd = 1\n)\n\nlines(cdf.samp, do.points = FALSE, lwd = 2)\nlines(cdf.est[[1]],\n  lwd = 2, col.vert = \"grey60\", col.hor = \"grey60\",\n  do.points = FALSE\n)\nlegend(\"bottomright\",\n  lwd = c(1, 2, 2), bty = \"n\",\n  col = c(\"black\", \"grey60\", \"black\"),\n  legend = c(\"Population\", \"Weighted estimate\", \"Unweighted Estimate\")\n)\n\n\n\n\nNext Lumley plots an adjusted kaplan meier curve using svykm() on the National Wilms Tumor Study data. Unfortunately he doesn’t show how to create the design object and I find no mention of it elsewhere in the book. I made a guess and it seems to be accurate. Note that I obtained the national wilms tumor study data from the survival package.\n\nShow the codedcchs &lt;- svydesign(ids = ~1, data = nwtco)\nscurves &lt;- svykm(Surv(edrel / 365.25, rel) ~ histol, dcchs)\nplot(scurves)\n\n\n\n\nThe interpretation of this graph is that relapse (into cancer) appears to occur within about three years, if at all. The top line shows the survival for those with a favorable histology classification with an even better survival rate.\nLumley makes a note here describing how the sampling weights make a large difference in the estimate because the design is a case cohort sample. That being said, I don’t really see the difference when I compare his figure to mine created with an equal probability sample… An error here perhaps.\nBoxplots\nLumley describes the typical box plots as\n\nbased on the quartiles of the data: the box shows the median and first and third quartiles, the whiskers extend out to the last observation within 1.5 interquartile ranges of the box ends, and all points beyond the whiskers are shown individually.\n\nThis is as good a summary as any, I’ll only note that the median is the line within the interior of the box and the first and third quartile define the boundaries of the box. It’s (somewhat) easy to image how Lumley estimates these values, as he has already introduced the svyquantile() function — this should be a straightforward application of that function.\nI reproduce Figure 4.10 from the text using the nhanes data and design object created in answering the questions for chapter 3. His code is equivalent.\n\nShow the code## Lumley's code\n# nhanes &lt;- svydesign(id = ~SDMVPSU, strat = ~SDMVSTRA,\n#                    weights = ~WTMEC2YR, data = both, nest = TRUE)\n\n## I use the bpdf design from the chapter 3 questions\nnhanes &lt;- bpdf_design %&gt;%\n  mutate(\n    age_group = cut(RIDAGEYR, c(0, 20, 30, 40, 50, 60, 80, Inf)),\n  )\nsvyboxplot(BPXSAR ~ age_group, subset(nhanes, BPXSAR &gt; 0),\n  col = \"gray80\",\n  varwidth = TRUE, ylab = \"Systolic BP (mmHg)\", xlab = \"Age\"\n)\n\n\n\n\nThe plot shows the distribution in blood pressure across the different age groups. I already showed how I visualized this previously, but it is nice to know how to do something multiple ways. In general, our observation shows an increase in systolic blood pressure medians and third quartiles across age.\nGraphs based on the density\nHistograms and kernel density estimators both visualize the probability distribution function(pdf) of a given data set. Confusingly, perhaps, there is no pdf to estimate in finite population inference because the sampling is random instead of the data. Still, the analogous effort can be made where a given proportion is estimated for some number of bins of the variable of interest. Instead of estimating a naive proportion, there’s now a sample based proportion. I reproduce Lumley’s Figure 4.12 below demonstrating how this works.\n\nShow the codesvyhist(~BPXSAR, subset(nhanes, RIDAGEYR &gt; 20 & BPXSAR &gt; 0),\n  main = \"\",\n  col = \"grey80\", xlab = \"Systolic BP (mmHg)\"\n)\nlines(svysmooth(~BPXSAR, nhanes,\n  bandwidth = 5,\n  subset = subset(nhanes, RIDAGEYR &gt; 20 & BPXSAR &gt; 0)\n), lwd = 2)\n\n\n\n\nThis looks considerably different then the naive estimate, again showcasing the need for taking the design into account.\n\nShow the codehist(bpdf_design$variables$BPXSAR,\n  xlab = \"Systolic BP (mmHg)\",\n  main = \"Naive Distribution of Blood Pressure\"\n)\n\n\n\n\nTwo Continuous Variables\nWhere the design could be used to adjust the raw sample summaries to adjust for the design with one variable, two variable visualizations make this much more difficult as there is no way to incorporate the design information into the presented estimate. An additional dimension has to be used to illustrate the design information.\nScatterplots\nTo that end, Lumley uses a bubble plot with the size of the “bubbles” proportional to the sampling weight. This way, the viewer can identify which points should be considered as “more” or less impactful in terms of representing the population. I reproduce part of Figures 4.13 - 4.15 from the text below, for those plots in which Lumley includes the code. plots the systolic and diastolic blood pressure from the NHANES design.\n\nShow the code# TODO(petersonadam): Try plotting the api data here too\npar(mfrow = c(1, 2))\nsvyplot(BPXSAR ~ BPXDAR,\n  design = nhanes, style = \"bubble\",\n  xlab = \"Diastolic pressure (mmHg)\",\n  ylab = \"Systolic pressure (mmHg)\"\n)\nsvyplot(BPXSAR ~ BPXDAR,\n  design = nhanes, style = \"transparent\",\n  pch = 19, alpha = c(0, 0.5),\n  xlab = \"Diastolic pressure (mmHg)\",\n  ylab = \"Systolic pressure (mmHg)\"\n)\n\n\n\n\nAs Lumley notes, this is a case in which the sampling weights — bubble size — is not particularly informative, as we don’t see any particularly obvious pattern relating the size of the bubbles to the two variables of interest. Turning to the shading plot, though we can now see more than just the “blob” in the first plot, there’s still no apparent pattern between the bubble size and the two variables to identify.\nBelow, I include Lumley’s “sub-sample” method, which draws a simple random sample from the target population by using the provided sampling probabilities. Lumley recommends looking at 2 or three iterations of a sub-sample plot in order to ensure that any features visualized are not “noise” from the sampling process.\n\nShow the codesvyplot(BPXSAR ~ BPXDAR,\n  design = nhanes, style = \"subsample\",\n  sample.size = 1000,\n  xlab = \"Diastolic pressure (mmHg)\",\n  ylab = \"Systolic pressure (mmHg)\"\n)\n\n\n\n\nAggregation and smoothing\nWhile less of an issue than it was at the time of Lumley’s writing, aggregating and smoothing across points makes it easier to condense the number of sample points in a visualization to reduce the memory required to visualize all the points in the sample. In a design setting it also provides an opportunity to incorporate the design information into a visualization specific estimate.\nLumley’s first example of this is a hexplot - a plot where a grid of hexagons is created and sized according to the number of points within the hex-bin.\n\nShow the codesvyplot(BPXSAR ~ BPXDAR,\n  design = nhanes, style = \"hex\", legend = 0,\n  xlab = \"Diastolic pressure (mmHg)\",\n  ylab = \"Systolic pressure (mmHg)\"\n)\n\n\n\n\nThis is perhaps the best visualization of the data thus far, where the “blob” is still apparent but outliers are still visible and a more concise summary of the data is easily visible.\nScatterplot smoothers\nLumley next describes how to take the discrete estimates examining blood pressure as a function of age and smooth them when both variables are continuous. The code below shows how to do this by using the svyplot() function using quantile regression methods from the quantreg package. Mean estimates can also be obtained by using method = 'locpoly'.\n\nShow the codeadults &lt;- subset(nhanes, !is.na(BPXSAR))\na25 &lt;- svysmooth(BPXSAR ~ RIDAGEYR,\n  method = \"quantreg\", design = adults,\n  quantile = .25, df = 4\n)\na50 &lt;- svysmooth(BPXSAR ~ RIDAGEYR,\n  method = \"quantreg\", design = adults,\n  quantile = .5, df = 4\n)\na75 &lt;- svysmooth(BPXSAR ~ RIDAGEYR,\n  method = \"quantreg\", design = adults,\n  quantile = .75, df = 4\n)\na10 &lt;- svysmooth(BPXSAR ~ RIDAGEYR,\n  method = \"quantreg\", design = adults,\n  quantile = .1, df = 4\n)\na90 &lt;- svysmooth(BPXSAR ~ RIDAGEYR,\n  method = \"quantreg\", design = adults,\n  quantile = .9, df = 4\n)\nplot(BPXSAR ~ RIDAGEYR,\n  data = nhanes, type = \"n\", ylim = c(80, 200),\n  xlab = \"Age\", ylab = \"Systolic Pressure\"\n)\nlines(a50, lwd = 3)\nlines(a25, lwd = 1)\nlines(a75, lwd = 1)\nlines(a10, lty = 3)\nlines(a90, lty = 3)\nlegend(\"topleft\",\n  legend = c(\"10%,90%\", \"25%, 75%\", \"median\"),\n  lwd = c(1, 1, 3), lty = c(3, 1, 1), bty = \"n\"\n)\n\n\n\n\nConditioning Plots\nA conditioning plot is effectively a scatter plot with a third variable fixed. This third variable is then displayed in the facet title of the plot. In Lumley’s text he shows how to do this using a call to the svycoplot() function to recreate the top half of Figure 4.20. Of note, the additional hexscale argument can be fed the \"absolute\" argument to make the scales comparable between panels - by default the scales are facet specific, though the data (for a continuous facet) has a ~50% overlap so the scales are not dramatically different.\n\nShow the codesvycoplot(BPXSAR ~ BPXDAR | equal.count(RIDAGEMN),\n  style = \"hex\", # or \"transparent\" for shaded hexs,\n  # hexscale = \"absolute\" # for fixed scales across facets.\n  design = subset(nhanes, BPXDAR &gt; 0), xbins = 20,\n  strip = strip.custom(var.name = \"AGE\"),\n  xlab = \"Diastolic pressure (mm Hg)\",\n  ylab = \"Systolic pressure (mm Hg)\"\n)"
  },
  {
    "objectID": "ComplexSurveyNotes.html#maps",
    "href": "ComplexSurveyNotes.html#maps",
    "title": "Complex Survey Notes",
    "section": "Maps",
    "text": "Maps\nDesign and Estimation Issues\nThe final section in this chapter looks at how to visualize survey data spatially across a map. Since many surveys contain geographic information in both their sampling design and the questions they seek to answer, it makes sense that one might want to visualize estimates at some geographic scale.\nLumley uses the maptools R package to take estimates computed using techniques/functions already demonstrated and visualize them on maps.\nSince maptools isn’t available on CRAN at the current time of writing, I’ll use the sf package and ggplot2.\nBelow I reproduce Figure 4.21 from the text using these packages with the same design based estimates in the text. The data was procured from Lumley’s website — search for “BRFSS”.\n\nShow the codestates &lt;- read_sf(\"Data/BRFSS2007/brfss_state_2007_download.shp\") %&gt;%\n  arrange(ST_FIPS)\nbys &lt;- svyby(~X_FRTSERV, ~X_STATE, svymean,\n  design = subset(brfss, X_FRTSERV &lt; 99999)\n)\nstate_fruit_servings &lt;- states %&gt;%\n  select(ST_FIPS) %&gt;%\n  st_drop_geometry() %&gt;%\n  left_join(bys, by = c(\"ST_FIPS\" = \"X_STATE\")) %&gt;%\n  mutate(geometry = states$geometry) %&gt;%\n  st_as_sf()\n\nstate_fruit_servings %&gt;%\n  ggplot() +\n  geom_sf(aes(fill = X_FRTSERV / 100)) +\n  theme_void() +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Servings of fruit per day, from BRFSS 2007\")\n\n\n\n\n\nShow the codehlth &lt;- brfss %&gt;%\n  as_survey_design() %&gt;%\n  mutate(\n    agegp = cut(AGE, c(0, 35, 50, 65, Inf)),\n    state = X_STATE,\n    covered = (HLTHPLAN == 1) * 1\n  ) %&gt;%\n  group_by(agegp, state) %&gt;%\n  summarize(\n    health_coverage = survey_mean(covered)\n  ) %&gt;%\n  ## Formatting\n  mutate(Age = case_when(\n    agegp == \"(0,35]\" ~ \"&lt;35\",\n    agegp == \"(35,50]\" ~ \"35-50\",\n    agegp == \"(50,65]\" ~ \"50-65\",\n    agegp == \"(65,Inf]\" ~ \"65+\"\n  ))\n\n\n\ninsurance_coverage &lt;- states %&gt;%\n  select(ST_FIPS, geometry) %&gt;%\n  left_join(hlth, by = c(\"ST_FIPS\" = \"state\")) %&gt;%\n  st_as_sf()\n\ninsurance_coverage %&gt;%\n  ggplot(aes(fill = health_coverage)) +\n  geom_sf() +\n  facet_wrap(~Age) +\n  theme_void() +\n  theme(legend.title = element_blank())\n\n\n\n\nLumley then shows two plots with insurance coverage at the state level. Looking at the code he posted on the web it isn’t clear to me what he’s estimating that’s different here as I don’t see any calls to any of the survey functions. Consequently, I’ve just created some fake simulated data and created a plot with the equivalent data.\n\nShow the codecities &lt;- read_sf(\"Data/BRFSS2007/BRFSS_MMSA_2007.shp\") %&gt;%\n  filter(NAME != \"Columbus\") %&gt;%\n  transmute(Insurance = cut(rbeta(n(), 1, 1), c(0, 0.25, .5, .75, 1)))\n\nmarginal_insurance &lt;- brfss %&gt;%\n  as_survey_design() %&gt;%\n  mutate(\n    covered = (HLTHPLAN == 1) * 1,\n    state = X_STATE,\n  ) %&gt;%\n  group_by(state) %&gt;%\n  summarize(\n    health_coverage = survey_mean(covered)\n  ) %&gt;%\n  ungroup()\n\nmap_data &lt;- states %&gt;%\n  select(ST_FIPS, geometry) %&gt;%\n  left_join(marginal_insurance, by = c(\"ST_FIPS\" = \"state\")) %&gt;%\n  st_as_sf()\n\nggplot() +\n  geom_sf(data = map_data, aes(fill = health_coverage)) +\n  geom_sf(data = cities, aes(color = Insurance)) +\n  theme_void() +\n  theme(legend.title = element_blank()) +\n  ggtitle(\"Insurance Coverage - from BRFSS and Fake Data\")"
  },
  {
    "objectID": "ComplexSurveyNotes.html#exercises-3",
    "href": "ComplexSurveyNotes.html#exercises-3",
    "title": "Complex Survey Notes",
    "section": "Exercises",
    "text": "Exercises\n\nDraw box plots of body mass index by race/ethnicity and by sex using the CHIS 2005 data introduced in Chapter 2.\n\n\nShow the codesvyboxplot(bmi_p ~ racehpr, chis, main = \"BMI By Race/Ethnicity\")\n\n\n\n\n\nShow the codesvyboxplot(bmi_p ~ srsex, chis, main = \"BMI BY Sex\")\n\n\n\n\n\nUsing the code in Figure 3.8 draw a bar plot of the quantiles of income and compare it to the dot chart in Figure 3.9. What are some advantages and disadvantages of each display.\n\n\nShow the codepltdf &lt;- as_tibble(qinc) %&gt;%\n  select(household_size, contains(\"thtotinc\"), -contains(\"se.\")) %&gt;%\n  gather(everything(), -household_size, key = \"quantile\", value = \"Total Income\") %&gt;%\n  mutate(quantile = as.numeric(str_extract(quantile, \"[0-9].[0-9]?[0-9]\")) * 100)\n\nse &lt;- as_tibble(qinc) %&gt;%\n  select(household_size, contains(\"se.\")) %&gt;%\n  gather(everything(), -household_size, key = \"quantile\", value = \"SE\") %&gt;%\n  mutate(quantile = as.numeric(str_extract(quantile, \"[0-9].[0-9]?[0-9]\")) * 100)\n\npltdf &lt;- pltdf %&gt;%\n  left_join(se) %&gt;%\n  mutate(\n    lower = `Total Income` - 2 * SE,\n    upper = `Total Income` + 2 * SE\n  )\n\npltdf %&gt;%\n  mutate(quantile = factor(quantile)) %&gt;%\n  ggplot(aes(x = household_size, y = `Total Income`, fill = quantile)) +\n  geom_bar(stat = 'identity', position='dodge') +\n  geom_errorbar(aes(ymin = lower, ymax = upper), position='dodge') +\n  xlab(\"Household Size\") +\n  ylab(\"Total Income (USD)\") +\n  ggtitle(\"Total Income Quantiles\",\n    subtitle = \"Survey of Income and Program Participation\"\n  )\n\n\n\n\nI prefer the dot plot as I feel the bar plot takes up more space to convey the same amount of information. However, I think the bar plot might make it easier to compare the income quantiles within household size as they are right next to each other.\n\nUse svysmooth() to draw a graph showing change in systolic and diastolic blood pressure over time in the NHANES 2003-2004 data. Can you see the change to isolated systolic hypertension in old age that is shown in Figure 4.5.\n\n\nShow the codeplot(svysmooth(BPXSAR ~ RIDAGEYR, nhanes))\n\n\n\n\n\nWith the data from the SIPP 1996 panel draw the cumulative distribution function density function, a histogram and a box plot of total household income. Compare these graphs for their usefulness in showing the distribution of income.\n\n\nShow the codesipp_hh &lt;- svydesign(\n  id = ~ghlfsam, strata = ~gvarstr, nest = TRUE,\n  weight = ~whfnwgt, data = \"household\", dbtype = \"SQLite\",\n  dbname = \"Data/SIPP/sipp.db\"\n)\nplot(svycdf(~thtotinc, sipp_hh))\n\n\n\n\n\nShow the codesvyhist(~thtotinc, sipp_hh)\n\n\n\n\n\nShow the codesvyboxplot(thtotinc~1, sipp_hh)\n\n\n\n\nI think I prefer the histogram the most out of these in terms of conveying the most information with the space available. Both the box plot and the cdf give a good sense that the data is skewed, but gives very little sense of how much density or probability mass can be found in the lower incomes. Only the histogram does that to my satisfaction.\n\nWith the data from the SIPP 1996 panel draw a graph showing amount of rent (tmthrnt) and proportion of income paid to rent. You will want to exclude some outlying households that report much higher rent than income.\n\n\nShow the codesipp_hh &lt;- update(sipp_hh,\n    pct_income_rent = (as.numeric(tmthrnt) / as.numeric(thtotinc) * 100),\n  ) \nsvyplot(tmthrnt ~ pct_income_rent, \n        subset(sipp_hh,\n               pct_income_rent &lt; Inf &\n               tmthrnt &lt; thtotinc &\n               pct_income_rent &lt; 100),\n        xlab = \"% Income Paid To Rent\",\n        ylab = \"Rent (USD)\") \n\n\n\n\n\nUsing data from CHIS 2005 (see section 2.3.1) examine how body mass index varies with age as we did with blood pressure in this chapter.\n\n\nShow the codeplot(svysmooth(bmi_p ~ srage_p, chis),\n  xlab = \"Age\", ylab = \"BMI\",\n  main = \"BMI vs. Age (CHIS data)\"\n)\n\n\n\n\nWe see a very distinctive U-shape - with middle aged individuals - 50 to 60 year olds - having the highest average BMI, but the young and old having lower BMI’s.\n\nThe left-hand panel of Figure 3.3 shows an interesting two-lobed pattern. Can you find what makes these lobes?\n\nAs stated previously, I don’t have access to this data.\n\nSet up a survey design for the BRFSS 2007 data as in Figure 3.7. BRFSS measured annual income (income2) in categories &lt; $10K, $10-15k $20-25k, $25-35k, $35-50k, $50-75k, &gt; $75k and race (orace): White, Black, Asian, Native Hawaiian/Pacific Islander, Native American, other.\n\n\nDraw a graph of income by race.\nDraw maps showing the geographical distribution of income and of racial groups.\nDraw a set of maps examining whether the geographical distribution of income differs by race.\n\nI don’t see the race or income variables listed as Lumley describes in my version of the BRFSS data, and while there are certain race variables, it isn’t clear how they map to White, black asian, etc.\n\nExplore the impact on the graphs in Figure 4.18 of changes in the amount of smoothing, by altering the df argument to the code in Figure 4.19\n\n\nShow the codea25 &lt;- svysmooth(BPXSAR ~ RIDAGEYR,\n  method = \"quantreg\", design = adults,\n  quantile = .25, df = 7\n)\na50 &lt;- svysmooth(BPXSAR ~ RIDAGEYR,\n  method = \"quantreg\", design = adults,\n  quantile = .5, df = 7\n)\na75 &lt;- svysmooth(BPXSAR ~ RIDAGEYR,\n  method = \"quantreg\", design = adults,\n  quantile = .75, df = 7\n)\na10 &lt;- svysmooth(BPXSAR ~ RIDAGEYR,\n  method = \"quantreg\", design = adults,\n  quantile = .1, df = 7\n)\na90 &lt;- svysmooth(BPXSAR ~ RIDAGEYR,\n  method = \"quantreg\", design = adults,\n  quantile = .9, df = 7\n)\nplot(BPXSAR ~ RIDAGEYR,\n  data = nhanes, type = \"n\", ylim = c(80, 200),\n  xlab = \"Age\", ylab = \"Systolic Pressure\"\n)\nlines(a50, lwd = 3)\nlines(a25, lwd = 1)\nlines(a75, lwd = 1)\nlines(a10, lty = 3)\nlines(a90, lty = 3)\nlegend(\"topleft\",\n  legend = c(\"10%,90%\", \"25%, 75%\", \"median\"),\n  lwd = c(1, 1, 3), lty = c(3, 1, 1), bty = \"n\"\n)\n\n\n\n\nWe see that increasing the degrees of smoothing increases the “wiggliness” of the plot which makes sense since we’re increasing the flexibility of the functional form estimated."
  },
  {
    "objectID": "ComplexSurveyNotes.html#ratio-estimation",
    "href": "ComplexSurveyNotes.html#ratio-estimation",
    "title": "Complex Survey Notes",
    "section": "Ratio Estimation",
    "text": "Ratio Estimation\nRatio estimation comes up first in this chapter because it is important in estimating (1) a population mean/total, (2) a ratio directly or (3) a subpopulation estimate of a mean.\nLumley illustrates how to estimate ratios in design methods by using the api dataset and estimating the proportion of students who took the Academic Performance Index exam.\n\nShow the codesvyratio(~api.stu, ~enroll, strat_design)\n\nRatio estimator: svyratio.survey.design2(~api.stu, ~enroll, strat_design)\nRatios=\n           enroll\napi.stu 0.8369569\nSEs=\n             enroll\napi.stu 0.007757103\n\n\nThis estimate does a good job of estimating the true population total, 0.84, which we happen to have access to for this example.\nIt’s worth noting here as Lumley does that ratio estimates are not unbiased but are classified as “approximately unbiased” since the bias decreases proportional to the sample size and is consequently smaller than the standard error – which decreases proportional to \\frac{1}{\\sqrt{n}}.\nRatios for subpopulation estimates\nIn the case where individual - not aggregate - data are available the ratio being estimated is simply a proportion. This is the same logic for which subpopulation estimates had been calculated previously in [Chapter 2:Simple Random Sample] via the svyby() function. These estimates require special handling - though in the survey package they can all be calculated via svymean(), svyratio() and svycontrast() which I show below using the same designe object from Chapter 2.\nIt is worth noting before doing so however, that the special handling needed here follows from the fact that both the numerator and the denominator are estimated. Lumley delves into this in the appendix which I’ll reproduce here alongside questions I have that I’ll look to return to in the future.\nA brief aside on ratio variance estimation\nWe’ll define the subpopulation estimate of interest using the indicator function Y_i = X_iI(Z_I &gt; 0), where I(Z_i &gt; 0) = 1 for members of the subpopulation and 0 otherwise, and X_i is the measurement of interest.\nThe variance estimate using replicate weights can be calculated similar to the typical variance estimate, those replicate weights belonging to sample observations outside the subpopulation are simply not used - this again highlights the utility of replicate weights.\nFor linearization - that is using a taylor series to estimate the variance - the value becomes more complicated, following Lumley’s appendix the HTE is defined as:\n\n\\hat{V}[\\hat{T}_Y] = \\sum_{i,j} \\left(\\frac{Y_i Y_j}{\\pi_ij} -\n\\frac{Y_i}{\\pi_i} \\frac{Y_j}{\\pi_j} \\right) \\\\\n= \\sum_{Z_i,Z_j &gt; 0} \\left(\\frac{Y_i Y_j}{\\pi_ij} -\n\\frac{Y_i}{\\pi_i} \\frac{Y_j}{\\pi_j} \\right).\n\nHere however, Lumley states\n\nbut the simplified computational formulas for special designs are not the same.\n\nwhich I don’t completely understand. I suppose he means for clustered or multiphase designs things but it isn’t clear as he goes onto say\n\nfor example, the formula for the variance of a total under simple random sampling (equation 2.2)\n\n\nV[\\hat{T}_X] = \\frac{N-n}{N} \\times N^2 \\times \\frac{V[X]}{n}\n\n\ncannot be replaced by\n\n\nV[\\hat{T}_Y] \\stackrel{?}{=} \\frac{N-n}{N} \\times N^2 \\times \\frac{V[X]}{n}\n\n\nor even, defining n_D as the number sampled in the subpopulation, by\n\n\n\\stackrel{?}{=} \\frac{N-n_D}{N} \\times N^2 \\times \\frac{V[X]}{n_D}\n\n\nIn order to use these simplified formulas it is necessary to work with the variable Y and use\n\n\nV[\\hat{T}_Y] = \\frac{N-n}{N} \\times N^2 \\times \\frac{V[Y]}{n}\n\nIts not clear in this last expression if we’re simply back to the initial expression that couldn’t be used, or if we’re using the smaller sample subset again for variance computations but Lumley’s next text suggests that’s the case:\n\nOperationally, this means that variance estimation in a subset of a survey design object in R needs to involve the n - n_D zero contributions to an estimation equation.\n\nI hope to shed more light on what’s going on here in the future but for now its clear why this is in the appendix, but not exactly clear to me why observations outside the subpopulation are simply zero’d in variance computation.\nLumley uses the following three function calls to illustrate three different ways to estimate a ratio (1) A call to svymean(), (2) A call to svyratio() and (3) `\n\nShow the codesvymean(~bmi_p, subset(chis, srsex == \"MALE\" & racehpr == \"AFRICAN AMERICAN\"))\n\n        mean     SE\nbmi_p 28.019 0.2663\n\n\n\nShow the codechis &lt;- update(chis,\n  is_aamale = (srsex == \"MALE\" & racehpr == \"AFRICAN AMERICAN\")\n)\nsvyratio(~ I(is_aamale * bmi_p), ~is_aamale, chis)\n\nRatio estimator: svyratio.svyrep.design(~I(is_aamale * bmi_p), ~is_aamale, chis)\nRatios=\n                     is_aamale\nI(is_aamale * bmi_p)  28.01857\nSEs=\n         [,1]\n[1,] 0.266306\n\n\n\nShow the codetotals &lt;- svytotal(~ I(bmi_p * is_aamale) + is_aamale, chis)\ntotals\n\n                        total       SE\nI(bmi_p * is_aamale) 19348927 260401.7\nis_aamaleFALSE       25697039   6432.3\nis_aamaleTRUE          690575   6341.9\n\nShow the codesvycontrast(\n  totals,\n  quote(`I(bmi_p * is_aamale)` / `is_aamaleTRUE`)\n)\n\n          nlcon     SE\ncontrast 28.019 0.2663\n\n\nRatio estimators of totals\nThe third use Lumley lists for the use of ratio estimators is to construct more accurate estimates of population means or totals. His motivating example is to take the ratio estimate of individuals who took the API tests and then use that to determine the approximate number of students who took the test, by multiplying the ratio estimate by the number of students. This can be done by hand or via the survey package predict() function used in conjunction with svyratio().\n\nShow the coder &lt;- svyratio(~api.stu, ~enroll, strat_design)\npredict(r, total = sum(apipop$enroll, na.rm = TRUE))\n\n$total\n         enroll\napi.stu 3190038\n\n$se\n          enroll\napi.stu 29565.98\n\n\nLumley uses this as a jumping off point to discuss linear regression since one can imagine the relationship between the number of students taking the API test as being roughly proportional to the number of students enrolled at the schools; E[tests_i] = \\alpha \\times \\text{enrollment}_i + \\epsilon_i where E[\\epsilon] = 0 (note that this is an assumption about the moment of the error distribution and not the shape of the error distribution itself)."
  },
  {
    "objectID": "ComplexSurveyNotes.html#linear-regression",
    "href": "ComplexSurveyNotes.html#linear-regression",
    "title": "Complex Survey Notes",
    "section": "Linear Regression",
    "text": "Linear Regression\nA quick review of the moment assumptions for linear regression - which is all that are needed for designed based estimation.\nIf we have random response variable Y and explanatory variable X then linear regression is looking at the relationship between the expectation of Y and X: \nE[Y] = \\alpha + X \\beta,\n\nWhere \\alpha is a constant offset, also called the intercept and \\beta is the slope coefficient that describes the change in E[Y] per unit change in X. Here I’m referring to X and \\beta as singular variables but they could also be a matrix and vector of explanatory variables and slope coefficients, respectively. The variance of the response variable, Y is assumed to be constant, i.e. V[Y] = \\sigma^2, unless otherwise modeled.\nFollowing the standard OLS estimation procedure, we’d normally minimize the squared error and Lumley notes that if we have the complete population data, we’d be finished at that:\n\nRSS = \\sum_{i=1}^{N} (Y_i - \\alpha X_i\\beta)^{2}.\n\nGiven that we’re typically dealing with (complex) samples though, we have to adjust the estimates to account for the weighting:\n\n\\hat{RSS} = \\sum_{i=1}^{n} \\frac{1}{\\pi_i}(Y_i - \\alpha - X_i \\beta)^2,\n\nso each error term is up weighted according to its sampling probability.\nRegression Estimation of Population Totals\nLumley’s ratio estimator of a population total described previously derives from the linear regression model with a single predictor and no intercept. The separate ratio estimator, similar to the cell means model estimates a ratio for each stratum and the estimate of the total is the sum of the denominator for all strata. Formally,\n\nE[Y_i] = \\beta_k \\times X_i \\times \\{i \\in \\text{ stratum } k\\},\n\nwhere \\beta_k is the ratio for stratum k estimates the given quantity. This setup can provide more precise estimates than the single ratio estimator when the sample size gets large and the strata are able to better explain the outcome variable. However, if the strata don’t have any correlation with the outcome then the standard errors increase, due to the need to estimate the extra parameters.\nTHIS EXAMPLE DOESN’T MAKE COMPLETE SENSE. Lumley illustrates this latter phenomenon with the California school data - using the percentage of english language learners as a predictor of the overall number of students taking the api tests.\n\nShow the codesep &lt;- svyratio(~api.stu, ~enroll, strat_design, separate = TRUE)\ncom &lt;- svyratio(~api.stu, ~enroll, strat_design)\nstratum_totals &lt;- list(E = 1877350, H = 1013824, M = 920298)\npredict(sep, total = stratum_totals)\n\n$total\n         enroll\napi.stu 3190022\n\n$se\n          enroll\napi.stu 29756.44\n\n\n\nShow the codepredict(com, total = sum(unlist(stratum_totals)))\n\n$total\n         enroll\napi.stu 3190038\n\n$se\n          enroll\napi.stu 29565.98\n\n\nWe see the common ratio has a smaller standard error than the separate ratio estimator.\n\nShow the codesvyby(~api.stu, ~stype, design = dstrata, denom = ~enroll, svyratio)\n\n  stype api.stu/enroll se.api.stu/enroll\nE     E      0.8558562       0.006034685\nH     H      0.7543378       0.031470156\nM     M      0.8331047       0.017694634\n\n\nIncomes in Scotland\nLumley works through an example looking at household incomes across Scotland from their national household survey. Unfortunately both the dataset subset he provides at his website to and the full dataset he links to don’t have the variables he uses in his example code in Figure 5.7. For example the code he provides is filtered using an ADULTH variable which isn’t found in either dataset.\n\nShow the codeload(\"Data/SHS/shs.rda\") # Lumley's website data\ncolnames(shs$variables)\n\n [1] \"psu\"      \"uniqid\"   \"ind_wt\"   \"shs_6cla\" \"council\"  \"rc5\"     \n [7] \"rc7e\"     \"rc7g\"     \"intuse\"   \"groupinc\" \"clust\"    \"stratum\" \n[13] \"age\"      \"sex\"      \"emp_sta\"  \"grosswt\"  \"groc\"    \n\nShow the codeload(\"Data/SHS/ex2.RData\") # PEAS \"full Data\" website\ncolnames(shs)\n\n [1] \"PSU\"      \"UNIQID\"   \"IND_WT\"   \"SHS_6CLA\" \"COUNCIL\"  \"RC5\"     \n [7] \"RC7E\"     \"RC7G\"     \"INTUSE\"   \"GROUPINC\" \"CLUST\"    \"STRATUM\" \n[13] \"AGE\"      \"SEX\"      \"EMP_STA\"  \"GROSSWT\"  \"GROC\"    \n\n\nConsequently, I won’t reproduce this example here, except to say that he uses the example to illustrate how oversampling certain strata (poorer households) can improve the precision associated with the household income estimate and that using the population information increased the precision of the weekly household income estimate via linear regression – for those sub-populations for which population information is available.\nUS Elections\nSimilarly it doesn’t look like the data set included in the current survey R package has data for the 2008 election - I only see Bush / McCain vote totals.\n\nShow the codedata(elections)\ncolnames(election)\n\n[1] \"County\"             \"TotPrecincts\"       \"PrecinctsReporting\"\n[4] \"Bush\"               \"Kerry\"              \"Nader\"             \n[7] \"votes\"              \"p\"                 \n\n\nConsequently I can’t do the analysis he shows predicting 2008 votes using 2000 votes. It’ll hopefully suffice to say that in theme with the content for the chapter, that because these values are correlated — 2000 vote % for republican candidate and 2008 vote % for republican candidate — it stands to reason that we can reduce the variance of the resulting estimate rather than using the 2008 data alone.\nConfouding and other criteria for model choice\nLumley describes three categories for describing why a predictor might be included in a regression, noting that this may help the model fit better and thus aid in reducing bias from a probability sample that results from, say non-response.\n\nExposure of interest: If we’re interested in a specific variable’s impact on a variable, it makes sense to include that in a model to estimate the relationship.\nConfounding variables: A variable may not be of primary interest, but may be associated with both the outcome variable and exposure of interest. Consequently, this will need to be adjusted for in order to isolate the effect of interest.\nPrecision variables: These are, again, associated with the outcome variable of interest, but not associated with the exposure of interest. However, because of their association alone, they can increase the precision with which the exposure effect is estimated.\n\nLumley goes on to describe methods for model selection which I’ll leave for the text.\nLinear models in the survey package\nExample: Dietary sodium and potassium and blood pressure\n\nShow the codedemo &lt;- haven::read_xpt(\"data/nhanesxpt/demo_c.xpt\")[, c(1:8, 28:31)]\nbp &lt;- haven::read_xpt(\"data/nhanesxpt/bpx_c.xpt\")\nbm &lt;- haven::read_xpt(\"data/nhanesxpt/bmx_c.xpt\")[, c(\"SEQN\", \"BMXBMI\")]\ndiet &lt;- haven::read_xpt(\"data/nhanesxpt/dr1tot_c.xpt\")[, c(1:52, 63, 64)]\nnhanes34 &lt;- merge(demo, bp, by = \"SEQN\")\nnhanes34 &lt;- merge(nhanes34, bm, by = \"SEQN\")\nnhanes34 &lt;- merge(nhanes34, diet, by = \"SEQN\")\n\ndemo5 &lt;- haven::read_xpt(\"data/nhanesxpt/demo_d.xpt\")[, c(1:8, 39:42)]\nbp5 &lt;- haven::read_xpt(\"data/nhanesxpt/bpx_x.xpt\")\n\nbp5$BPXSAR &lt;- rowMeans(bp5[, c(\"BPXSY1\", \"BPXSY2\", \"BPXSY3\", \"BPXSY4\")],\n  na.rm = TRUE\n)\nbp5$BPXDAR &lt;- rowMeans(bp5[, c(\"BPXDI1\", \"BPXDI2\", \"BPXDI3\", \"BPXDI4\")],\n  na.rm = TRUE\n)\nbm5 &lt;- haven::read_xpt(\"data/nhanesxpt/bmx_d.xpt\")[, c(\"SEQN\", \"BMXBMI\")]\ndiet5 &lt;- haven::read_xpt(\"data/nhanesxpt/dr1tot_d.xpt\")[, c(1:52, 64, 65)]\n\n\nnhanes56 &lt;- merge(demo5, bp5, by = \"SEQN\")\nnhanes56 &lt;- merge(nhanes56, bm5, by = \"SEQN\")\nnhanes56 &lt;- merge(nhanes56, diet5, by = \"SEQN\")\n\nnhanes &lt;- rbind(nhanes34, nhanes56)\nnhanes$fouryearwt &lt;- nhanes$WTDRD1 / 2\n# I added the two lines below to make graphing the\n# smooth plot easier\nnhanes$sodium &lt;- nhanes$DR1TSODI / 1000\nnhanes$potassium &lt;- nhanes$DR1TPOTA / 1000\n\ndes &lt;- svydesign(\n  id = ~SDMVPSU, strat = ~SDMVSTRA, weights = ~fouryearwt,\n  nest = TRUE, data = subset(nhanes, !is.na(WTDRD1))\n)\n\ndes &lt;- update(des, sodium = DR1TSODI / 1000, potassium = DR1TPOTA / 1000)\ndes\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (60) clusters.\nupdate(des, sodium = DR1TSODI/1000, potassium = DR1TPOTA/1000)\n\n\nLumley uses the following plot — examining systolic blood pressure as a function of daily sodium intake — to motivate the need to adjust for confounders. As we can see below in the reproduced Figure 5.10, there doesn’t appear to be much a relationship between sodium intake and average blood pressure. Lumley argues that we observe this simpler relationship because of the association between sodium and blood pressure is confounded by age.\n\nShow the codeplot(BPXSAR ~ sodium, data = nhanes, type = \"n\")\n\n\n\nShow the codepoints(svyplot(BPXSAR ~ sodium,\n  design = des, style = \"transparent\", xlab =\n    \"Dietary Sodium (g/day)\", ylab = \"Systolic Blood Pressure (mm Hg)\",\n))\nlines((svysmooth(BPXSAR ~ sodium, des)))\n\n\n\n\nTo test this hypothesis, Lumley first visualizes the three variables using the conditional plot demonstrated previously.\n\nShow the codesvycoplot(BPXSAR ~ sodium | equal.count(RIDAGEYR), des,\n  style = \"hexbin\",\n  xlab = \"Dietary Sodium (g/day)\",\n  ylab = \"Systolic BP (mmHg)\",\n  strip = strip.custom(var.name = \"Age\")\n)\n\n\n\n\nIn the above plot we see a greater indication that as dietary sodium increases , so too does systolic blood pressure.\nTo more formally test the hypothesis, Lumley fits several models with these variables included. The first two just include (1) sodium and potassium and (2) sodium, potassium and Age.\n\nShow the codemodel0 &lt;- svyglm(BPXSAR ~ sodium + potassium, design = des)\nsummary(model0)\n\n\nCall:\nsvyglm(formula = BPXSAR ~ sodium + potassium, design = des)\n\nSurvey design:\nupdate(des, sodium = DR1TSODI/1000, potassium = DR1TPOTA/1000)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 120.3899     0.7105 169.436  &lt; 2e-16 ***\nsodium       -0.6907     0.1658  -4.166 0.000268 ***\npotassium     0.7750     0.2655   2.919 0.006853 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 382.6159)\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nShow the codemodel1 &lt;- svyglm(BPXSAR ~ sodium + potassium + RIDAGEYR, design = des)\nsummary(model1)\n\n\nCall:\nsvyglm(formula = BPXSAR ~ sodium + potassium + RIDAGEYR, design = des)\n\nSurvey design:\nupdate(des, sodium = DR1TSODI/1000, potassium = DR1TPOTA/1000)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 99.73535    0.79568 125.346  &lt; 2e-16 ***\nsodium       0.79846    0.14866   5.371 1.13e-05 ***\npotassium   -0.91148    0.18994  -4.799 5.23e-05 ***\nRIDAGEYR     0.49561    0.01169  42.404  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 275.4651)\n\nNumber of Fisher Scoring iterations: 2\n\n\nAs we can see, the sodium and potassium coefficient signs in the first model switch direction once age is included in the second model, demonstrating that the two variables are associated with both age and systolic blood pressure. The second model makes more sense intuitively, because we expect systolic blood pressure to increase, on average, as a function of sodium intake.\nLumley adds a few more possible confounders to model2 and then tests to see whether the effects of daily dietary sodium and potassium on systolic blood pressure are significantly different than zero using the regTermTest() function.\n\nShow the codemodel2 &lt;- svyglm(BPXSAR ~ sodium + potassium + RIDAGEYR + RIAGENDR + BMXBMI,\n  design = des\n)\nsummary(model2)\n\n\nCall:\nsvyglm(formula = BPXSAR ~ sodium + potassium + RIDAGEYR + RIAGENDR + \n    BMXBMI, design = des)\n\nSurvey design:\nupdate(des, sodium = DR1TSODI/1000, potassium = DR1TPOTA/1000)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 97.32903    1.42668  68.220  &lt; 2e-16 ***\nsodium       0.43458    0.16164   2.689   0.0126 *  \npotassium   -0.96119    0.17043  -5.640 7.19e-06 ***\nRIDAGEYR     0.45791    0.01080  42.380  &lt; 2e-16 ***\nRIAGENDR    -3.38208    0.38403  -8.807 3.90e-09 ***\nBMXBMI       0.38460    0.03797  10.129 2.48e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 263.5461)\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nShow the coderegTermTest(model2, ~ potassium + sodium, df = NULL)\n\nWald test for potassium sodium\n in svyglm(formula = BPXSAR ~ sodium + potassium + RIDAGEYR + RIAGENDR + \n    BMXBMI, design = des)\nF =  15.98481  on  2  and  25  df: p= 3.3784e-05 \n\n\nThe test is formally examining whether a model with these terms is more likely, given the data, then one without, with the null hypothesis assuming that the extra terms are unneccessary. As we can see, the model is very unlikely to fit so well with the two extra terms, so there’s evidence to support the association between the terms and blood pressure.\nLumley digs into further details of why the effect is so small — 1 gram of sodium (2.5 grams of salt) is a lot of salt required to increase systolic blood presssure “only” .43 mmHg. Some explanations include measurement error, missing data, and model misspecification. Lumley examines the last of these by displaying the model diagnostic plots. Model misspecification can be examined by identifying any association between the partial residuals and the observed sodium intake. I replicate Lumley’s code below.\n\nShow the codepar(mfrow = c(1, 2))\nplot(as.vector(predict(model1)), resid(model1),\n  xlab = \"Fitted Values\", ylab = \"Residuals\"\n)\nnonmissing &lt;- des[-model1$na.action]\nplot(nonmissing$variables$sodium,\n  resid(model1, \"partial\")[, 1],\n  xlab = \"Sodium\",\n  ylab = \"Partial Residuals\"\n)\n\n\n\n\n\nShow the codenonmissing &lt;- des[-model1$na.action]\npar(mfrow = c(1, 2))\nplot(model1, panel = make.panel.svysmooth(nonmissing))\n\n\n\n\n\n\nShow the codetermplot(model1,\n  data = model.frame(nonmissing),\n  partial = TRUE, se = TRUE, smooth = make.panel.svysmooth(nonmissing)\n)\n\n\n\n\n\n\n\n\nShow the codeint1 &lt;- svyglm(BPXSAR ~ (sodium + potassium) * I(RIDAGEYR - 40) + RIAGENDR + BMXBMI,\n  design = des\n)\nsummary(int1)\n\n\nCall:\nsvyglm(formula = BPXSAR ~ (sodium + potassium) * I(RIDAGEYR - \n    40) + RIAGENDR + BMXBMI, design = des)\n\nSurvey design:\nupdate(des, sodium = DR1TSODI/1000, potassium = DR1TPOTA/1000)\n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                116.215191   1.240298  93.699  &lt; 2e-16 ***\nsodium                       0.309581   0.165746   1.868 0.074583 .  \npotassium                   -0.975994   0.182598  -5.345 1.99e-05 ***\nI(RIDAGEYR - 40)             0.605278   0.022047  27.455  &lt; 2e-16 ***\nRIAGENDR                    -3.516219   0.377502  -9.314 2.87e-09 ***\nBMXBMI                       0.387957   0.038347  10.117 6.14e-10 ***\nsodium:I(RIDAGEYR - 40)     -0.015707   0.008767  -1.792 0.086356 .  \npotassium:I(RIDAGEYR - 40)  -0.039575   0.010121  -3.910 0.000703 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 261.658)\n\nNumber of Fisher Scoring iterations: 2\n\n\nIs Weighting Needed in a Regression Model ?\nLumley caps off this chapter asking whether we even need to bother with the specialized weighting built into the survey package. His answer is worth digging into:\n\nSince regression models use adjustment for confounders as a way of removing distorted associations between exposure and response, it is plausible that a regression model might not need sampling weights.\n\nA key assumption here is whether the population we’re estimating is stable across the population(s) represented by our data set. The naive, biased population that our sample represents when unweighted, or the “target” population our sample represents when re-weighted. A follow-up question would ask whether we could even estimate the relationship as desired, if the effect is heterogeneous across populations. Lumley cites (DuMouchel and Duncan 1983) for further discussion on this topic.\nI have more to say here — Thinking of this article [gelman2007struggles] —, but it may not fit in these notes. For now I’ll end with Lumley’s two limitations for regression models when not using weights:\n\nSome important variables used in constructing the weights may not be available,\nFurther, the important variables mentioned above may not be suitable for including in the model.\n\nLumley urges caution in this regard, advising that even a small amount of bias introduced from not including the weights may make any potential increase in precision that comes from not using the weights as poor trade-off."
  },
  {
    "objectID": "ComplexSurveyNotes.html#exercises-4",
    "href": "ComplexSurveyNotes.html#exercises-4",
    "title": "Complex Survey Notes",
    "section": "Exercises",
    "text": "Exercises\n\nThis exercise uses the WA State crime data for 2004 as the population. The data consists of crime rates and population sizes for the police districts (in cities/towns) and sheriffs’ offices (in unincorporated areas), grouped by county.\n\n\nTake a simple random sample of ten counties from the state and use all the data from the sampled counties. Estimate the total number of murders and burglaries in the state.\n\n\nShow the codecounty_sample &lt;- wa_crime_df %&gt;%\n  distinct(County) %&gt;%\n  slice_sample(n = 10) %&gt;%\n  pull(County)\n\nwa_crime_df %&gt;%\n  filter(County %in% county_sample) %&gt;%\n  as_survey_design() %&gt;%\n  summarize(\n    crime = survey_total(murder_and_crime)\n  )\n\n# A tibble: 1 × 2\n  crime crime_se\n  &lt;dbl&gt;    &lt;dbl&gt;\n1  8644    2238.\n\n\n\nUse the population of each county as an auxiliary variable to estimate the totals.\n\n\nShow the coderatio_estimate &lt;- wa_crime_df %&gt;%\n  filter(County %in% county_sample) %&gt;%\n  as_survey_design() %&gt;%\n  svyratio(~murder_and_crime, ~Population, design = .)\n\npredict(ratio_estimate, total = sum(wa_crime_df$Population))\n\n$total\n                 Population\nmurder_and_crime   56678.22\n\n$se\n                 Population\nmurder_and_crime   5888.009\n\n\n\nUse the numbers of murders and burglaries in the previous year as auxiliary variables in a regression estimate of the totals (why can’t we use a ratio estimate here?)\n\n\nShow the codewa_crime_03_df &lt;- readxl::read_xlsx(\"data/WA_crime/1984-2011.xlsx\", skip = 4) %&gt;%\n  filter(Year == \"2003\", Population &gt; 0) %&gt;%\n  mutate(\n    murder_and_crime = `Murder Total` + `Burglary Total`,\n    state_pop = sum(Population),\n    County = stringr::str_to_lower(County),\n    num_counties = n_distinct(County),\n  ) %&gt;%\n  group_by(County) %&gt;%\n  mutate(num_agencies = n_distinct(Agency)) %&gt;%\n  ungroup() %&gt;%\n  select(\n    County, Agency, Population, murder_and_crime,\n    num_counties, num_agencies\n  )\n\nmodel_df &lt;- wa_crime_03_df %&gt;%\n  mutate(year = \"2003\") %&gt;%\n  bind_rows(wa_crime_df %&gt;% mutate(year = \"2004\")) %&gt;%\n  mutate(\n    # We'll use 2004's numbers here for the fpc.\n    num_counties = if_else(year == \"2004\", num_counties, 0),\n    num_agencies = if_else(year == \"2004\", num_agencies, 0),\n  ) %&gt;%\n  spread(year, murder_and_crime) %&gt;%\n  group_by(County, Agency) %&gt;%\n  summarize(\n    num_counties = sum(num_counties),\n    num_agencies = sum(num_agencies),\n    `2003` = sum(replace_na(`2003`, 0)),\n    `2004` = sum(replace_na(`2004`, 0))\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(\n    # Agencies were removed between 2003 and 2004.\n    num_counties &gt; 0, num_agencies &gt; 0,\n    County %in% county_sample\n  )\n\nmodel_design &lt;- model_df %&gt;%\n  as_survey_design(\n    id = c(County, Agency),\n    fpc = c(num_counties, num_agencies)\n  )\n\nfit &lt;- svyglm(`2004` ~ `2003`, design = model_design)\n\ntotal_matrix &lt;- c(sum(wa_crime_03_df$murder_and_crime))\ntotal_matrix &lt;- as.data.frame(total_matrix)\nnames(total_matrix) &lt;- \"2003\"\n\npredict(fit, newdata = total_matrix)\n\n   link     SE\n1 52874 1138.2\n\n\nWe can’t use a ratio estimator here because we’re not using the total population of the year in question as the denominator, we’re only looking at the total number of murders and burglaries in the previous year and relating it to the next, without measuring the total population, explicitly.\n\nStratify the sampling so that King County is sampled with 100% probability together with a simple random sample of five other counties. use population as an auxiliary variable to construct a common ratio estimate and a separate ratio estimate of the population totals.\n\n\nShow the codesmaller_county_sample &lt;- wa_crime_df %&gt;%\n  distinct(County) %&gt;%\n  filter(County != \"king\") %&gt;%\n  slice_sample(n = 5) %&gt;%\n  pull(County)\n\ncounty_list &lt;- unique(wa_crime_df$County)\n\ndesign &lt;- wa_crime_df %&gt;%\n  filter(County == \"king\" | County %in% smaller_county_sample) %&gt;%\n  mutate(\n    strata_label = if_else(County == \"king\", \"King County\", \"WA Counties\"),\n    num_counties = if_else(County == \"king\", 1, length(county_list) - 1)\n  ) %&gt;%\n  as_survey_design(\n    ids = c(County, Agency),\n    fpc = c(num_counties, num_agencies),\n    strata = strata_label\n  )\n\nstrata_totals &lt;- wa_crime_df %&gt;%\n  mutate(strata = if_else(County == \"king\", \"King\", \"WA Counties\")) %&gt;%\n  group_by(strata) %&gt;%\n  summarize(Population = sum(Population)) %&gt;%\n  spread(strata, Population) %&gt;%\n  as.matrix()\n\nseparate_estimator &lt;- svyratio(~murder_and_crime, ~Population, design,\n  separate = TRUE\n)\ncommon_estimator &lt;- svyratio(~murder_and_crime, ~Population, design)\npredict(separate_estimator, total = strata_totals)\n\n$total\n                 Population\nmurder_and_crime    68697.7\n\n$se\n                 Population\nmurder_and_crime   2141.227\n\n\n\nShow the codepredict(common_estimator, total = sum(wa_crime_df$Population))\n\n$total\n                 Population\nmurder_and_crime   68915.53\n\n$se\n                 Population\nmurder_and_crime   3370.313\n\n\n\nTake simple random samples of five police districts from King County and five counties from the rest of the state. use population as an auxiliary variable to construct a common ratio estimate and a separate ratio estimate of the population totals.\n\n\nShow the codeking_districts &lt;- wa_crime_df %&gt;%\n  filter(County == \"king\") %&gt;%\n  pull(Agency)\nsampled_king_districts &lt;- sample(king_districts, 5)\nsampled_counties &lt;- sample(county_list, 5)\n\ndesign &lt;- wa_crime_df %&gt;%\n  filter(County %in% sampled_counties | Agency %in% sampled_king_districts) %&gt;%\n  mutate(\n    strata_label = if_else(County == \"king\", \"King County\", \"WA Counties\"),\n    num_counties = if_else(County == \"king\", 1, length(county_list) - 1),\n  ) %&gt;%\n  as_survey_design(\n    id = c(County, Agency),\n    fpc = c(num_counties, num_agencies),\n    strata = strata_label\n  )\n\n\nseparate_estimator &lt;- svyratio(~murder_and_crime, ~Population, design,\n  separate = TRUE\n)\ncommon_estimator &lt;- svyratio(~murder_and_crime, ~Population, design)\npredict(separate_estimator, total = strata_totals)\n\n$total\n                 Population\nmurder_and_crime   61185.56\n\n$se\n                 Population\nmurder_and_crime   5576.053\n\n\n\nShow the codepredict(common_estimator, total = sum(wa_crime_df$Population))\n\n$total\n                 Population\nmurder_and_crime   61727.29\n\n$se\n                 Population\nmurder_and_crime   8328.097\n\n\n\nUsing the WA state crime data as a population, take a stratified sample of five police districts from King County and five counties from the rest of the state. Estimate the ratio of violent crimes to non-violent crimes. Compare to the population value.\n\n\nShow the codesampled_king_districts &lt;- sample(king_districts, 5)\nsampled_counties &lt;- sample(county_list, 5)\n\nwa_crime_df %&gt;%\n  filter(County %in% sampled_counties | Agency %in% sampled_king_districts) %&gt;%\n  mutate(\n    strata_label = if_else(County == \"king\", \"King County\", \"WA Counties\"),\n    num_counties = if_else(County == \"king\", 1, length(county_list) - 1),\n  ) %&gt;%\n  as_survey_design(\n    id = c(County, Agency),\n    fpc = c(num_counties, num_agencies),\n    strata = strata_label\n  ) %&gt;%\n  summarize(\n    violent_non_violent = survey_ratio(violent_crime, property_crime)\n  )\n\n# A tibble: 1 × 2\n  violent_non_violent violent_non_violent_se\n                &lt;dbl&gt;                  &lt;dbl&gt;\n1              0.0608                0.00749\n\n\n\nShow the coderound(sum(wa_crime_df$violent_crime) / sum(wa_crime_df$property_crime), 2)\n\n[1] 0.07\n\n\nWe can see that the estimate is quite close to the population value\n\nUsing the data from Wave 1 of the 1996 SIPP panel (see Figure 3.8)\n\n\nEstimate the ratio of population totals for monthly rent (tmthrnt) and total household income (thtrninc) over the whole population and over the sub-population who pay rent.\n\nI think Lumley may have an error here when he says that thtrninc is the total household monthly income - earlier we used thtotinc for this measure as he had in creating Figure 3.9. Consequently, I use thtotinc below.\n\nShow the codesipp_hh_sub %&gt;%\n  # Total population\n  summarize(\n    ratio_of_monthly_rent_to_household_income = survey_ratio(tmthrnt, thtotinc)\n  )\n\n# A tibble: 1 × 2\n  ratio_of_monthly_rent_to_household_income ratio_of_monthly_rent_to_household…¹\n                                      &lt;dbl&gt;                                &lt;dbl&gt;\n1                                   0.00236                            0.0000800\n# ℹ abbreviated name: ¹​ratio_of_monthly_rent_to_household_income_se\n\n\n\nShow the codesipp_hh_sub %&gt;%\n  filter(tmthrnt &gt; 0) %&gt;%\n  # Rent paying subpopulation\n  summarize(\n    ratio_of_monthly_rent_to_household_income = survey_ratio(tmthrnt, thtotinc)\n  )\n\n# A tibble: 1 × 2\n  ratio_of_monthly_rent_to_household_income ratio_of_monthly_rent_to_household…¹\n                                      &lt;dbl&gt;                                &lt;dbl&gt;\n1                                     0.209                              0.00506\n# ℹ abbreviated name: ¹​ratio_of_monthly_rent_to_household_income_se\n\n\n\nCompute the individual-level ratio, i.e., the proportion of household income paid in rent, and estimate the population mean over the whole population and over the sub-population who pay rent.\n\n\nShow the code# Full Population\nsipp_hh_sub %&gt;%\n  mutate(\n    # I also ran the numbers if we excluded those with 0 household rent\n    # and the estimates are effectively the same.\n    prop_income_rent = if_else(thtotinc == 0, 0, (tmthrnt / thtotinc)),\n  ) %&gt;%\n  summarize(\n    prop_income_rent_est = survey_mean(prop_income_rent)\n  )\n\n# A tibble: 1 × 2\n  prop_income_rent_est prop_income_rent_est_se\n                 &lt;dbl&gt;                   &lt;dbl&gt;\n1               0.0221                 0.00624\n\n\n\nShow the codesipp_hh_sub %&gt;%\n  # Rent paying subpopulation\n  filter(tmthrnt &gt; 0) %&gt;%\n  mutate(\n    # I also ran the numbers if we excluded those with 0 household rent\n    # and the estimates are effectively the same.\n    prop_income_rent = if_else(thtotinc == 0, 0, (tmthrnt / thtotinc)),\n  ) %&gt;%\n  summarize(\n    prop_income_rent_est = survey_mean(prop_income_rent)\n  )\n\n# A tibble: 1 × 2\n  prop_income_rent_est prop_income_rent_est_se\n                 &lt;dbl&gt;                   &lt;dbl&gt;\n1                0.548                   0.154\n\n\nWhat are we to make of these estimates being different? Well that’s because they’re estimating two different things. As Lumley points out at the start of the chapter, one is a ratio of two population-level quantities, the other is the population estimate of a ratio measured at the individual level.\n\nUse the stratified sample from the Academic Performance Index population to examine whether the proportion of teachers with only emergency qualifications (emer) affects academic performance (as measured by 2000 API).\n\n\nWhat confounding variables measuring socioeconomic status of students should be included in the model?\n\nGoing off the web documentation of the api dataset from the survey package, it looks like there are a number of possible confounding variables should be included in the model. Here is a list with a brief explanation: * meals: The % of students eligible for subsidized meals. This is a proxy for poverty and is likely correlated with the academic achievement measured by the test.\n\nhsg: percent of parents who are high school graduates. Parental academic achievement is likely associated with their students’ academic achievement.\navg.ed: Average parental education level - this might be able to combine the above variable along with those who have college or post-graduate education.\ncomp.imp refers to a school “growth” improvement targets that may be related to students’ academic performance.\nacs.k3 average class size years K-3 - class size is often associated with academic performance There is a similar acs.46 variable for grades 4-6.\nell The percent of english language learners. Since most classes are typically instructed in english, a non-native english speakers may struggle more with academic instruction.\nfull percent fully qualified teachers. A fully qualified teacher will presumably be more capable of teaching than one that’s not fully qualified.\nenroll Enrollment may also be associated with the API, if larger schools have access to greater resources, or inversely, worse teacher to student ratios.\n\nShould 1999 API be in the model (and why, or why not?)\n\nThe value of including the 1999 API score would be that its very likely one of the most correlated variables with the 2000 score. The downside is that it is also likely correlated with all the other measures, including the emer measure, and may mask that variable’s weaker impact. I’ll leave it out in my estimate to try to avoid this problem.\n\n\n\nDo any of the confounding variables need to be transformed?\n\nSeveral of the binary / categorical variables will need to be transformed into 0 / 1 or cell means encoding. It could also be beneficial to center several of the continuous variables at the mean to offer an easier interpretation of the model.\n\n\nDoes emer need to be transformed?\n\nIt doesn’t look like it needs to be to me. I include two plots below that visualize the emer distribution (in the target population) as well as its relationship with the api00 measure. While there is a right skew in the distribution, this doesn’t strike me as problematic and I think a log transformation - an attempt to fix the skew - would not be helpful both because of the difficulty in handling 0 values as well as the log-scale interpretation.\nIt could be worth centering the emer value by the estimated population mean to make the interpretation of the intercept more valueable but I don’t think that’s necessary for an adequate model interpretation here.\n\nShow the codesvyhist(~emer, strat_design)\n\n\n\n\n\nShow the codesvyplot(api00 ~ emer, strat_design,\n  main = \"CA 2000 API Scores vs. Teacher Emergency Training Preparedness\",\n  xlab = \"Teachers with emergency training\",\n  ylab = \"API 2000\"\n)\n\n\n\n\n\nWhat is the conclusion at the end?\n\n\nShow the codefit &lt;- svyglm(\n  api00 ~ emer + meals + hsg + avg.ed + comp.imp +\n    acs.k3 + acs.46 + ell + full + enroll,\n  design = strat_design\n)\nsummary(fit)\n\n\nCall:\nsvyglm(formula = api00 ~ emer + meals + hsg + avg.ed + comp.imp + \n    acs.k3 + acs.46 + ell + full + enroll, design = strat_design)\n\nSurvey design:\nsvydesign(id = ~1, strata = ~stype, fpc = ~fpc, data = apistrat)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 652.826459 163.333076   3.997 0.000137 ***\nemer         -0.726831   1.485803  -0.489 0.625986    \nmeals        -2.378649   0.483856  -4.916 4.32e-06 ***\nhsg           0.424671   0.416138   1.021 0.310419    \navg.ed       48.006215  17.485877   2.745 0.007390 ** \ncomp.impYes  15.075954  14.397161   1.047 0.298035    \nacs.k3       -3.366084   4.984776  -0.675 0.501357    \nacs.46        2.989147   1.389796   2.151 0.034367 *  \nell          -0.228954   0.407923  -0.561 0.576110    \nfull          0.006335   1.242588   0.005 0.995944    \nenroll       -0.042131   0.035810  -1.176 0.242720    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 3905.969)\n\nNumber of Fisher Scoring iterations: 2\n\n\nFrom the output above, we see that the emer value isn’t found to be significantly associated (at \\alpha = 0.05) with the 2000 API measure after adjusting for other variables. Indeed, meals, avg.ed and acs.46 are the only values for which there is evidence to support a relationship at this level.\n\nFollowing on from the previous exercise, fit the same model to the whole population (the data set apipop) using the glm() function.\n\n\nShow the codefit_pop &lt;- glm(api00 ~ emer + meals + hsg + avg.ed + comp.imp +\n  acs.k3 + acs.46 + ell + full + enroll, data = apipop)\nsummary(fit_pop)\n\n\nCall:\nglm(formula = api00 ~ emer + meals + hsg + avg.ed + comp.imp + \n    acs.k3 + acs.46 + ell + full + enroll, data = apipop)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 460.895349  21.325334  21.613  &lt; 2e-16 ***\nemer          0.542863   0.171137   3.172  0.00152 ** \nmeals        -2.180465   0.058205 -37.462  &lt; 2e-16 ***\nhsg           0.209642   0.078151   2.683  0.00734 ** \navg.ed       50.288082   2.345033  21.445  &lt; 2e-16 ***\ncomp.impYes  30.659677   2.103305  14.577  &lt; 2e-16 ***\nacs.k3        0.816373   0.556165   1.468  0.14222    \nacs.46        0.863564   0.268385   3.218  0.00130 ** \nell          -0.488302   0.064119  -7.616 3.25e-14 ***\nfull          1.560638   0.152078  10.262  &lt; 2e-16 ***\nenroll       -0.035925   0.004889  -7.348 2.43e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2748.517)\n\n    Null deviance: 70739140  on 4070  degrees of freedom\nResidual deviance: 11158979  on 4060  degrees of freedom\n  (2123 observations deleted due to missingness)\nAIC: 43803\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nDo the sample estimates agree with the population data? Do your decisions about transforming variables hold up in the population data?\n\nSome of the sample estimates agree with the population data. It is very clear that there is a whole lot more power to detect associations when the entire population is present. In brief, we see that the three variables for which an association was detected previously — meals, acs.46 and avg.edu — all have similar (within the sample based estimate margin of error) estimates on the full population data. Additionally, many other variables now have significant associations that did not previously. Notably the emer variable has a positive association with the api00 such that we’d expect to see a .5 gain in API score for every additional percent gain in teachers that are emergency qualified.\n\nFit the same model to 100 stratified samples from the population. Is the sampling distribution of the coefficients close to a Normal distribution?\n\n\nShow the codeOneSimulation &lt;- function() {\n  coefs &lt;- apipop %&gt;%\n    group_by(stype) %&gt;%\n    slice_sample(n = 66) %&gt;%\n    ungroup() %&gt;%\n    as_survey_design(\n      strata = stype\n    ) %&gt;%\n    svyglm(api00 ~ emer + meals + hsg + avg.ed + comp.imp +\n      acs.k3 + acs.46 + ell + full + enroll, design = .) %&gt;%\n    coef(.)\n  return(coefs)\n}\ncoef_dist &lt;- replicate(100, OneSimulation())\npar(mfrow = c(1, 2))\nhist(coef_dist[1, ], main = \"Histogram of Intercept\")\nhist(coef_dist[2, ], main = \"Histogram of emer\")\n\n\n\n\nYes, as we’d expect the sampling distributions are roughly normal.\n\nUsing the blood pressure data from NHANES 2003 - 2006, investigate the effect of obesity on blood pressure using the Body Mass Index and blood pressure data.\n\n\nWhat variables in the data set are potential confounders?\n\nGiven the discussion in the book’s example, sodium and potassium intake are likely confounders alongside the usual, age, sex race and socioeconomic status. That said, i don’t know if the last of these two are available given that the variable names in the dataset are not particularly descriptive.\n\nAre there important confounders that are not measured?\n\nSee above - race and socioeconomic status stand out as two confounding variables that don’t appear to be measured in this dataset.\n\nFit one or more suitable regression models and summarize the output.\n\nI’ll fit the same model as model2 in the text since Lumley explains what the variables are in that model.\n\nShow the codefit &lt;- svyglm(BPXSAR ~ sodium + potassium + RIDAGEYR + RIAGENDR + BMXBMI,\n  design = des\n)\nsummary(fit)\n\n\nCall:\nsvyglm(formula = BPXSAR ~ sodium + potassium + RIDAGEYR + RIAGENDR + \n    BMXBMI, design = des)\n\nSurvey design:\nupdate(des, sodium = DR1TSODI/1000, potassium = DR1TPOTA/1000)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 97.32903    1.42668  68.220  &lt; 2e-16 ***\nsodium       0.43458    0.16164   2.689   0.0126 *  \npotassium   -0.96119    0.17043  -5.640 7.19e-06 ***\nRIDAGEYR     0.45791    0.01080  42.380  &lt; 2e-16 ***\nRIAGENDR    -3.38208    0.38403  -8.807 3.90e-09 ***\nBMXBMI       0.38460    0.03797  10.129 2.48e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 263.5461)\n\nNumber of Fisher Scoring iterations: 2\n\n\nAccording to this model there is a .38 mm Hg expected increase in systolic blood pressure for every one unit increase in BMI after adjusting for age, sex, and daily sodium and potassium intake. In other words we’d expect a higher blood pressure amongst those with higher BMIs.\n\nExamine whether there is an interaction with age or sex.\n\n\nShow the codefit &lt;- svyglm(BPXSAR ~ (RIDAGEYR + RIAGENDR) * I(BMXBMI - 25) + sodium + potassium,\n  design = des\n)\nsummary(fit)\n\n\nCall:\nsvyglm(formula = BPXSAR ~ (RIDAGEYR + RIAGENDR) * I(BMXBMI - \n    25) + sodium + potassium, design = des)\n\nSurvey design:\nupdate(des, sodium = DR1TSODI/1000, potassium = DR1TPOTA/1000)\n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             107.178639   1.200310  89.292  &lt; 2e-16 ***\nRIDAGEYR                  0.462853   0.011268  41.076  &lt; 2e-16 ***\nRIAGENDR                 -3.485444   0.353566  -9.858 1.00e-09 ***\nI(BMXBMI - 25)            0.510054   0.104374   4.887 6.18e-05 ***\nsodium                    0.438810   0.163223   2.688 0.013119 *  \npotassium                -0.989246   0.169362  -5.841 5.95e-06 ***\nRIDAGEYR:I(BMXBMI - 25)  -0.005031   0.001305  -3.855 0.000806 ***\nRIAGENDR:I(BMXBMI - 25)   0.041993   0.059821   0.702 0.489736    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 263.1093)\n\nNumber of Fisher Scoring iterations: 2\n\n\nI center the BMI variable at 25 (roughly the marginal average) and fit the model with the age and sex interactions with the centered BMI. The model fit shows that there’s a significant negative association with the BMI-age interaction and no association with the gender/sex - age interaction. This corresponds to a amplifying effect of age — the older you are the lower your blood pressure is, on average, a higher BMI will then also lead to a lower expected blood pressure in addition to this affect.\n\nProve that an unweighted regression estimator is approximately unbiased when the weights depend only on variables in the model. Specifically, if the true population regression coefficients \\beta^* satisfy:\n\n\n\\sum_{i=1}^{N} x_i(y_i - x_i\\beta^*) = 0\n\nand R_i indicates that observation i is in the sample prove that\n\nE \\left [ \\sum_{i=1}^{N} R_ix_i (y_i - x_i \\beta^*)\\right ] = 0\n\nso that the un-weighted sample estimating equations are unbiased.\nIf we take the expectation over R_i in the last formula we get \n\\sum_{i=1}^{N} \\pi_ix_i (y_i - x_i \\beta^*)  = 0,\n\nwhere \\pi_i = E[R_i], the probability of being included in the sample. At this point it isn’t immediately clear to me how to proceed. One easy result is that if \\pi_i = \\pi \\forall i, then we have \n\\pi \\sum_i^N x_i(y_i - x_i\\beta^*) = 0 \\\\\n\\iff\n\\sum_i^N x_i(y_i - x_i\\beta^*) = 0\n\nwhich proves the desired claim.\n5.8 A rough approximation to the loss of efficiency from unnecessarily using weights can be constructed by considering the variance of the residuals in weighted and un-weighted estimation. Assume as an approximation that the residuals r_i are independent of the sampling weights w_i\n\nShow that \nV[\\sum_{i=1}^{n} w_i r_i] = E[w^2] V[r] + V[w]E[r^2]\n\n\n\nI show the proof below, though I think Lumley forgot a factor of n. This doesn’t change anything substantial.\n\nV[\\sum_{i=1}^{n} w_i r_i] \\stackrel{ind}{=} \\sum_{i=1}^{n} V[w_ir_i] \\\\\n\\stackrel{id}{=} n (E[(wr)^2] -  E[wr]^2) \\\\\n\\stackrel{ind}{=} n (E[w^2r^2] -  E[w]^2E[r]^2) \\\\\n\\stackrel{ind}{=} n (E[w^2]E[r^2] -  E[w]^2E[r]^2) \\\\\n= n(E[w^2](E[r^2] - E[r]^2)  + (E[w^2]E[r]^2 - E[w]^2E[r]^2)) \\\\\n= n(E[w^2V[r] + V[w]E[r^2]) \\\\\n\\blacksquare\n\n\nNow assume that the mean of the residuals is zero, and show that the relative efficiency of the un-weighted estimate is 1 + cv(w) where cv is the coefficient of variation, the ratio of the standard deviation to the mean.\n\nFirst note that E[r] = 0 \\implies V[r] = E[r^2]. Then the expression from the first part — now omitting n — reduces to:\n\nE[w^2]E[r^2] + V[w]E[r^2] = E[r^2](E[w^2] + V[w])\n If we take the ratio of this to the variance of the un-weighted residuals we’d get\n\n\\frac{V[\\sum_{i=1}^{n}r_iw_i]}{V[\\sum_{i=1}^{n} r_i]} = \\frac{E[r^2](E[w^2] + V[w])}{E[r^2] - E[r]^2} \\\\\n= (E[w^2] + V[w]) \\\\\n= 1 + \\frac{V[w]}{E[w^2]}\n\nAt this point I’d like to say that you can take the square root of the term on the right, but that’s not quite accurate and doesn’t make sense algebraically. To get to the mean we’d need to have E[w]^2 in the denominator which we don’t quite have."
  },
  {
    "objectID": "ComplexSurveyNotes.html#logistic-regression",
    "href": "ComplexSurveyNotes.html#logistic-regression",
    "title": "Complex Survey Notes",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLumley gives a brief overview of logistic regression which is roughly equivalent to what can be found on wikipedia, so I won’t reiterate his points here. The main take home is that while the interpretation of the coefficients change from a linear association to an odds ratio association the only change to svyglm() is adding the family = quasibinomial() option.\nExample: Internet use in Scotland\nTo demonstrate how to analyze binary data, Lumley uses the Scottish Household Survey data, examining internet use amongst the nation’s populace.\n\nShow the codeload(\"Data/SHS/shs.rda\") # Lumley's website data\npar(mfrow = c(2, 1))\nbys &lt;- svyby(~intuse, ~ age + sex, design = shs, svymean)\nplot(\n  svysmooth(intuse ~ age,\n    design = subset(shs, sex == \"male\" & !is.na(age)),\n    bandwidth = 5\n  ),\n  ylim = c(0, 0.8), ylab = \"% Using Internet\",\n  xlab = \"Age\"\n)\nlines(svysmooth(intuse ~ age,\n  design = subset(shs, sex == \"female\" & !is.na(age)),\n  bandwidth = 5\n), lwd = 2, lty = 3)\npoints(bys$age, bys$intuse, pch = ifelse(bys$sex == \"male\", 19, 1))\nlegend(\"topright\",\n  pch = c(19, 1), lty = c(1, 3), lwd = c(1, 2),\n  legend = c(\"Male\", \"Female\"), bty = \"n\"\n)\nbyinc &lt;- svyby(~intuse, ~ sex + groupinc, design = shs, svymean)\nbarplot(byinc, xlab = \"Income\", ylab = \"% Using Internet\")\n\n\n\n\nSince binary data can’t be easily visualized across a continuous variable very easily, the plots above use both smoothing and binning — computing the proportion point estimate within some range of age — to understand how internet use changes across these continuous dimensions.\nWe can see in the plot that internet use is lower amongst the older respondents in the survey and men ten to use the internet more then women, except perhaps the very youngest. My phrasing here is intentional, to demonstrate that the phenomenon we’re observing is likely a cohort effect since, as Lumley notes, its likely that born before the arrival of the internet are less likely to use it.\nWe see this same pattern in the box plot showing internet use across income; men using internet more than women. We also see that those with higher incomes tend to use the internet more than those with lower incomes. In contrast to the cohort effect seen above, Lumley suggests that income may be more of a real effect, and those who start earning more may be more likely to use the internet.\nLumley then fits a series of models to quantify the relationships we’re seeing in the plots formally. I’ll summarize these briefly here and fit them below.\n\nModel 1 estimates the log odds of internet as a linear (on the log odds scale) function of age, sex and their interaction.\nModel 2 is the same as Model 1 except with two slopes for those younger and older than age 35, respectively — a low dimensional way to account for the nonlinear shape we observed previously.\nModel 3 is the same as Model 2 but adds an additional fixed effect term for income.\nModel 4 now adds an interaction between income and sex to account for the differences.\n\nLumley examines the output of all the models and for models 2 and 3 examines what the two age-slopes for the reference group (women) is by using svycontrast(). Similarly with model 4, Lumley tests whether the 5 additional parameters added as the result of the income / sex interaction lead to better model fit. As we can see below, they do.\n\nShow the codem &lt;- svyglm(intuse ~ I(age - 18) * sex, design = shs, family = quasibinomial())\n\nm2 &lt;- svyglm(intuse ~ (pmin(age, 35) + pmax(age, 35)) * sex,\n  design = shs,\n  family = quasibinomial()\n)\nsummary(m)\n\n\nCall:\nsvyglm(formula = intuse ~ I(age - 18) * sex, design = shs, family = quasibinomial())\n\nSurvey design:\nsvydesign(id = ~psu, strata = ~stratum, weight = ~grosswt, data = ex2)\n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            0.804113   0.047571  16.903  &lt; 2e-16 ***\nI(age - 18)           -0.044970   0.001382 -32.551  &lt; 2e-16 ***\nsexfemale             -0.116442   0.061748  -1.886   0.0594 .  \nI(age - 18):sexfemale -0.010145   0.001864  -5.444 5.33e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 0.950831)\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nShow the codesummary(m2)\n\n\nCall:\nsvyglm(formula = intuse ~ (pmin(age, 35) + pmax(age, 35)) * sex, \n    design = shs, family = quasibinomial())\n\nSurvey design:\nsvydesign(id = ~psu, strata = ~stratum, weight = ~grosswt, data = ex2)\n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              2.152291   0.156772  13.729  &lt; 2e-16 ***\npmin(age, 35)            0.014055   0.005456   2.576 0.010003 *  \npmax(age, 35)           -0.063366   0.001925 -32.922  &lt; 2e-16 ***\nsexfemale                0.606718   0.211516   2.868 0.004133 ** \npmin(age, 35):sexfemale -0.017155   0.007294  -2.352 0.018691 *  \npmax(age, 35):sexfemale -0.009804   0.002587  -3.790 0.000151 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 0.9524217)\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nShow the codesvycontrast(m2, quote(`pmin(age, 35)` + `pmin(age, 35):sexfemale`))\n\n           nlcon     SE\ncontrast -0.0031 0.0049\n\n\n\nShow the codesvycontrast(m2, quote(`pmax(age, 35)` + `pmax(age, 35):sexfemale`))\n\n            nlcon     SE\ncontrast -0.07317 0.0018\n\n\n\nShow the codeshs &lt;- update(shs, income = relevel(groupinc, ref = \"under 10K\"))\nm3 &lt;- svyglm(intuse ~ (pmin(age, 35) + pmax(age, 35)) * sex + income,\n  design = shs, family = quasibinomial()\n)\nsummary(m3)\n\n\nCall:\nsvyglm(formula = intuse ~ (pmin(age, 35) + pmax(age, 35)) * sex + \n    income, design = shs, family = quasibinomial())\n\nSurvey design:\nupdate(shs, income = relevel(groupinc, ref = \"under 10K\"))\n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              1.275691   0.179902   7.091 1.41e-12 ***\npmin(age, 35)           -0.009041   0.006170  -1.465  0.14286    \npmax(age, 35)           -0.049408   0.002124 -23.259  &lt; 2e-16 ***\nsexfemale                0.758883   0.235975   3.216  0.00130 ** \nincomemissing            0.610892   0.117721   5.189 2.15e-07 ***\nincome10-20K             0.533093   0.048473  10.998  &lt; 2e-16 ***\nincome20-30k             1.246396   0.052711  23.646  &lt; 2e-16 ***\nincome30-50k             2.197628   0.063644  34.530  &lt; 2e-16 ***\nincome50K+               2.797022   0.132077  21.177  &lt; 2e-16 ***\npmin(age, 35):sexfemale -0.023225   0.008137  -2.854  0.00432 ** \npmax(age, 35):sexfemale -0.008103   0.002858  -2.835  0.00459 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 0.9574657)\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nShow the codem4 &lt;- svyglm(intuse ~ (pmin(age, 35) + pmax(age, 35)) * sex + income * sex,\n  design = shs, family = quasibinomial()\n)\nregTermTest(m4, ~ income:sex)\n\nWald test for income:sex\n in svyglm(formula = intuse ~ (pmin(age, 35) + pmax(age, 35)) * sex + \n    income * sex, design = shs, family = quasibinomial())\nF =  1.872811  on  5  and  11641  df: p= 0.095485 \n\n\nDouble check Lumley’s assertions made on linear vs. logistic regression here.\nRelative Risk Regression\nLogistic regression gets its name from the “link” function that determines the scale on which outcome mean is modeled and estimated. For a logistic function the link is the “logit” function, which models the log odds. Other link functions are also possible. In particular, using the log() link function models the relative risk i.e. \\log(P(Y=1)) = X\\beta.\nLumley goes into the details of how to fit these models via svyglm and the potential difficulties of estimating the relative risk between the two distributional families. Of note, because the binomial family is more restrictive, estimation is more sensitive to the fitting algorithm’s parameters starting values and\n\nShow the coderr3 &lt;- svyglm(intuse ~ (pmin(age, 35) + pmax(age, 35)) * sex + income,\n  design = shs, family = quasibinomial(log),\n  start = c(-0.5, rep(0, 10))\n)\nrr4 &lt;- svyglm(intuse ~ (pmin(age, 35) + pmax(age, 35)) * sex + income,\n  design = shs, family = quasipoisson(log)\n)\nsummary(rr3)\n\n\nCall:\nsvyglm(formula = intuse ~ (pmin(age, 35) + pmax(age, 35)) * sex + \n    income, design = shs, family = quasibinomial(log), start = c(-0.5, \n    rep(0, 10)))\n\nSurvey design:\nupdate(shs, income = relevel(groupinc, ref = \"under 10K\"))\n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             -0.455448   0.092857  -4.905 9.48e-07 ***\npmin(age, 35)            0.002853   0.003017   0.946    0.344    \npmax(age, 35)           -0.026658   0.001260 -21.149  &lt; 2e-16 ***\nsexfemale                0.623713   0.112658   5.536 3.16e-08 ***\nincomemissing            0.489194   0.079296   6.169 7.09e-10 ***\nincome10-20K             0.416094   0.036710  11.335  &lt; 2e-16 ***\nincome20-30k             0.820195   0.036838  22.265  &lt; 2e-16 ***\nincome30-50k             1.124631   0.037188  30.241  &lt; 2e-16 ***\nincome50K+               1.247014   0.044928  27.756  &lt; 2e-16 ***\npmin(age, 35):sexfemale -0.007584   0.003801  -1.995    0.046 *  \npmax(age, 35):sexfemale -0.012816   0.001721  -7.446 1.03e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 0.9256337)\n\nNumber of Fisher Scoring iterations: 16\n\n\n\nShow the codesummary(rr4)\n\n\nCall:\nsvyglm(formula = intuse ~ (pmin(age, 35) + pmax(age, 35)) * sex + \n    income, design = shs, family = quasipoisson(log))\n\nSurvey design:\nupdate(shs, income = relevel(groupinc, ref = \"under 10K\"))\n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             -0.1679519  0.0846956  -1.983  0.04739 *  \npmin(age, 35)           -0.0001441  0.0024776  -0.058  0.95363    \npmax(age, 35)           -0.0312057  0.0012654 -24.660  &lt; 2e-16 ***\nsexfemale                0.5946038  0.1036091   5.739 9.77e-09 ***\nincomemissing            0.4673470  0.0791934   5.901 3.71e-09 ***\nincome10-20K             0.4152093  0.0367406  11.301  &lt; 2e-16 ***\nincome20-30k             0.8348862  0.0369330  22.605  &lt; 2e-16 ***\nincome30-50k             1.2039269  0.0368608  32.661  &lt; 2e-16 ***\nincome50K+               1.3601811  0.0429853  31.643  &lt; 2e-16 ***\npmin(age, 35):sexfemale -0.0087552  0.0033861  -2.586  0.00973 ** \npmax(age, 35):sexfemale -0.0116652  0.0017556  -6.645 3.18e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 0.5969912)\n\nNumber of Fisher Scoring iterations: 5\n\n\nOrdinal Regression\nThe logit function introduced in the first section can be extended for use from binomial outcome data to categorical – typically ordinal, or ordered categorical data. Unfortunately, the data that Lumley uses for this section is not available so I’ll skip any notes on this, noting that the model interpretation for logit ordinal regression is the same as for a typical iid logit ordinal model.\n\nShow the code## Data isn't available on lumley's website - or at least the data that is\n# there associated with \"nhanes3\" does not contain the variables below.\ndhanes &lt;- svydesign(\n  id = ~SDPPSU6, strat = ~SDPSTRA6,\n  weight = ~WTPFHX6,\n  ## nest = TRUE indicates the PSU identifier is nested\n  ## within stratum - repeated across strata\n  nest = TRUE,\n  data = subset(nhanes3, !is.na(WTPFHX6))\n)\ndhanes &lt;- update(dhanes, fpg = ifelse(phpfast &gt;= 8 & gip &lt; 8E3, gip, NA))\ndhanes &lt;- update(dhanes,\n  diab = cut(fpg, c(0, 110, 125, Inf)),\n  diabi = cut(fpg, c(0, 100, 125, Inf))\n)\ndhanes &lt;- update(dhanes,\n  cadmium = ifelse(upd &lt; 88880, udp, NA),\n  creatinine = ifelse(urp &lt; 88880, urp, NA)\n)\ndhanes &lt;- update(dhanes,\n  age = ifelse(hsaitmor &gt; 1000, NA, hsaitmor / 12)\n)\n\nmodel0 &lt;- svyolr(diab ~ cadmium + creatine, design = dhanes)\n\n\nOther cumulative Link models\nIn this subsection Lumley discusses the log-log link function. The log log, probit, and cauchit or inverse Cauchy link function are all supported by the svyolr function and are collectively referred to as cumulative link models."
  },
  {
    "objectID": "ComplexSurveyNotes.html#loglinear-models",
    "href": "ComplexSurveyNotes.html#loglinear-models",
    "title": "Complex Survey Notes",
    "section": "Loglinear Models",
    "text": "Loglinear Models\nLumley introduces log linear models in the context of the chi-square tests (via svychisq()) introduced previously testing association between count variables. These functions test a similar hypothesis using different methods and I’d suggest looking at the function documentation closely to determine if choosing between the two would matter in a particular circumstance.\n\nShow the codedroplevels &lt;- function(f) as.factor(as.character(f))\nchis &lt;- update(chis, smoking = droplevels(smoking), ins = droplevels(ins))\nnull &lt;- svyloglin(~ smoking + ins, chis)\n# Dot below includes all previous variables\nsaturated &lt;- update(null, ~ . + smoking:ins)\nanova(null, saturated)\n\nAnalysis of Deviance Table\n Model 1: y ~ smoking + ins\nModel 2: y ~ smoking + ins + smoking:ins \nDeviance= 659.1779 p= 1.850244e-82 \nScore= 694.3208 p= 9.032412e-168 \n\n\nThe anova output above shows two p-values according to two test statistics, one the deviance and the other the score. Again, the difference here is quite “in the weeds” of how test statistic distributions are computed so I’d suggest consulting a general reference on generalized linear models like (Dobson and Barnett 2018) and/or the function documentation.\n\nShow the codesvychisq(~ smoking + ins, chis)\n\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  svychisq(~smoking + ins, chis)\nF = 130.11, ndf = 1.9923, ddf = 157.3884, p-value &lt; 2.2e-16\n\n\n\nShow the codepf(130.1143, 1.992, 157.338, lower.tail = FALSE)\n\n[1] 5.37419e-34\n\n\n\nShow the codesvychisq(~ smoking + ins, chis, statistic = \"Chisq\")\n\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  svychisq(~smoking + ins, chis, statistic = \"Chisq\")\nX-squared = 694.32, df = 2, p-value &lt; 2.2e-16\n\n\n\nShow the codesummary(null)\n\nLoglinear model: svyloglin(~smoking + ins, chis)\n               coef         se            p\nsmoking1 -0.6209497 0.01251103 0.000000e+00\nsmoking2 -0.1391308 0.01066059 6.275861e-39\nins1      0.8246480 0.01015826 0.000000e+00\n\n\n\nShow the codesummary(saturated)\n\nLoglinear model: update(null, ~. + smoking:ins)\n                    coef         se             p\nsmoking1      -0.4440871 0.01612925 7.065991e-167\nsmoking2      -0.3086129 0.01735214  9.187439e-71\nins1           0.8052182 0.01151648  0.000000e+00\nsmoking1:ins1 -0.2821710 0.01847008  1.085166e-52\nsmoking2:ins1  0.2510967 0.01875679  7.205759e-41\n\n\n\nShow the codemodel.matrix(saturated)\n\n  (Intercept) smoking1 smoking2 ins1 smoking1:ins1 smoking2:ins1\n1           1        1        0    1             1             0\n2           1        0        1    1             0             1\n3           1       -1       -1    1            -1            -1\n4           1        1        0   -1            -1             0\n5           1        0        1   -1             0            -1\n6           1       -1       -1   -1             1             1\nattr(,\"assign\")\n[1] 0 1 1 2 3 3\nattr(,\"contrasts\")\nattr(,\"contrasts\")$smoking\n[1] \"contr.sum\"\n\nattr(,\"contrasts\")$ins\n[1] \"contr.sum\"\n\n\nChoosing Models\nLumley explores model selection beyond just statistical testing by discussing graphical and hierarchical loglinear models. My sense is that the discussion here is not complete and I’d encourage any reader here to consult the references he points to for further reading on the topic.\nExample: neck and back pain in NHIS\nI couldn’t find the data associated with Lumley’s example on his website or from a cursory Google search for NHIS data. Furthermore, I found it difficult to parse some of Lumley’s words here. For example, for those reading the book, the two graphical models listed in Figure 6.11 look identical to me when the text states that the graphics are supposed to correspond to two different models.\nLinear Association Models\nThe log linear models can be used to test linear association of ordered categorical variables by coding the variables with ordered numbers and then running the loglinear tests discussed previously. Lumley demonstrates with the Scottish Household Survey data.\nLumley first starts by looking at internet use, now an ordinal categorical variable, rc5 across income levels as before. Again, the similarity to the chi-square model is obvious, as we’re looking at models with fixed effects only — a “null” model — and comparing it to a “saturated” model — one with interaction effects. The anova() call below, tests whether the additional parameters estimated by the interaction model provide a greater model fit than the model with the fixed effects alone. The idea being that if it does, then there’s evidence to support the hypothesis that internet use varies across income levels, and since we’re measuring internet use on an ordinal scale we have evidence that more income corresponds to more internet use. Note that below, only the models fit with the as.numeric() pieces are testing linear association on the ordinal scale, while the others are a simple categorical test.\n\nShow the codeshs &lt;- update(shs, income = ifelse(groupinc == \"missing\", NA, groupinc))\nnull &lt;- svyloglin(~ rc5 + income, shs)\nsaturated &lt;- update(null, ~ .^2)\nanova(null, saturated)\n\nAnalysis of Deviance Table\n Model 1: y ~ rc5 + income\nModel 2: y ~ rc5 + income + rc5:income \nDeviance= 289.2165 p= 6.429997e-26 \nScore= 282.5453 p= 0 \n\n\nFor the first model fit, we have a low p-value, so we reject the null model which make sense substantively. We’ve already seen that income is associated with internet use in the previous analysis.\n\nShow the codelin &lt;- update(null, ~ . + as.numeric(rc5):as.numeric(income))\nanova(null, lin)\n\nAnalysis of Deviance Table\n Model 1: y ~ rc5 + income\nModel 2: y ~ rc5 + income + as.numeric(rc5):as.numeric(income) \nDeviance= 105.3609 p= 1.305691e-21 \nScore= 105.6157 p= 1.152954e-21 \n\n\nNow the model is updated with the ordinal categorical variable of internet use and income, again we have a significant result.\n\nShow the codeanova(lin, saturated)\n\nAnalysis of Deviance Table\n Model 1: y ~ rc5 + income + as.numeric(rc5):as.numeric(income)\nModel 2: y ~ rc5 + income + rc5:income \nDeviance= 183.8556 p= 4.346664e-14 \nScore= 184.0606 p= 5.465958e-182 \n\n\nNow we compare the ordinal model to the categorical, non-ordered model. It would make sense that the ordinal model provides a better fit than the categorical, and although that’s what we see in the book it isn’t what we see when I run the code, even with the same Deviance and Score test statistics… I suspect this is a bug. The survey R package isn’t hosted on github but I’ll try to raise an issue with Lumley via email.\n\nShow the codeshs &lt;- update(shs, agegp = cut(age, c(20, 40, 60, 100)))\nnull &lt;- svyloglin(~ agegp + rc5, shs)\nsat &lt;- update(null, ~ .^2)\nlin &lt;- update(null, ~ . + as.numeric(rc5):as.numeric(agegp))\nanova(null, lin)\n\nAnalysis of Deviance Table\n Model 1: y ~ agegp + rc5\nModel 2: y ~ agegp + rc5 + as.numeric(rc5):as.numeric(agegp) \nDeviance= 350.1315 p= 1.163786e-73 \nScore= 340.592 p= 1.284355e-71 \n\n\nNow we look at the association of internet use with age tested via the log linear models. Again, we find that the ordinal interaction model has a better fit to the data according to the Deviance and Score tests.\n\nShow the codeanova(lin, sat)\n\nAnalysis of Deviance Table\n Model 1: y ~ agegp + rc5 + as.numeric(rc5):as.numeric(agegp)\nModel 2: y ~ agegp + rc5 + agegp:rc5 \nDeviance= 45.60211 p= 0.02538278 \nScore= 48.02927 p= 7.788495e-09 \n\n\nLooking at the ordinal vs. categorical model comparison we again see a discrepancy between my results and Lumleys, with mine suggesting that the ordinal model does not provide a better fit than the categorical model…\n\nShow the code# code for producing Table 6.1\nnull &lt;- svyloglin(~ agegp + income + rc5, shs)\nm1 &lt;- update(null, ~ . + as.numeric(agegp):as.numeric(rc5))\nm2 &lt;- update(m1, ~ . + agegp:income)\nm3 &lt;- update(m2, ~ . + as.numeric(income):as.numeric(rc5))\nm4 &lt;- update(m2, ~ . + income:rc5)\nfull &lt;- update(null, ~ .^2)\n\n\nAs a final step in this demonstration, Lumley then looks at each possible model that has both interactions, noting that an interaction between age group and income isn’t possible because of sparsity of data amongst some income / age group categories.\nLooking at Table 6.1 in the book, Lumley examines the ratio of deviance to degrees of freedom to provide a heuristic of determining whether the full model fits well. I’m not fully convinced of his discussion here and there, again, appears to be an error in his table."
  },
  {
    "objectID": "ComplexSurveyNotes.html#exercises-5",
    "href": "ComplexSurveyNotes.html#exercises-5",
    "title": "Complex Survey Notes",
    "section": "Exercises",
    "text": "Exercises\n\nUsing the same data as in Section 5.2.4, define hypertension as systolic blood pressure greater than 140 mm Hg or diastolic blood pressure greater than 90 mmHg. Fit logistic regression models to investigate the association between dietary sodium and potassium and hypertension.\n\n\nShow the codedes &lt;- svydesign(\n  id = ~SDMVPSU, strat = ~SDMVSTRA, weights = ~fouryearwt,\n  nest = TRUE, data = subset(nhanes, !is.na(WTDRD1))\n)\n\ndes &lt;- update(des,\n  sodium = DR1TSODI / 1000, potassium = DR1TPOTA / 1000,\n  hypertension = (BPXSAR &gt; 140 | BPXDAR &gt; 90) * 1\n)\nsummary(svyglm(hypertension ~ sodium + potassium, design = des))\n\n\nCall:\nsvyglm(formula = hypertension ~ sodium + potassium, design = des)\n\nSurvey design:\nupdate(des, sodium = DR1TSODI/1000, potassium = DR1TPOTA/1000, \n    hypertension = (BPXSAR &gt; 140 | BPXDAR &gt; 90) * 1)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.162091   0.010856  14.931 7.33e-15 ***\nsodium      -0.014148   0.003770  -3.753 0.000811 ***\npotassium    0.010877   0.005121   2.124 0.042645 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1361541)\n\nNumber of Fisher Scoring iterations: 2\n\n\nWe see sodium intake is negatively associated with hypertension while potassium is positively associated. Because we’re not adjusting for age like we saw previously, the sodium intake coefficient is biased up from its adjusted rate.\n\nThis exercise uses the WA State Crime data for 2004 as the population. The data consists of crime rates and population size for the police districts in cities/towns and sheriffs’ offices in unincorporated areas, grouped by county.\n\n\nTake a simple random sample of 10 counties from the state and use all the data from the sampled counties. Estimate the total number of murders and burglaries in the state.\n\n\nShow the codecounty_sample &lt;- wa_crime_df %&gt;%\n  distinct(County) %&gt;%\n  slice_sample(n = 10) %&gt;%\n  pull(County)\n\nwa_crime_df %&gt;%\n  filter(County %in% county_sample) %&gt;%\n  as_survey_design(\n    ids = c(County, Agency),\n    fpc = c(num_counties, num_agencies)\n  ) %&gt;%\n  summarize(total = survey_total(murder_and_crime)) %&gt;%\n  mutate(Q = \"a\")\n\n# A tibble: 1 × 3\n   total total_se Q    \n   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n1 19566.    5414. a    \n\n\n\nFit a poisson regression (family=quasipoisson) to model the relationship between number of murders and population. Poisson regression fits a linear model to the logarithm of the mean of the outcome variable. If the murder rate were constant, the optimal transformation of the predictor variable would be the logarithm of population, and its coefficient would be 1.0. Is this supported by the data?\n\n\nShow the codesrs_design &lt;- wa_crime_df %&gt;%\n  filter(County %in% county_sample) %&gt;%\n  as_survey_design(\n    id = c(County, Agency),\n    fpc = c(num_counties, num_agencies)\n  )\n\npoisson_fit &lt;- svyglm(murder ~ I(log(Population)),\n  family = quasipoisson,\n  design = srs_design\n)\nsummary(poisson_fit)\n\n\nCall:\nsvyglm(formula = murder ~ I(log(Population)), design = srs_design, \n    family = quasipoisson)\n\nSurvey design:\nCalled via srvyr\n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)         -9.8678     2.0628  -4.784  0.00138 **\nI(log(Population))   0.9508     0.2092   4.546  0.00189 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 0.9839811)\n\nNumber of Fisher Scoring iterations: 6\n\n\nYes. We see that the estimate is 1.13 which is roughly 1 and within the standard error. Furthermore, the t-test shows that there is evidence this estimate is not 0 at \\alpha = 0.05.\n\nPredict the total number of murders in the state using the Poisson regression model. Compare to a ratio estimator using population as the auxiliary variable.\n\n\nShow the codetrue_murder_count &lt;- sum(wa_crime_df$murder)\ntotal_population &lt;- sum(wa_crime_df$Population)\n# doesn't work\n# predict(poisson_fit, newdata = wa_crime_df, type = \"response\",\n#        total = sum(wa_crime_df$Population))\n# Works - for mean estimate, care needed for summing variance\nsum(predict(poisson_fit, newdata = wa_crime_df, type = \"response\"))\n\n[1] 181.9975\n\n\n\nShow the codepredict(\n  svyratio(~murder, ~Population,\n    design = srs_design\n  ),\n  total = sum(wa_crime_df$Population)\n)\n\n$total\n       Population\nmurder    193.091\n\n$se\n       Population\nmurder   30.04947\n\n\nWe see that while both estimates are reasonably close to the true state 2004 murder count of 189, the ratio estimator offers a readily available standard error while its not clear how to extract the same estimate from the svyglm output. Note that the same code arguments used extract totals in Lumley’s election example does not appear to work here…\n\nTake simple random samples of five police districts from King County and five counties from the rest of the state. Fit a Poisson regression model with population and stratum (King County vs elsewhere) as predictors. Predict the total number of murders in the state.\n\n\nShow the codeagency_sample &lt;- wa_crime_df %&gt;%\n  filter(County == \"king\") %&gt;%\n  distinct(Agency) %&gt;%\n  slice_sample(n = 10) %&gt;%\n  pull(Agency)\n\nnon_king_county_sample &lt;- wa_crime_df %&gt;%\n  filter(County != \"king\") %&gt;%\n  distinct(County) %&gt;%\n  slice_sample(n = 10) %&gt;%\n  pull(County)\n\ncounty_list &lt;- unique(wa_crime_df$County)\n\nstrata_design &lt;- wa_crime_df %&gt;%\n  filter(County %in% non_king_county_sample | Agency %in% agency_sample) %&gt;%\n  mutate(\n    strata_label = if_else(County == \"king\", \"strata 1\", \"strata 2\"),\n    num_counties = if_else(County == \"king\", 1, length(county_list) - 1)\n  ) %&gt;%\n  as_survey_design(\n    id = c(County, Agency),\n    fpc = c(num_counties, num_agencies),\n    strata = strata_label\n  )\n\n\npoisson_fit &lt;- svyglm(murder ~ I(log(Population)) + strata_label,\n  design = strata_design,\n  family = quasipoisson\n)\n# doesn't produce appropriate output\n# predict(poisson_fit, total = total_population)\nsum(predict(poisson_fit,\n  newdata = wa_crime_df %&gt;% mutate(strata_label = if_else(County == \"king\", \"strata 1\", \"strata 2\")),\n  type = \"response\"\n))\n\n[1] 186.9499\n\n\nThis estimate appears to again, make sense though we have no standard error estimate readily available to compare with the previous. One would guess that it would be more precise, given extra information that comes from including the King County agencies.\n\nThe variable MISEFFRT asks “How often in the past 30 days did you feel that everything was an effort?”, on a 1-5 scale with 1 meaning “All” and 5 meaning “none”. Investigate whether this variable varies seasonally and whether it peaks in winter by defining predictor variables cos(IMONTH * 0.5236) and sin(IMONTH * 0.5236), which describe smooth anual cycles and fitting:\n\n\nA logistic regression model with outcome MISEFFRT = 5\n\nA linear regresion model with outcome MISEFFRT\nWhat further modeling could you do to investigate whether sunlight intensity was related to this seasonal variation.\n\nIt isn’t clear to what dataset Lumley is referring to here… so I’ll leave this question unanswered.\n\nUsing the California Health Interview Study 2005 data,\n\n\nFit a logistic regression model to the relationship between the probability of having health insurance (ins) and household annual income (ak22_p). Important potential confounders include age (srage_p), sex (srsex) and race (racecen) and interactions may also be important.\n\n\nShow the codefit &lt;- svyglm(I((ins == 1) * 1) ~ ak22_p + srage_p + srsex,\n  design = chis,\n  family = quasibinomial\n)\nsummary(fit)\n\n\nCall:\nsvyglm(formula = I((ins == 1) * 1) ~ ak22_p + srage_p + srsex, \n    design = chis, family = quasibinomial)\n\nSurvey design:\nupdate(chis, smoking = droplevels(smoking), ins = droplevels(ins))\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.269e+00  7.360e-02  -17.24   &lt;2e-16 ***\nak22_p       2.061e-05  1.032e-06   19.97   &lt;2e-16 ***\nsrage_p      4.035e-02  1.150e-03   35.09   &lt;2e-16 ***\nsrsexFEMALE  4.785e-01  4.368e-02   10.96   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 1.591744)\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nWhat would be the impact on the interpretation of the income coefficient of adding a variable to the model indicating whether an employer provided health insurance?\n\nPresumably income is correlated with employee offered healthcare insurance, though I don’t know this for a fact. If this were the case then we’d see that the effect of the income coefficient would be attenuated (move towards zero) since its effect would now be more precisely captured by the employee offered health care measurement.\n\nUsing the California Health Interview Study 2005 data, fit a relative risk regression model to the relationship between the probability of having health insurance (ins) and household annual income (ak22_p), age (srage_p), sex (srsex) and race (racecen) . Compare the coefficients to those from a logistic regression.\n\n\nShow the codefit &lt;- svyglm(I((ins == 1) * 1) ~ ak22_p,\n  design = chis,\n  family = quasibinomial(link = \"log\"),\n  start = c(-.5, 0),\n  control = list(maxit = 150)\n)\nsummary(fit)\n\n\nDespite trying several different starting values, “max iterations”, and model specifications, I wasn’t able to fit a model as specified without getting warning messages or errors. In general if the model can be fit, the exponentiated intercept represents the adjusted probability of the event occuring — in this case having health insurance — and each subsequent exponentiated coefficient represents the increase in probability, or relative risk of the event occuring, conditional on a one unit change in value of the covariate.\n\nUsing the same data as in Section 5.2.4, create an ordinal blood pressure variable based on systolic and diastolic pressure with categories “normal” (systolic &lt; 120 and diastolic &lt; 80), “prehypertension” (systolic &lt; 140, diastolic &lt; 90), “hypertension stage 1” (systolic &lt; 160, diastolic &lt; 100), and “hypertension stage 2” ( systolic at least 160 or diastolic at least 100). Fit proportional odds regression models to investigate the association between dietary sodium and potassium and hypertension.\n\n\nShow the codenhanes &lt;- nhanes %&gt;%\n  mutate(\n    ordinal_high_bp = factor(dplyr::case_when(\n      BPXSAR &lt; 120 & BPXDAR &lt; 80 ~ \"Normal\",\n      (BPXSAR &lt; 140 & BPXSAR &gt;= 120) & (BPXDAR &gt;= 80 & BPXDAR &lt; 90) ~\n        \"PreHypertension\",\n      (BPXSAR &gt;= 140 & BPXSAR &lt; 160) & (BPXDAR &gt;= 90 & BPXDAR &lt; 100) ~\n        \"Hypertension Stage 1\",\n      BPXSAR &gt;= 160 & BPXDAR &gt;= 100 ~ \"Hypertension Stage 1\"\n    ), levels = c(\n      \"Normal\", \"PreHypertension\", \"Hypertension Stage 1\",\n      \"Hypertension Stage 2\"\n    ))\n  )\n\ndes &lt;- svydesign(\n  id = ~SDMVPSU, strat = ~SDMVSTRA, weights = ~fouryearwt,\n  nest = TRUE, data = subset(nhanes, !is.na(WTDRD1))\n)\n\nfit &lt;- svyolr(ordinal_high_bp ~ sodium + potassium, design = des)\nsummary(fit)\n\nCall:\nsvyolr(ordinal_high_bp ~ sodium + potassium, design = des)\n\nCoefficients:\n                Value Std. Error   t value\nsodium    -0.02732083 0.03175951 -0.860241\npotassium  0.14089855 0.05304160  2.656378\n\nIntercepts:\n                                          Value   Std. Error t value\nNormal|PreHypertension                     2.1054  0.1284    16.3969\nPreHypertension|Hypertension Stage 1       3.8628  0.2031    19.0148\nHypertension Stage 1|Hypertension Stage 2 12.0641  0.1381    87.3570\n(9218 observations deleted due to missingness)\n\n\nBecause many of the individuals have blood pressure values that aren’t covered by the categories Lumley listed, we lose ~9000 observations leading to a decrease in the precision of our estimates. Notably, the sodium coefficient is no now significant (|t-value| &lt; 2). Because the missignness here is self-caused I wouldn’t draw any strong conclusions from this model.\n\nUsing data for Florida (X_STATE = 12) from the 2007 BRFSS, fit a loglinear model to associations between the following risk factors and behaviors: perform vigorous physical activity (VIGPACT), eat five or more servings of fruit or vegetables per day (X_FV5SRV), binge drinking of alcohol (X_RFBING4), ever had an HIV test (HIVST5), ever had Hepatitis B vaccine (HEPBVAC), age group (X_AGE_G) and sex, X_SEXG. All except age group are binary. You will need to remove missing values, coded 7,8 or 9.\n\n\nShow the codebrfss_sub &lt;- subset(brfss, X_STATE == 12 & !any(VIGPACT %in% c(7, 8, 9)) &\n  !any(X_FV5SRV %in% 7:9) & !any(X_RFBING4 %in% 7:9) &\n  !any(HIVTST5 %in% 7:9) & !any(HEPBVAC %in% 7:9) &\n  !any(X_AGE_G %in% 7:9) & !any(X_SEXG_ %in% 7:9))\nloglin_fit &lt;- svyloglin(~ VIGPACT + X_FV5SRV + X_RFBING4 + HIVTST5 + HEPBVAC +\n  X_AGE_G + X_SEXG_, design = brfss_sub)\nsummary(loglin_fit)\n\n\nDespite all the subsetting I received the following error message when trying to fit the model as suggested. I’m guessing the dimensionality of the model matrix is quite large and the underlying functions don’t use a sparse representation…\nError: vector memory limit of 50.0 Gb reached, see mem.maxVSize()"
  },
  {
    "objectID": "ComplexSurveyNotes.html#introduction---motivation",
    "href": "ComplexSurveyNotes.html#introduction---motivation",
    "title": "Complex Survey Notes",
    "section": "Introduction - Motivation",
    "text": "Introduction - Motivation\nLumley motivates the need to explore the three titular topics by expanding on the principle developed in the second chapter — stratification. Similar as to how making use of the extra information available in strata we can improve estimates in straightforward estimation of totals and means, Lumley’s focus in this chapter is how to use the “auxiliary” information to adjust for non-response bias and improve the precision of the estimates."
  },
  {
    "objectID": "ComplexSurveyNotes.html#post-stratification",
    "href": "ComplexSurveyNotes.html#post-stratification",
    "title": "Complex Survey Notes",
    "section": "Post-Stratification",
    "text": "Post-Stratification\nPost-stratification is exactly what it sounds like - re-weighting estimates according to strata totals after or apart from any initial strata that might have been involved in the inital sampling design.\nConsider a relatively straightforward design in which there’s a population of subjects of size N that can be partitioned into K mutually exclusive strata from which any of the N_k individuals in that strata can be sampled for n_k strata samples. In this setting the sampling weights for each individual in group k is \\frac{N_k}{n_k} and N_k is known without any uncertainty.\nIf the sampling were not stratified but N_k were still known, the group sizes would not be exactly correct by simple Horvitz-Thompson estimation, but they could be corrected by re-weighting so that the sizes are correct as they would be in stratified sampling.\nSpecifically, take each weight w_i = \\frac{1}{\\pi_i} and construct new weights w_i^* = \\frac{g_i}{\\pi_i} = \\frac{N_k}{\\hat{N}_k} \\times \\frac{1}{\\pi_i}.\nFor estimating the group side of the kth group then, we’ll have\n\nn_k \\times \\frac{g_i}{\\pi_i} = n_k \\times \\frac{1}{\\pi_i} \\times\n\\frac{N_k}{\\hat{N}_k} = n_k \\times \\frac{\\hat{N}_k}{n_k} \\times\n\\frac{N_k}{\\hat{N}_k} = N_k,\n where \\pi_i = \\frac{n_k}{\\hat{N}_k}. The consequence of this re-weighting means that the estimated sub group population is exactly correct and subsequent estimates within or across these groups benefit from the extra information.\nOf course, as Lumley notes, there’s a problem if no entities were sampled in the particular strata of interest - you can’t re-weight the number 0. Still since this is unlikely to happen for groups and samples of “reasonable” size post-stratification is still a worthy strategy given the potential reductions in variance that are possible.\nIllustration\nLumley’s illustration of post-stratification looks at the two-stage sample drawn from the API population, with 40 school districts sampled from California and then up to 5 schools sampled from each district. Lumley uses this example to illustrate how improvements to precision can be made via post-stratification – or not.\nWe’ll start with a reminder of the sample design used here: a two-stage sample.\n\nShow the codeclus2_design\n\n2 - level Cluster Sampling design\nWith (40, 126) clusters.\nsvydesign(id = ~dnum + snum, fpc = ~fpc1 + fpc2, data = apiclus2)\n\n\nThen information about the population group sizes is included in the call to postStratify() as well as the variable/strata across which to post-stratify.\n\nShow the codepop.types &lt;- data.frame(stype = c(\"E\", \"H\", \"M\"), Freq = c(4421, 755, 1018))\nps_design &lt;- postStratify(clus2_design, strata = ~stype, population = pop.types)\nps_design\n\n2 - level Cluster Sampling design\nWith (40, 126) clusters.\npostStratify(clus2_design, strata = ~stype, population = pop.types)\n\n\nTotals, and so on are then estimated in the usual fashion. In this example there’s a large difference in the variability of the estimated total when comparing the naive and post-stratified estimates because much of the variability in the number of students enrolled can be explained by the school type - elementary schools are typically smaller than middle schools and highschools. By including more information about the types of schools in the overall population, the standard error is decreased by a factor of ~ 2.6.\n\nShow the codesvytotal(~enroll, clus2_design, na.rm = TRUE)\n\n         total     SE\nenroll 2639273 799638\n\n\n\nShow the codesvytotal(~enroll, ps_design, na.rm = TRUE)\n\n         total     SE\nenroll 3074076 292584\n\n\nIn contrast, school type is not associated with the variability in the school scores measured by the Academic Performance Index - denoted below by api00.\n\nShow the codesvymean(~api00, clus2_design)\n\n        mean     SE\napi00 670.81 30.099\n\n\n\nShow the codesvymean(~api00, ps_design)\n\n      mean     SE\napi00  673 28.832\n\n\nIndeed, the score is specifically setup to be standardized across school types and as such there’s little variance reduction observed by using the post-stratification information in this instance.\nLumley notes that if the api dataset were a real survey, non response might vary as a function of school type and in which case post-stratification could help reduce non-response bias."
  },
  {
    "objectID": "ComplexSurveyNotes.html#raking",
    "href": "ComplexSurveyNotes.html#raking",
    "title": "Complex Survey Notes",
    "section": "Raking",
    "text": "Raking\nIf one were to post-stratify using more than one variable would require the complete joint distribution of both variables. This can be problematic because the population totals for the joint distribution - or cross classification as Lumley calls it - is not available. Raking is a method that aims to overcome this problem.\n\nThe process involves post-stratifying on each set of variables in turn, and repeating this process until the weights stop changing.\n\nLumley further highlights the connection between log-linear regression models and raking:\n\nRaking could be considered a form of post-stratification where a log-linear model is used to smooth out the sample and population tables before the weights are adjusted.\n\n\nShow the codeload(\"Data/Family Resource Survey/frs.rda\")\nfrs.des &lt;- svydesign(ids = ~PSU, data = frs)\npop.ctband &lt;- data.frame(\n  CTBAND = 1:9,\n  Freq = c(\n    515672, 547548, 351599, 291425,\n    266257, 147851, 87767, 9190, 19670\n  )\n)\npop.tenure &lt;- data.frame(\n  TENURE = 1:4,\n  Freq = c(1459205, 493237, 128189, 156348)\n)\n\nfrs.raked &lt;- rake(frs.des,\n  sample = list(~CTBAND, ~TENURE),\n  population = list(pop.ctband, pop.tenure)\n)\noverall &lt;- svymean(~HHINC, frs.raked)\nwith_children &lt;- svymean(~HHINC, subset(frs.raked, DEPCHLDH &gt; 0))\nchildren_singleparent &lt;- svymean(~HHINC, subset(frs.raked, DEPCHLDH &gt; 0 & ADULTH == 1))\nc(\n  \"Overall\" = overall,\n  \"With Children\" = with_children,\n  \"Single Parent\" = children_singleparent\n)\n\n      Overall.HHINC With Children.HHINC Single Parent.HHINC \n           475.2484            605.1928            282.7683"
  },
  {
    "objectID": "ComplexSurveyNotes.html#generalized-raking-greg-estimation-and-calibration",
    "href": "ComplexSurveyNotes.html#generalized-raking-greg-estimation-and-calibration",
    "title": "Complex Survey Notes",
    "section": "Generalized Raking, Greg Estimation, and Calibration",
    "text": "Generalized Raking, Greg Estimation, and Calibration\nLumley identifies two ways to understand post-stratification:\n\nPost-stratification makes small changes to the sampling weights such that the estimated totals match the population totals.\nPost-stratification is a regression estimator, where each post-stratum is an indicator in a regression model.\n\nAn extension of the first view leads to calibration estimators while the second leads to generalized regression estimators. The former requires correctly specifying how to change the weights, the second requires correctly specifying the model. Both can lead to great increases in precision of the target estimates.\nLumley’s motivation of calibration starts with the regression estimate of a population total: if we have the auxiliary variable X_i available on all units in the population and an estimated \\hat{\\beta} from a sample, we can estimate \\hat{T} as the sum of the predicted values from the regression.\n\n\\hat{T}_{reg} = \\sum_{i=1}^N X_i\\hat{\\beta}\n\nFrom this starting point, Lumley describes that calibration follows a similar form, with the unknown parameter \\hat{\\beta} know a function of the calibration weights g_i and original sampling weights \\pi_i defined in such a way that the population total of X is equal to the estimated total of X. Lumley identifies this as the calibration constraints:\n\nT_x = \\sum_{i=1}^{n} \\frac{g_i}{\\pi_i} X_i.\n\nHowever, this constraint alone will not uniquely identify the g_i. Consequently, the specification of calibration weights are completed by requiring that they be “close” to the sampling weights - minimizing a distance function, while still satisfying the previous constraint.\nCalibration provides a unified view of post-stratification and raking. As Lumley states:\n\nIn linear regression calibration, the calibration weights g_i are a linear function of the auxiliary variables; in raking calibration the calibration weights are a multiplicative function of the auxiliary variables.\n\nVariance estimation proceeds in a similar fashion for calibration as it did for post-stratification, by constructing an unbiased estimator for the residual between Y_i and the estimated mean, or population total."
  },
  {
    "objectID": "ComplexSurveyNotes.html#calibration-in-r",
    "href": "ComplexSurveyNotes.html#calibration-in-r",
    "title": "Complex Survey Notes",
    "section": "Calibration in R",
    "text": "Calibration in R\nLinear regression calibration\nWe’re in a similar spot as before with regard to the election data example.\n\nShow the codepop.size &lt;- sum(pop.ctband$Freq)\npop.totals &lt;- c(\n  \"(intercept)\" = pop.size, pop.ctband$Freq[-1],\n  pop.tenure$Freq[-1]\n)\nfrs.cal &lt;- calibrate(frs.des,\n  formula = ~ factor(CTBAND) + factor(TENURE),\n  population = pop.totals,\n  calfun = \"raking\"\n)\n\nSample:  [1] \"(Intercept)\"     \"factor(CTBAND)2\" \"factor(CTBAND)3\" \"factor(CTBAND)4\"\n [5] \"factor(CTBAND)5\" \"factor(CTBAND)6\" \"factor(CTBAND)7\" \"factor(CTBAND)8\"\n [9] \"factor(CTBAND)9\" \"factor(TENURE)2\" \"factor(TENURE)3\" \"factor(TENURE)4\"\nPopltn:  [1] \"(intercept)\" \"\"            \"\"            \"\"            \"\"           \n [6] \"\"            \"\"            \"\"            \"\"            \"\"           \n[11] \"\"            \"\"           \n\nShow the codesvymean(~HHINC, frs.cal)\n\n        mean     SE\nHHINC 475.25 5.7306\n\nShow the codesvymean(~HHINC, subset(frs.cal, DEPCHLDH &gt; 0))\n\n        mean     SE\nHHINC 605.19 11.221\n\nShow the codesvymean(~HHINC, subset(frs.cal, DEPCHLDH &gt; 0 & ADULTH == 1))\n\n        mean     SE\nHHINC 282.77 9.3403\n\n\nComparing calibration methods\n\nShow the codeclus1 &lt;- svydesign(id = ~dnum, weights = ~pw, data = apiclus1, fpc = ~fpc)\nlogit_cal &lt;- calibrate(clus1, ~ stype + api99,\n  population = c(6194, 755, 1018, 3914069),\n  calfun = \"logit\", bounds = c(0.7, 1.7)\n)\nsvymean(~api00, clus1)\n\n        mean     SE\napi00 644.17 23.542\n\n\n\nShow the codesvymean(~api00, logit_cal)\n\n        mean   SE\napi00 665.46 3.42\n\n\n\nShow the codem0 &lt;- svyglm(api00 ~ ell + mobility + emer, clus1)\nsummary(svyglm(api00 ~ ell + mobility + emer, clus1))\n\n\nCall:\nsvyglm(formula = api00 ~ ell + mobility + emer, design = clus1)\n\nSurvey design:\nsvydesign(id = ~dnum, weights = ~pw, data = apiclus1, fpc = ~fpc)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 780.4595    30.0210  25.997 3.16e-11 ***\nell          -3.2979     0.4689  -7.033 2.17e-05 ***\nmobility     -1.4454     0.7343  -1.968  0.07474 .  \nemer         -1.8142     0.4234  -4.285  0.00129 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 6628.496)\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nShow the codem1 &lt;- svyglm(api00 ~ ell + mobility + emer, logit_cal)\nsummary(svyglm(api00 ~ ell + mobility + emer, logit_cal))\n\n\nCall:\nsvyglm(formula = api00 ~ ell + mobility + emer, design = logit_cal)\n\nSurvey design:\ncalibrate(clus1, ~stype + api99, population = c(6194, 755, 1018, \n    3914069), calfun = \"logit\", bounds = c(0.7, 1.7))\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 789.1015    17.7622  44.426 9.18e-14 ***\nell          -3.2425     0.4803  -6.751 3.15e-05 ***\nmobility     -1.5140     0.6436  -2.352 0.038318 *  \nemer         -1.7793     0.3824  -4.653 0.000702 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 7034.423)\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nShow the codepredict(m0, newdata = data.frame(ell = 5, mobility = 10, emer = 10))\n\n    link     SE\n1 731.37 26.402\n\n\n\nShow the codepredict(m1, newdata = data.frame(ell = 5, mobility = 10, emer = 10))\n\n    link    SE\n1 739.96 15.02\n\n\nCluster-level weights\nAlthough altering the weights can lead to higher precision of estimates sampling weights applied to clusters - which are identical - can then lead to unintuitive results – Lumley gives the example of mothers and infants sampled together but the number of infants not adding up to the number of mothers when using the calibrated weights. Consequently, Lumley includes an option in the survey package to force within-cluster weights to be identical when calibrated at a given stage — denoted below by the aggregate.stage argument.\nTo illustrate Lumley uses the clus2_design from earlier in the chapter, where the design was post-stratified on school type. I don’t really follow Lumley’s argument here but he says that the there is a loss of precision because of the added constraints, but that’s not what we see below or in the text - the constrained calibration has a lower standard error than the post-stratified design… Its hard to inspect things too carefully here because Lumley uses a cal variable that’s assigned a calibration object, presumably, but does not show how he assigns it in the text, so we’re left wondering.\n\nShow the codecal2 &lt;- calibrate(clus2_design, ~stype,\n  pop = c(4421 + 755 + 1018, 755, 1018),\n  aggregate.stage = 1\n)\nsvytotal(~enroll, cal2, na.rm = TRUE)\n\n         total     SE\nenroll 3084777 246321\n\n\n\nShow the codesvytotal(~enroll, ps_design, na.rm = TRUE)\n\n         total     SE\nenroll 3074076 292584\n\n\n\nShow the coderange(weights(cal) / weights(clus2_design))\n\n\n\nShow the coderange(weights(cal2) / weights(clus2_design))\n\n[1] 0.6418424 1.6764125\n\n\nBasu’s Elephants\nLumley walks through a classic story(Basu 2011) in statistics about how\npoor use of auxiliary information can lead to unreasonable behavior of design- based inference. Lumley then goes on to show how calibrated weights could make this more efficient. I’ll summarize briefly below.\nSuppose a circus owner has 50 elephants and wants to estimate their total weight. The catch? The owner only wants to use one of the elephants to construct the estimate.\nOne could construct an unbiased point estimate by taking a randomly sampled elephant’s weight and multiplying it by fifty. No estimate of the variance is available of course, because there’s no way to get an estimate of the variability in the elephants’ weights.\nAlternatively, a model based approach could depend on some auxiliary variable. Suppose the circus owner knew one particular elephant named Sambo was about average weight when all the elephants were weighed some years ago. The proposed method here then would be to sample Sambo with 100% probability and multiply his weight by 50. Lumley notes that this can’t be considered a valid design based estimate because all the other elephants had 0% probability of being sampled.\nBasu then imagines a compromise design that conforms to the necessary conditions required for design based inference, but that produces a nonsensical estimate. If Sambo was sampled with high probility, say 99%, then the point estimate would be \\frac{100}{99} \\times Sambo’s weight if Sambo is sampled and 5000 \\times the weight of any other elephant.\nObviously, this is not a good estimate, though Lumley notes that it will fulfill the Horvitz Thompson Property of being unbiased when averaging over repeated sampling. The problem of course being in the extreme variability of the estimate.\nLumley uses this scenario as an opportunity to discuss how to best use the auxiliary information (Sambo’s roughly average weight) in the context of setting up a design based estimator, arguing that the use of the information and the Horvitz Thompson Estimator are inappropriately used.\nUsing auxiliary information in design\nThe first point Lumley expands on is how to use auxiliary information. He argues that a stratified sample would be a better way to use information that Sambo is roughly average — splitting elephants according to whether they were small, middle or large looking. Ranked Set Sampling is also called out as a strategy that might be more useful in this setting.\nUsing Auxiliary Information in Analysis\nThe bigger point Lumley wants to make is that it would’ve been better to use the population size information to calibrate the weights, i.e. estimate the population total with weights \\frac{g_i}{\\pi_i} := 50, so that the estimate would be, as in the first approach, 50 times a sampled elephant’s weight.\nLumley also lays out two ratio and difference calibration based models that use the previous elephant’s weight as a way to more precisely estimate the change in elephants weight since last weighing as as a generalization to the total.\nAll-in-all, Lumley’s point is to highlight how auxiliary information can be more efficiently utilized and to demonstrate how the simple example fails to realize the full power of a design based approach.\nSelecting Auxiliary Variables for Non-Response\nAll the methods discussed thus far this chapter can be used to reduce the bias from missing data, specifically what Lumley calls unit non-response. The phenomenon when the sampled unit cannot be measured, for example a person sampled for a telephone interview does not pick up the phone.\nMissing data is an interesting research topic in its own right with standard fundamentals describing the different [mechanisms]((https://en.wikipedia.org/wiki/Missing_data#Types) or models of how missing data may emerge. Lumley’s discussion of how auxiliary variables for non-response can be selected is equivalent to the missing data mechanism known as “Missing at Random”, which implies that the fact that a measurement is missing is independent of the measurement itself, conditional on the other variables measured.\nIn the design-based view, this can mean one of several things, though Lumley focuses mainly on the strata in post-stratification. That is, the “other variables measured” refers to the strata information.\nLumley uses a telephone survey to illustrate this concept. If non-response was higher for land line than cellphone users but within each group the probability of responding was independent of the measure itself, then an analysis that combined these two groups together would be biased, but a post-stratified analysis would not be, since it estimated the strata specific rates.\nDirect Standardization\nLumley identifies “Direct Standardization” as an application of post-stratification to extrapolate an estimate in a different population than the sampled population, while adjusting for as many known confounders as possible. This is a bit like comparing regression predictions.\nStandard error estimation\nLumley makes a brief note here that the same approach for computing standard errors with complete data is used for incomplete data. Though, he makes a note that “secondary analysis” of large-scale surveys is not completely possible — I’m not clear what he means here, if this is dependent on some sampling stage specific information or perhaps he is referring to “domain” or sub-population estimation.\nIn any case, Lumley states that a conservative approach is used in this case when the calibration estimates are used — again, I’m guessing he’s referring to the survey R package’s implementation, though it isn’t explicit. However, if replicate weights are available, then a full estimate is available."
  },
  {
    "objectID": "ComplexSurveyNotes.html#exercises-6",
    "href": "ComplexSurveyNotes.html#exercises-6",
    "title": "Complex Survey Notes",
    "section": "Exercises",
    "text": "Exercises\n\nUsing the WA State Crime population, take a stratified random sample of five police districts from King County and five counties from the rest of the state.\n\nSetting up the sample design similar to before…\n\nShow the codeagency_sample &lt;- wa_crime_df %&gt;%\n  filter(County == \"king\") %&gt;%\n  distinct(Agency) %&gt;%\n  slice_sample(n = 10) %&gt;%\n  pull(Agency)\n\nnon_king_county_sample &lt;- wa_crime_df %&gt;%\n  filter(County != \"king\") %&gt;%\n  distinct(County) %&gt;%\n  slice_sample(n = 10) %&gt;%\n  pull(County)\n\ncounty_list &lt;- unique(wa_crime_df$County)\n\nstrata_design &lt;- wa_crime_df %&gt;%\n  filter(County %in% non_king_county_sample | Agency %in% agency_sample) %&gt;%\n  mutate(\n    strata_label = factor(if_else(County == \"king\", \"strata 1\", \"strata 2\")),\n    num_counties = if_else(County == \"king\", 1, length(county_list) - 1)\n  ) %&gt;%\n  as_survey_design(\n    id = c(County, Agency),\n    fpc = c(num_counties, num_agencies),\n    strata = strata_label\n  )\n\n\n\n\nCalibrate the sample using stratum and population as the auxiliary variables. Estimate the number of murders and number of burglaries in the state using the calibrated and un-calibrated sample.\n\n\n\nShow the codepop_size &lt;- wa_crime_df %&gt;%\n  summarize(p = sum(Population)) %&gt;%\n  pull(p)\nnon_king_county_pop_size &lt;- wa_crime_df %&gt;%\n  filter(County != \"king\") %&gt;%\n  summarize(p = sum(Population)) %&gt;%\n  pull(p)\n\npop.totals &lt;- c(\n  \"(Intercept)\" = pop_size,\n  \"strata_labelstrata 2\" = non_king_county_pop_size\n)\nwa_crime_cal &lt;- calibrate(strata_design,\n  formula = ~strata_label,\n  population = pop.totals\n)\nrbind(\n  c(\"calibration\", svytotal(~murder_and_crime, wa_crime_cal)),\n  c(\"Sample-Based\", svytotal(~murder_and_crime, strata_design))\n)\n\n                    murder_and_crime  \n[1,] \"calibration\"  \"3106294463.83561\"\n[2,] \"Sample-Based\" \"96464\"           \n\n\nThe calibration numbers look far too high here, which is puzzling given that the weights should be exactly calibrated to better reproduce the population total.\n\n\nConvert the original survey design object to use jackknife replicate weights. Calibrate the replicate-weight design using the same auxiliary variables and estimate the number of burglaries and of murders in the state.\n\n\n\nShow the coderep_design &lt;- survey::as.svrepdesign(strata_design)\n\nwa_crime_rep_cal &lt;- calibrate(rep_design,\n  formula = ~strata_label,\n  population = pop.totals\n)\nsvytotal(~murder_and_crime, wa_crime_rep_cal)\n\n                      total        SE\nmurder_and_crime 3106294464 164778523\n\n\nThis produces the same, erroneous, number as before.\n\n\nCalibrate the sample using the population and number of burglaries in the previous year as auxiliary variables, and estimate the number of burglaries and murders in the state.\n\n\n\nShow the codewa_crime_03_df &lt;- readxl::read_xlsx(\"data/WA_crime/1984-2011.xlsx\",\n  skip = 4\n) %&gt;%\n  filter(Year == \"2003\", Population &gt; 0) %&gt;%\n  mutate(\n    murder_and_crime = `Murder Total` + `Burglary Total`,\n    violent_crime = `Violent Crime Total`,\n    burglaries = `Burglary Total`,\n    property_crime = `Property Crime Total`,\n    state_pop = sum(Population),\n    County = stringr::str_to_lower(County),\n    num_counties = n_distinct(County),\n  ) %&gt;%\n  group_by(County) %&gt;%\n  mutate(num_agencies = n_distinct(Agency)) %&gt;%\n  ungroup() %&gt;%\n  select(\n    County, Agency, Population, murder_and_crime, burglaries, property_crime,\n    violent_crime, num_counties, num_agencies\n  )\n\ncols_to_keep &lt;- c(\n  \"County\", \"Agency\", \"Population\", \"burglaries\",\n  \"num_counties\", \"num_agencies\", \"murder_and_crime\", \"violent_crime\",\n  \"property_crime\"\n)\nwa_crime_df_mod &lt;- wa_crime_df %&gt;%\n  select(all_of(cols_to_keep)) %&gt;%\n  rename_if(is.numeric, function(x) str_c(x, \"_04\"))\n\nsample_design &lt;- wa_crime_03_df %&gt;%\n  select(all_of(cols_to_keep)) %&gt;%\n  rename_if(is.numeric, function(x) str_c(x, \"_03\")) %&gt;%\n  right_join(wa_crime_df_mod) %&gt;%\n  filter(\n    (County %in% non_king_county_sample |\n      Agency %in% agency_sample)\n  ) %&gt;%\n  mutate(\n    strata_label = factor(if_else(County == \"king\", \"strata 1\", \"strata 2\")),\n    num_counties = if_else(County == \"king\", 1, length(county_list) - 1)\n  ) %&gt;%\n  as_survey_design(\n    id = c(County, Agency),\n    fpc = c(num_counties_04, num_agencies_04),\n    strata = strata_label\n  )\n\npop_03 &lt;- wa_crime_03_df %&gt;%\n  summarize(P = sum(Population)) %&gt;%\n  pull(P)\n\nprevious_year_calibration &lt;- calibrate(sample_design,\n  formula = ~burglaries_03,\n  population = c(\n    \"(Intercept)\" = pop_size,\n    \"burglaries_03\" = pop_03\n  )\n)\nsvytotal(~burglaries_04, previous_year_calibration)\n\n\nHere I get an error message that one of the inner computations doesn’t have the correct dimensions. I suspect this is a bug in Lumley’s code as the sampling design I constructed above isn’t too complicated.\n\n\nEstimate the ratio of violent crimes to property crimes in the state, using the un-calibrated sample and the sample calibrated on population and number of burglaries.\n\n\n\nShow the codestrata_design %&gt;%\n  summarize(\n    ratio = survey_ratio(violent_crime, property_crime)\n  )\n\n# A tibble: 1 × 2\n   ratio ratio_se\n   &lt;dbl&gt;    &lt;dbl&gt;\n1 0.0684  0.00599\n\n\n\nShow the codesvyratio(\n  numerator = ~violent_crime_04,\n  denominator = ~property_crime_04,\n  design = previous_year_calibration\n)\n\n\nAgain, the sample based estimator makes sense here, while the calibrated estimator has issues. So far this is my best understanding of how to apply Lumley’s code but I’ll try to revisit this in the future.\n\nWrite an R function that accepts a set of 50 elephant weights and simulates repeatedly choosing a single elephant and computing the Horvitz-Thompson and ratio estimators of the total weight, reporting the mean and variance over the repeated simulations. Explore the behavior for several sets of elephant weights. Verify that the Horvitz-Thompson estimator is always unbiased, but usually further from the truth than the ratio estimator.\n\n\nShow the codeBasuHTE &lt;- function(elephant_weights) {\n  N &lt;- length(elephant_weights)\n  sambo &lt;- which.min(abs(elephant_weights - mean(elephant_weights)))\n  sample_probs &lt;- rep((1 - 0.99) / (N - 1), N)\n  sample_probs[sambo] &lt;- 0.99\n  # Okay to use the base R sample implementation here because we're only\n  # sampling 1 item.\n  x_ix &lt;- sample(1:N, size = 1, prob = sample_probs)\n  hte_weight &lt;- 1 / sample_probs[x_ix] * elephant_weights[x_ix]\n  ratio_estimate &lt;- N * elephant_weights[x_ix]\n  return(c(\"hte\" = hte_weight, \"ratio\" = ratio_estimate))\n}\nelephant_is_male &lt;- rbinom(50, size = 1, prob = 0.5)\nelephant_weights &lt;- elephant_is_male * rnorm(50, mean = 1.3E4, sd = 2E3) +\n  (1 - elephant_is_male) * rnorm(50, mean = 6.6E3, sd = 1E3)\nresults &lt;- as_tibble(t(replicate(1000, BasuHTE(elephant_weights)))) %&gt;%\n  mutate(rep_ix = 1:n()) %&gt;%\n  gather(hte, ratio, key = \"estimator\", value = \"estimate\") %&gt;%\n  group_by(estimator) %&gt;%\n  mutate(mean = mean(estimate), sd = sd(estimate)) %&gt;%\n  ungroup()\n\n\n\nShow the codetrue_total &lt;- sum(elephant_weights)\nresults %&gt;%\n  ggplot(aes(x = estimate, fill = estimator)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = mean, color = estimator)) +\n  geom_vline(aes(xintercept = true_total), linetype = 2, color = \"red\") +\n  facet_wrap(~estimator, nrow = 2) +\n  ggtitle(\"Illustrating Basu's Elephants\")\n\n\n\n\n\nShow the coderesults %&gt;%\n  filter(estimator == \"hte\") %&gt;%\n  summarize(average_estimate = mean(estimate)) %&gt;%\n  mutate(\n    bias = average_estimate - true_total,\n    truth = true_total\n  )\n\n# A tibble: 1 × 3\n  average_estimate   bias   truth\n             &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1          591775. 57162. 534613.\n\n\nFrom the plot above we can see that the horvitz thompson estimator is more variable than the ratio estimator though both provide the appropriate estimate on average.\n\nWrite an R function that accepts a set of 50 elephant weights and performs the ranked set sampling procedure on page 150 to choose three of them. By simulation, compare the bias and variance of the estimated total from ranked-set sample to estimated totals from a simple random sample of three elephants.\n\n\nShow the codeRankedSetSampleComparison &lt;- function(elephant_weights) {\n  srs_sample &lt;- sample(elephant_weights, size = 3)\n  sampling_weight &lt;- length(elephant_weights) / 3\n  srs_total &lt;- sum(srs_sample * length(elephant_weights) / 3)\n  rs_sample_one &lt;- sample(elephant_weights, size = 3)\n  rs_sample_two &lt;- sample(elephant_weights, size = 3)\n  rs_sample_three &lt;- sample(elephant_weights, size = 3)\n  rs_total &lt;- rs_sample_one[which.min(rank(rs_sample_one))] * sampling_weight +\n    rs_sample_two[which(rank(rs_sample_two) == 2)] * sampling_weight +\n    rs_sample_three[which.max(rank(rs_sample_three))] * sampling_weight\n  return(c(\"Ranked Set Estimate\" = rs_total, \"SRS Total\" = srs_total))\n}\nresults &lt;- as_tibble(t(replicate(\n  1000,\n  RankedSetSampleComparison(elephant_weights)\n))) %&gt;%\n  mutate(rep_ix = 1:n()) %&gt;%\n  gather(everything(), -rep_ix, key = \"estimator\", value = \"estimate\") %&gt;%\n  group_by(estimator) %&gt;%\n  mutate(mean = mean(estimate), sd = sd(estimate))\n\n\n\nShow the coderesults %&gt;%\n  ggplot(aes(x = estimate, fill = estimator)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = true_total), linetype = 2) +\n  ggtitle(\"Ranked Set and Simple Random Sample Estimator Comparison\")\n\n\n\n\nAs seen above the ranked set estimator has much lower variability, while maintaining the same mean estimate as compared to the simple random sample.\n\nEstimate the proportions of people in CA with normal weight, overweight and obesity using the BRFSS 2007 data (X_STATE = 6, X_BMI4CAT = BMI). Post-stratify the CA data to have the same age and sex distribution as the data for FL (X_STATE = 12) and compute the directly standardized estimates based on CA data to estimates form the data for FL to see if the differences in BMI between the states are explained by differences in age distribution.\n\n\nShow the codedb &lt;- DBI::dbConnect(RSQLite::SQLite(), \"Data/BRFSS/brfss07.db\")\nbrfss_ca &lt;- tbl(db, sql(\"SELECT * FROM brfss\")) %&gt;%\n  filter(X_STATE == 6) %&gt;%\n  collect() %&gt;%\n  mutate(\n    BMI_Cat = case_when(\n      X_BMI4CAT == 1 ~ \"Normal BMI\",\n      X_BMI4CAT == 2 ~ \"Overweight BMI\",\n      X_BMI4CAT == 3 ~ \"Obese BMI\",\n      TRUE ~ NA\n    )\n  ) %&gt;%\n  as_survey_design(\n    id = X_PSU, strata = X_STATE, weight = X_FINALWT,\n    nest = TRUE\n  )\n\nbrfss_ca %&gt;%\n  group_by(BMI_Cat) %&gt;%\n  summarize(\n    proportion = survey_mean()\n  )\n\n# A tibble: 4 × 3\n  BMI_Cat        proportion proportion_se\n  &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n1 Normal BMI         0.392        0.00889\n2 Obese BMI          0.223        0.00769\n3 Overweight BMI     0.341        0.00856\n4 &lt;NA&gt;               0.0446       0.00377\n\n\n\nShow the codebrfss_fl &lt;- tbl(db, sql(\"SELECT * FROM brfss WHERE X_STATE == 12\")) %&gt;%\n  collect() %&gt;%\n  mutate(\n    BMI_Cat = case_when(\n      X_BMI4CAT == 1 ~ \"Normal BMI\",\n      X_BMI4CAT == 2 ~ \"Overweight BMI\",\n      X_BMI4CAT == 3 ~ \"Obese BMI\",\n      TRUE ~ NA\n    )\n  ) %&gt;%\n  as_survey_design(\n    id = X_PSU, strata = X_STATE, weight = X_FINALWT, nest = TRUE\n  )\n\nfl_sex_age_totals &lt;- brfss_fl %&gt;%\n  group_by(SEX, X_AGE_G) %&gt;%\n  survey_count() %&gt;%\n  select(-n_se) %&gt;%\n  rename(Freq = n)\n\npostStratify(\n  design = brfss_ca, strata = ~ SEX + X_AGE_G,\n  population = fl_sex_age_totals\n) %&gt;%\n  svymean(~BMI_Cat, design = ., na.rm = TRUE)\n\n                         mean     SE\nBMI_CatNormal BMI     0.40323 0.0083\nBMI_CatObese BMI      0.23407 0.0075\nBMI_CatOverweight BMI 0.36269 0.0083\n\n\n\nShow the codeDBI::dbDisconnect(db)\nbrfss_fl %&gt;%\n  group_by(BMI_Cat) %&gt;%\n  summarize(\n    proportion = survey_mean()\n  )\n\n# A tibble: 4 × 3\n  BMI_Cat        proportion proportion_se\n  &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n1 Normal BMI         0.360        0.00585\n2 Obese BMI          0.229        0.00505\n3 Overweight BMI     0.361        0.00593\n4 &lt;NA&gt;               0.0508       0.00300\n\n\nIn general we see that the two estimates - the post stratified estimate and the “direct” estimates largely agree, for the proportion of Obese and Overweight BMI estimates. The normal BMI Estimates appear to be substantially different which may be due to factors that aren’t unaccounted for in the age and sex strata.\n\nConsider a categorical post-stratification variable with K categories having as population counts N_1, N_2,...N_K. Suppose we are interested in estimating the total of a variable Y.\n\n\n\nShow that the post-stratified estimate is \n\\hat{T}_{ps} = \\sum_{k=1}^{K} N_k \\hat{\\mu}_k,\n\n\n\n\nwhere \\hat{\\mu}_k is the estimated mean of Y in group K before post-stratification.\nWe’ll start with using the result derived in the text that shows when we post-stratify we scale the weights, \\pi_i^* = \\frac{g_i}{\\pi_i}, such that the estimated strata size is exactly equal to the known (post) strata size. This results in a g_i = \\frac{N_k}{\\hat{N_k}} scaling weight.\nFor our post-stratified total estimate then we have \n\\hat{T}_{ps} = \\sum_{i=1}^{n} y_iI(y_i \\in S_k) \\frac{N_k}{\\hat{N}_k} \\\\\n= \\sum_{k=1}^{K} \\hat{\\mu}_k N_k\n Where the mean estimate, \\hat{\\mu}_k comes from dividing the y_i in each k group by the \\hat{N}_k denominator.\n\n\nShow that the regression estimate from a model with indicator variables for each group is also\n\n\n\n\\hat{T}_{reg} = \\sum_{k=1}^{K} N_k \\hat{\\mu}_k\n\nThis follows in a straightforward fashion from a regression model set-up. If we fit the following model: \nE[Y_i] = \\beta_k I(y_i \\in S_k)\n Then it follows that \n\\hat{\\beta}_k = \\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i=1}^{n}y_iI(y_i \\in S_k), k = 1,...,K\n Predicting a new total then with the new counts re-weighted amounts to a regression prediction \\hat{T}_{ps} = \\sum_{k=1}^{K} N_k \\hat{\\beta}_k = \\sum_{k=1}^{K} N_k \\hat{\\beta}_k"
  },
  {
    "objectID": "ComplexSurveyNotes.html#multistage-and-multiphase-sampling",
    "href": "ComplexSurveyNotes.html#multistage-and-multiphase-sampling",
    "title": "Complex Survey Notes",
    "section": "MultiStage and Multiphase Sampling",
    "text": "MultiStage and Multiphase Sampling\nIn comparison to multistage sampling, where individuals or clusters were sampled independently of one another across stages, multiphase sampling is a design where individuals are sampled dependent upon information obtained in the first sample. Consequently, instead of having two sampling probabilities which are multiplied by each other \\pi_1 \\times \\pi_2 we have conditional weights \\pi_1 and \\pi_{2|1} which describe the probability of an entity being sampled in phase one and the the probability of a phase being sampled in stage two, conditional on being sampled in phase one. Multiplying and differencing these two probabilities together can be used to construct an estimator very similar to the horvitz-thompson estimator, though is theoretically distinct. Lumley notes here that his software and exposition only covers the simple cases of both types of sampling designs described here can be expanded to cover multiple phases and combinations of both types of designs."
  },
  {
    "objectID": "ComplexSurveyNotes.html#sampling-for-stratification",
    "href": "ComplexSurveyNotes.html#sampling-for-stratification",
    "title": "Complex Survey Notes",
    "section": "Sampling for Stratification",
    "text": "Sampling for Stratification\nOne common motivation for two-phase sampling is to measure some strata variable on a sample of the populations. The first phase sample is a random sample of the general population on which the strata variable is then measured. The second phase of sampling then stratifies on this variable for greater precision.\nLumley gives several examples of this setup including the NHANES and NHIS surveys. Though the data associated with these are not made publicly available, unfortunately."
  },
  {
    "objectID": "ComplexSurveyNotes.html#the-case-control-design",
    "href": "ComplexSurveyNotes.html#the-case-control-design",
    "title": "Complex Survey Notes",
    "section": "The Case-Control Design",
    "text": "The Case-Control Design\n\nA type of sampling for stratification is the case-control design (or choice-based design in economics).\n\nThis design is used when the measure of interest is particularly sparse or rare in the population. The basic setup is the same as described in the previous section where the rare disease is measured in the first phase sample from a hopefully well specified population. The second phase samples all individuals with the disease (cases) and some proportion of the controls. Typically matching k controls to each case.\nA few brief notes Lumley makes here:\n\nIf done properly, the design effect is very large for this design; this design is far more efficient than a simple random sample proportional to the sparsity of the disease.\nTwo competing interests in selecting controls that leads to criticism of this design — they have to be representative of the population, but also good “matches” for the cases. They need to be comparable in some way.\nDesign based inference was not as frequently used for case control designs. – Because the odds ratio estimate is independent of the sampling fraction and the estimated standard errors are typically less, a model based approach is often used. However, this requires the model to be specified correctly which is not always the case. – Lumley notes that there seems to be less hesitancy about using either design based approach or a model based approach as there once was.\n\nOesophageal cancer in Ille-et-Vilaine\nLumley re analyzes data published as part of a previous study (Breslow, Day, and Heseltine 1980) (AJ 1977) looking at esophageal cancer in northwest France as impacted by of alcohol and tobacco consumption.\nLumley gets a control sampling weight of 441 by digging around in the related papers and uses this to expand the original dataset, which contains the aggregate numbers to construct a survey design object below.\n\nShow the codecases &lt;- cbind(esoph[rep(1:88, esoph$ncases), ], case = 1, weight = 1)\ncontrols &lt;- cbind(esoph[rep(1:88, esoph$ncontrols), ], case = 0, weight = 441)\nesoph.x &lt;- rbind(cases, controls)\nd_esoph &lt;- svydesign(\n  id = ~1, strata = ~case, weights = ~weight,\n  data = esoph.x\n)\nunwtd &lt;- glm(case ~ agegp + as.numeric(tobgp) + as.numeric(alcgp),\n  data = esoph.x, family = binomial()\n)\nwtd &lt;- svyglm(case ~ agegp + as.numeric(tobgp) + as.numeric(alcgp),\n  design = d_esoph, family = quasibinomial\n)\ncoef(unwtd)[7:8]\n\nas.numeric(tobgp) as.numeric(alcgp) \n        0.4395543         1.0676597 \n\nShow the codecoef(wtd)[7:8]\n\nas.numeric(tobgp) as.numeric(alcgp) \n        0.4453378         1.0448879 \n\nShow the codeSE(unwtd)[7:8]\n\nas.numeric(tobgp) as.numeric(alcgp) \n       0.09623424        0.10492518 \n\nShow the codeSE(wtd)[7:8]\n\nas.numeric(tobgp) as.numeric(alcgp) \n        0.1221853         0.1236996 \n\n\n\nShow the codetbl_merge(\n  tbls = list(tbl_regression(unwtd), tbl_regression(wtd)),\n  tab_spanner = c(\"**Unweighted**\", \"**Weighted**\")\n)\n\n\n\n\n\n\n\n\nCharacteristic\n\n      \n        \nUnweighted\n\n      \n      \n        \nWeighted\n\n      \n    \n\n\n\nlog(OR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \n\nlog(OR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n    \n\n\n\nagegp\n\n\n\n\n\n\n\n\n    agegp.L\n3.8\n2.7, 5.6\n\n\n3.9\n2.5, 5.2\n\n\n\n    agegp.Q\n-1.5\n-3.1, -0.51\n0.014\n-1.4\n-2.6, -0.16\n0.026\n\n\n    agegp.C\n0.08\n-0.73, 1.2\n0.9\n0.29\n-0.63, 1.2\n0.5\n\n\n    agegp^4\n0.12\n-0.58, 0.74\n0.7\n0.25\n-0.40, 0.90\n0.4\n\n\n    agegp^5\n-0.25\n-0.67, 0.17\n0.2\n-0.28\n-0.73, 0.17\n0.2\n\n\nas.numeric(tobgp)\n0.44\n0.25, 0.63\n\n\n0.45\n0.21, 0.69\n\n\n\nas.numeric(alcgp)\n1.1\n0.87, 1.3\n\n\n1.0\n0.80, 1.3\n\n\n\n\n\n1 \nOR = Odds Ratio, CI = Confidence Interval\n\n\n    \n\n\n\n\nRegardless of which model is used, both show that tobacco and alcohol use increases the odds of esophageal cancer, though the un-weighted model does have slightly smaller confidence intervals for the coefficients of interest, as we can see in the table above.\nSimulations: efficiency of the design-based estimator\nIn this section Lumley computes the relative efficiency of the weighted estimator relative to the weighted comparing different distributions of cases and controls. Lumley defines the relative efficiency as the number of observations needed to produce the same accuracy but it isn’t clear how accuracy is measured here (length of standard error?). Since he doesn’t show how he computes the resulting table, I’ve omitted his example from my notes.\nFrequency matching\n\nMany case-control designs use a further level of stratification and unequal sampling in the phase-two sample, a practice known in the epidemiology literature as frequency matching.\n\nThe idea is to avoid wasting cases / controls in regions of low exposure. Lumley uses the example of the esophageal cancer study to illustrate. Bracket text below is mine:\n\n… very little information about the effects of alcohol and tobacco is present in the youngest age group, because there is only one case. If the associations with age had already been understood and the study had been designed to estimate the effect of alcohol and tobacco [only] this would be an inefficient design that effectively wasted 116 controls. A more efficient design would saple more controls at older ages and end up with five controls per case in each age group rather than five controls per case on average over all ages.\n\nLumley goes on to say that frequency matching isn’t the most efficient way to use the age information, but he doesn’t say what would be. I’m guessing he’s thinking of further stratification or unequal sampling but I can’t be sure."
  },
  {
    "objectID": "ComplexSurveyNotes.html#sampling-from-existing-cohorts",
    "href": "ComplexSurveyNotes.html#sampling-from-existing-cohorts",
    "title": "Complex Survey Notes",
    "section": "Sampling from Existing Cohorts",
    "text": "Sampling from Existing Cohorts\nLumley notes in this section that it is often advantageous in larger randomized control trials or cohort studies to sub-sample once certain measurements have been taken. Lumley gives an account of methods used on these data — previously nested case control designs, ignoring the phase one sample and cohort representativeness but more recently stratifying on both outcome and covariates in phase two and post-stratifying to the original cohort. The trade-offs between model and design based approaches are/were not clear at the date of publication in this setting. My take is that the design based approach seems better given how many design elements are involved in the construction of the study.\nLogistic regression\n\n\n\nExposed\nUnexposed\n\n\n\nCase\na\nb\n\n\nControl\nc\nd\n\n\nTotal\na+c\nb + d\n\n\n\nLumley’s goal in this section is to show how case control designs can be optimised to reduce the variance associated with the odds ratio estimate, typically estimated via logistic regression. Starting from a 2 x 2 contigency table like that shown above the variance of the log of the odds ratio can be estimated as follows:\n\nV[\\log \\psi] = \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d},\n\nwhere \\psi represents the odds of the disease in the exposed cases vs the odds of disease in the exposed controls. From the expression above, we see that if we increase any of the cell counts, we decrease the variance. However, a classic case control design will only control the table row margins, meaning there might be greater improvement in controlling the sampling from the exposure covariate(s) as well.\nHowever, in that setting the classic logistic regression model no longer gives a valid analysis because the odds ratio will be impacted by the unequal sampling fractions amongst the exposure covariate. In that case, a design based approach is necessary.\nLumley uses the National Wilms Tumor Study group to illustrate this idea. Kidney histology was available for all members of the study. Although the initial histology had an appreciable error rate, it could still be used to sample from. Though Lumley does not give an exact procedure for how he does this he shows a table of of biased sampling that includes 183 controls with unfavorable histology ratings, a higher number than what would be included with random sampling matched case-control procedure.\nTwo-phase case-control designs in R\nLumley illustrates how to fit a two-phase sample estimate using the national wilms tumor study data. The code and model output are below.\n\nShow the codenwts &lt;- addhazard::nwtsco\nset.seed(1337)\nsubsample &lt;- with(nwts, c(\n  which(relaps == 1 | instit == 1),\n  sample(which(relaps == 0 & instit == 0), 499)\n))\nnwts$in.subsample &lt;- (1:nrow(nwts)) %in% subsample\nnwts_design &lt;- twophase(\n  id = list(~1, ~1), subset = ~in.subsample,\n  strata = list(NULL, ~ interaction(instit, relaps)),\n  data = nwts\n)\nnwts_design\n\nTwo-phase sparse-matrix design:\n twophase2(id = id, strata = strata, probs = probs, fpc = fpc, \n    subset = subset, data = data, pps = pps)\nPhase 1:\nIndependent Sampling design (with replacement)\nsvydesign(ids = ~1)\nPhase 2:\nStratified Independent Sampling design\nsvydesign(ids = ~1, strata = ~interaction(instit, relaps), fpc = `*phase1*`)\n\n\n\nShow the codeset.seed(1337)\ncasectrl &lt;- with(nwts, c(which(relaps == 1), sample(which(relaps == 0), 699)))\nnwts$in.ccs &lt;- (1:nrow(nwts)) %in% casectrl\nccs_design &lt;- twophase(\n  id = list(~1, ~1), subset = ~in.ccs,\n  strata = list(NULL, ~relaps), data = nwts\n)\nm1 &lt;- svyglm(relaps ~ histol * stage + age + tumdiam,\n  design = nwts_design,\n  family = quasibinomial()\n)\nm2 &lt;- svyglm(relaps ~ histol * stage + age + tumdiam,\n  design = ccs_design,\n  family = quasibinomial()\n)\nm3 &lt;- glm(relaps ~ histol * stage + age + tumdiam,\n  data = nwts, subset = in.ccs,\n  family = binomial()\n)\nm1a &lt;- svyglm(relaps ~ histol * stage + age + tumdiam,\n  design = nwts_design,\n  family = quasipoisson(log)\n)\nm2a &lt;- svyglm(relaps ~ histol * stage + age + tumdiam,\n  design = ccs_design,\n  family = quasipoisson(log)\n)\n\n\n\nShow the codetbl_merge(\n  tbls = list(\n    tbl_regression(m1, conf.int = FALSE),\n    tbl_regression(m2, conf.int = FALSE),\n    tbl_regression(m3, conf.int = FALSE),\n    tbl_regression(m1a, conf.int = FALSE),\n    tbl_regression(m2a, conf.int = FALSE)\n  ),\n  tab_spanner = c(\n    \"Two Phase Design based Odds Ratio\",\n    \"CC Design Based Odds Ratio\",\n    \"Model Based Odds Ratio\",\n    \"Two Phase Design Based Relative Risk\",\n    \"CC Design Based Relative Risk\"\n  )\n)\n\n\n\n\n\n\n\n\nCharacteristic\n\n      \n        \nTwo Phase Design based Odds Ratio\n\n      \n      \n        \nCC Design Based Odds Ratio\n\n      \n      \n        \nModel Based Odds Ratio\n\n      \n      \n        \nTwo Phase Design Based Relative Risk\n\n      \n      \n        \nCC Design Based Relative Risk\n\n      \n    \n\n\n\nlog(OR)\n\n1\n\n      \np-value\n\n      \n\nlog(OR)\n\n1\n\n      \np-value\n\n      \n\nlog(OR)\n\n1\n\n      \np-value\n\n      \n\nlog(IRR)\n\n1\n\n      \np-value\n\n      \n\nlog(IRR)\n\n1\n\n      \np-value\n\n    \n\n\n\nhistol\n0.36\n0.3\n-0.10\n0.8\n0.09\n0.8\n0.65\n0.009\n0.37\n0.2\n\n\nstage\n0.26\n\n\n0.18\n0.004\n0.18\n0.003\n0.23\n\n\n0.16\n0.002\n\n\nage\n0.05\n0.032\n0.10\n\n\n0.10\n\n\n0.04\n0.028\n0.07\n\n\n\ntumdiam\n0.01\n0.6\n0.00\n&gt;0.9\n0.00\n0.8\n0.01\n0.6\n0.00\n&gt;0.9\n\n\nhistol * stage\n0.50\n0.001\n0.68\n\n\n0.62\n\n\n0.17\n0.048\n0.28\n0.004\n\n\n\n\n1 \nOR = Odds Ratio, IRR = Incidence Rate Ratio\n\n\n    \n\n\n\n\nFew quick things to note:\n\nThe estimates between the relative risk rate ratio and odds ratio vary substantially, as we’d expect.\nThere is an appreciable precision gain in the histol x stage estimates comparing the design based to the model based, with a slight gain from using the instit for sampling in the more general two-phased sampling model.\nLumley notes that model based analysis are also available for two-phased samples. In my training we learned how to use a conditional logistic regression in this setting. You can see a similar example here.\nSurvival Analysis\nIn this section Lumley briefly introduces the concept of the case-cohort study, as well as the topic of survival analysis. Notes on each below.\nCase-Cohort Study\n\nA two phase sample in which the the second phase sample is a sub-cohort chosen at the beginning in addition to all cases identified during follow-up.\nLumley notes that this initial sub-cohort can be used for comparing against multiple types of measured “cases” or events, in contrast to a case-control design which is constrained to analysis of the single type of case.\n\nSurvival Analysis\n\nSurvival analysis is a large enough topic to be given its own book but here Lumley focuses on the Cox proportional hazards model, which estimates the hazard function, or instantaneous rate of an event (like cancer relapse) occuring as a function of some covariates.\nNotably, survival analysis incorporates assumptions for censoring, or the unobservance of the event occuring within the follow-up period.\n\nCase Cohort Designs\nLumley uses the Wilms Tumor study again, to illustrate how a case-cohort analysis can be used to estimate patients survival from Wilms tumor as a function of the measured covariates. Here’s the setup in Lumley’s own words:\n\nFor the classical case-cohort analysis we take a sample from the cohort at the start of the followup and then add all the cases to it. If the expected event rate is about 1 in 7, giving about 650 expected cases, and we want 650 non-cases, this means sampling a subcohort of 650 x 7/6 or about 750… Under this sampling design there are two sampled strata: those in the subcohort and those not in the subcohort. The sampling probabilities are \\pi_{(2|1)} = 750 / 3915 for the subcohort and \\pi_{(2|1)} = 1 for cases not in the subcohort.\n\nThe code is below.\n\nShow the codeset.seed(1729)\nsubcohort &lt;- with(nwts, sample(1:nrow(nwts), 750))\ncases &lt;- which(nwts$relaps == 1)\nnwts$in.cchsample &lt;- (1:nrow(nwts)) %in% c(subcohort, cases)\nnwts$in.subcohort &lt;- (1:nrow(nwts)) %in% subcohort\nnwts$wts &lt;- ifelse(nwts$in.subcohort, 3915 / 750, 1)\ncch_design &lt;- twophase(\n  id = list(~1, ~1), subset = ~in.cchsample,\n  strata = list(NULL, ~in.subcohort),\n  weights = list(NULL, ~wts),\n  method = \"approx\",\n  data = nwts\n)\n\ns1 &lt;- svycoxph(Surv(trel, relaps) ~ histol * stage + age + tumdiam,\n  design = cch_design\n)\n\ncch_data &lt;- subset(nwts, in.cchsample)\ncch_data$id &lt;- 1:nrow(cch_data)\n# \"classic\" analysis based only on phase 2 data\ns2 &lt;- cch(Surv(trel, relaps) ~ histol * stage + age + tumdiam,\n  id = ~id,\n  data = cch_data, subcoh = ~in.subcohort, cohort.size = 3915\n)\ntbl_merge(\n  tbls = list(tbl_regression(s1), tbl_regression(s2)),\n  tab_spanner = c(\n    \"**Case-Cohort Analysis**\",\n    \"**Classic Phase 2 Analysis**\"\n  )\n)\n\nTwo-phase design: twophase(id = list(~1, ~1), subset = ~in.cchsample, strata = list(NULL, \n    ~in.subcohort), weights = list(NULL, ~wts), method = \"approx\", \n    data = nwts)\nPhase 1:\nIndependent Sampling design (with replacement)\nsvydesign(ids = ~1)\nPhase 2:\nStratified Independent Sampling design\nsvydesign(ids = ~1, strata = ~in.subcohort, weights = ~wts, fpc = `*phase1*`)\n\n\n\n\n\n\n\n\n\nCharacteristic\n\n      \n        \nCase-Cohort Analysis\n\n      \n      \n        \nClassic Phase 2 Analysis\n\n      \n    \n\n\n\nlog(HR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \n\nlog(HR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n    \n\n\n\nhistol\n0.75\n0.06, 1.4\n0.033\n0.96\n0.21, 1.7\n0.012\n\n\nstage\n0.14\n0.01, 0.26\n0.029\n0.21\n0.10, 0.33\n\n\n\nage\n0.06\n0.01, 0.11\n0.015\n0.06\n0.01, 0.11\n0.013\n\n\ntumdiam\n0.01\n-0.02, 0.04\n0.6\n0.01\n-0.03, 0.04\n0.7\n\n\nhistol * stage\n0.24\n-0.02, 0.49\n0.067\n0.12\n-0.16, 0.40\n0.4\n\n\n\n\n1 \nHR = Hazard Ratio, CI = Confidence Interval\n\n\n    \n\n\n\n\nLooking at the table above, we can see that there’s a decent amount of disagreement between the two estimates. Lumley argues this is because of the poor fit of the cox model. An assumption that’s necessary for the Cox model to fit well is that the hazards functions are proportional across time. As we see in the plot below and in the text, the hazard estimate for age is lower earlier in the trial than later on.\n\nShow the codeplot(cox.zph(s1), var = \"age\")\n\n\n\n\nLumley notes that the case-cohort analysis could just as easily be framed as a stratified sample on cases in the second phase. The code below performs this analysis — again using both case and case + institution for the stratifying variables. The last bit of code extracts the variance estimates that’s from each phase of sampling and computes how much of the variance comes from the first phase as opposed to the second phase for each of the estimated coefficients.\n\nShow the codescch_design &lt;- twophase(\n  id = list(~1, ~1), subset = ~in.cchsample,\n  strata = list(NULL, ~relaps), data = nwts\n)\n\ns1 &lt;- svycoxph(Surv(trel, relaps) ~ histol * stage + age + tumdiam,\n  design = scch_design\n)\nnwts_design &lt;- twophase(\n  id = list(~1, ~1), subset = ~in.subsample,\n  strata = list(NULL, ~ interaction(instit, relaps)),\n  data = nwts\n)\ns2 &lt;- svycoxph(Surv(trel, relaps) ~ histol * stage + age + tumdiam,\n  design = nwts_design\n)\n\n\nv1 &lt;- vcov(s1)\nv2 &lt;- vcov(s2)\n\nrbind(\n  diag(attr(v1, \"phases\")$phase2 / attr(v1, \"phases\")$phase1),\n  diag(attr(v2, \"phases\")$phase2 / attr(v2, \"phases\")$phase1)\n)\n\n          [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] 1.4892354 0.7446175 1.3303922 1.1322496 1.7652980\n[2,] 0.6304098 0.7120240 0.7681475 0.5199323 0.7651971\n\n\n\nShow the codes3 &lt;- coxph(Surv(trel, relaps) ~ histol * stage + age + tumdiam,\n  data = nwts\n)\ntbl_merge(\n  tbls = list(\n    tbl_regression(s1),\n    tbl_regression(s2),\n    tbl_regression(s3)\n  ),\n  c(\n    \"**Stratified Case Cohort**\",\n    \"**Stratified Case Cohort - Institution**\",\n    \"**Full Data**\"\n  )\n)\n\nTwo-phase sparse-matrix design:\n twophase2(id = id, strata = strata, probs = probs, fpc = fpc, \n    subset = subset, data = data, pps = pps)\nPhase 1:\nIndependent Sampling design (with replacement)\nsvydesign(ids = ~1)\nPhase 2:\nStratified Independent Sampling design\nsvydesign(ids = ~1, strata = ~relaps, fpc = `*phase1*`)\nTwo-phase sparse-matrix design:\n twophase2(id = id, strata = strata, probs = probs, fpc = fpc, \n    subset = subset, data = data, pps = pps)\nPhase 1:\nIndependent Sampling design (with replacement)\nsvydesign(ids = ~1)\nPhase 2:\nStratified Independent Sampling design\nsvydesign(ids = ~1, strata = ~interaction(instit, relaps), fpc = `*phase1*`)\n\n\n\n\n\n\n\n\n\nCharacteristic\n\n      \n        \nStratified Case Cohort\n\n      \n      \n        \nStratified Case Cohort - Institution\n\n      \n      \n        \nFull Data\n\n      \n    \n\n\n\nlog(HR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \n\nlog(HR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \n\nlog(HR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n    \n\n\n\nhistol\n0.89\n0.16, 1.6\n0.017\n0.48\n-0.11, 1.1\n0.11\n0.59\n0.13, 1.1\n0.013\n\n\nstage\n0.20\n0.08, 0.31\n\n\n0.26\n0.14, 0.38\n\n\n0.22\n0.13, 0.31\n\n\n\nage\n0.05\n0.00, 0.10\n0.039\n0.03\n-0.01, 0.07\n0.2\n0.06\n0.03, 0.08\n\n\n\ntumdiam\n0.01\n-0.02, 0.04\n0.5\n0.01\n-0.02, 0.03\n0.6\n0.02\n0.00, 0.04\n0.081\n\n\nhistol * stage\n0.19\n-0.09, 0.47\n0.2\n0.35\n0.12, 0.57\n0.003\n0.31\n0.14, 0.47\n\n\n\n\n\n1 \nHR = Hazard Ratio, CI = Confidence Interval\n\n\n    \n\n\n\n\nThe above table contains the data from both of the stratified case-cohort analyses as well as a model fit to the full data. The case-institution stratified analysis is more efficient than only the case stratified analysis. Both design based analyses agree with the full data analysis, though, of course, the full data analysis has less variability. Interestingly though, it is not that much better than the stratified case-cohort-institution estimates."
  },
  {
    "objectID": "ComplexSurveyNotes.html#using-auxiliary-information-from-phase-1",
    "href": "ComplexSurveyNotes.html#using-auxiliary-information-from-phase-1",
    "title": "Complex Survey Notes",
    "section": "Using Auxiliary Information from Phase 1",
    "text": "Using Auxiliary Information from Phase 1\nIn this section Lumley reviews how information recorded on individuals during the first phase can be used to improve the precision of estimates in the second phase via calibration. Lumley differentiates the use of auxiliary information here as compared to chapter 7’s review of post-stratification, raking and calibration by noting the following 3 differences:\n\nThe Phase one data includes individual level auxiliary variables.\nRe-weighting can be customized to a particular analysis.\nThere may be — when sampling from a cohort — a large number of auxiliary variables available.\n\nWith this set-up Lumley discusses how to construct more effective auxiliary variables using influence functions.\nPopulation calibration for regression models\nLumley motivates his discussion of influence functions with the api data, where we can reference the full population data.\nLumley’s description of influence functions are intentionally simple, describing them as\n\n“The influence function for an estimate \\hat{\\beta} describes how the estimate changes when observations are added to or removed from the data.”\n\nFor linear, glms, and cox models, these are \\Delta\\beta deletion diagnostics that are often taught as part of the statistics curriculum covering the subjects. For example, a simple linear regression has the following influence function for the slope parameter: \n\\mathcal{I}(x_i, y_i; \\beta) = \\frac{1}{\\pi_i}\\frac{1}{V[X]}(x_i - \\bar{x})(y_i - \\mu_i(\\beta)),\n\nwhere \\mu_i(\\beta) is the fitted value for the ith observation. Lumley reasons that\n\nIf an auxiliary variable Z is highly correlated with Y it will have a low correlation with \\mathcal{I}, because the multiplier $(x_i - {x}) can be negative or positive with about equal probability.\n\nI don’t quite see how Lumley is drawing the connection that Cor(Y,Z) \\approx 1 \\implies Cor(Z,\\mathcal{I}) \\approx 0 unless there is some way that Z’s impact on Y is through X — epidemiologists would then say “The effect of Z on Y is mediated by X. I don’t that’s what Lumley’s referring to here…\nThe next part makes sense - Lumley says that in order to construct new variables for use in calibration, we can use the influence functions from linear regressions constructed from Z \\tilde X. Though I should say, it isn’t clear to me why we might choose Z as the regression variable here…\nIn any case, Lumley demonstrates this with the api data, using the full population data to estimate the influence functions.\n\nShow the codem0 &lt;- svyglm(api00 ~ ell + mobility + emer, clus1_design)\nvar_cal &lt;- calibrate(clus1_design,\n  formula = ~ api99 + ell + mobility + emer,\n  pop = c(6194, 3914069, 141685, 106054, 70366),\n  bounds = c(0.1, 10)\n)\nm1 &lt;- svyglm(api00 ~ ell + mobility + emer, design = var_cal)\n\n\npopmodel &lt;- glm(api99 ~ ell + mobility + emer,\n  data = apipop,\n  na.action = na.exclude\n)\n\ninffun &lt;- dfbeta(popmodel)\nindex &lt;- match(apiclus1$snum, apipop$snum)\nclus1if &lt;- update(clus1_design,\n  ifint = inffun[index, 1],\n  ifell = inffun[index, 2], ifmobility = inffun[index, 3],\n  ifemer = inffun[index, 4]\n)\nif_cal &lt;- calibrate(clus1if,\n  formula = ~ ifint + ifell + ifmobility + ifemer,\n  pop = c(6194, 0, 0, 0, 0)\n)\n\nm2 &lt;- svyglm(api00 ~ ell + mobility + emer, design = if_cal)\ntbl_merge(\n  tbls = list(\n    tbl_regression(m0, intercept = TRUE),\n    tbl_regression(m1, intercept = TRUE),\n    tbl_regression(m2, intercept = TRUE)\n  ),\n  tab_spanner = c(\"Cluster Design\", \"Calibrated\", \"Influence Calibrated\")\n)\n\n\n\n\n\n\n\n\nCharacteristic\n\n      \n        \nCluster Design\n\n      \n      \n        \nCalibrated\n\n      \n      \n        \nInfluence Calibrated\n\n      \n    \n\n\nBeta\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \nBeta\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \nBeta\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n    \n\n\n\n(Intercept)\n780\n714, 847\n\n\n785\n755, 816\n\n\n791\n778, 803\n\n\n\nell\n-3.3\n-4.3, -2.3\n\n\n-3.3\n-4.6, -1.9\n\n\n-3.3\n-3.5, -3.0\n\n\n\nmobility\n-1.4\n-3.1, 0.17\n0.075\n-1.5\n-2.9, 0.00\n0.050\n-1.4\n-1.9, -0.91\n\n\n\nemer\n-1.8\n-2.7, -0.88\n0.001\n-1.7\n-2.5, -0.85\n\n\n-2.2\n-2.7, -1.8\n\n\n\n\n\n1 \nCI = Confidence Interval\n\n\n    \n\n\n\n\nWe can see the precision in the estimates gets better as we move from the sampled model to the influence calibrated model.\nThese can obviously get much more complex for more complex models. For more theory Lumley refers the reader to (Breslow, Day, and Heseltine 1980).\nTwo-phase designs\nThe best case for using influence functions as above in a two-phase design is when a phase-one variable is correlated with the variable of interest measured in phase two. Lumley gives an example for when this might occur: Self-reported smoking at phase one then followed up by a urinary test in phase two. In this setting Lumley proposes the following approach for constructing auxiliary variables based on influence functions.\n\nBuild an imputation model to predict the phase-two variable from the phase-one variables.\nFit a model to all of phase one, using the imputed value for observations that are not in the phase-two sample.\nUse the influence functions from this model as auxiliary variables in calibration.\n\nIn the API example, the imputation model step was replaced by using the previous year’s API test values.\nExample: Wilms’ tumor\nLumley walks through (Breslow et al. 2009)’s analysis of the NWTS data illustrating this approach. Lumley walks through the analysis in the text which has a clear mapping to the three steps above so I won’t repeat him. The code to fit the imputation model and calibrations are shown below.\n\nShow the codeimpmodel &lt;- glm(histol ~ instit + I(age &gt; 10) + I(stage == 4) * study,\n  data = nwts, subset = in.subsample, family = binomial()\n)\n\nnwts$imphist &lt;- predict(impmodel, newdata = nwts, type = \"response\")\n\nnwts$imphist[nwts$in.subsample] &lt;- nwts$histol[nwts$in.subsample]\n\nifmodel &lt;- coxph(Surv(trel, relaps) ~ imphist * age + I(stage &gt; 2) * tumdiam,\n  data = nwts\n)\n\ninffun &lt;- resid(ifmodel, \"dfbeta\")\n\ncolnames(inffun) &lt;- paste(\"if\", 1:6, sep = \"\")\nnwts_if &lt;- cbind(nwts, inffun)\nif_design &lt;- twophase(\n  id = list(~1, ~1), subset = ~in.subsample,\n  strata = list(NULL, ~ interaction(instit, relaps)),\n  data = nwts_if\n)\n\nif_cal &lt;- calibrate(if_design,\n  phase = 2, calfun = \"raking\",\n  formula = ~ if1 + if2 + if3 + if4 + if5 + if6 +\n    relaps * instit\n)\n\nm1 &lt;- svycoxph(Surv(trel, relaps) ~ histol * age + I(stage &gt; 2) * tumdiam,\n  design = nwts_design\n)\n\nm2 &lt;- svycoxph(Surv(trel, relaps) ~ histol * age + I(stage &gt; 2) * tumdiam,\n  design = if_cal\n)\nm3 &lt;- coxph(Surv(trel, relaps) ~ imphist * age + I(stage &gt; 2) * tumdiam,\n  data = nwts\n)\nm4 &lt;- coxph(Surv(trel, relaps) ~ histol * age + I(stage &gt; 2) * tumdiam,\n  data = nwts\n)\n\ntbl_merge(\n  tbls = list(\n    tbl_regression(m1),\n    tbl_regression(m2),\n    tbl_regression(m3),\n    tbl_regression(m4)\n  ),\n  tab_spanner = c(\n    \"2-Phase Weighted\",\n    \"2-Phase Raked\",\n    \"2 Phase Imputed\",\n    \"Full Data\"\n  )\n)\n\nTwo-phase sparse-matrix design:\n twophase2(id = id, strata = strata, probs = probs, fpc = fpc, \n    subset = subset, data = data, pps = pps)\nPhase 1:\nIndependent Sampling design (with replacement)\nsvydesign(ids = ~1)\nPhase 2:\nStratified Independent Sampling design\nsvydesign(ids = ~1, strata = ~interaction(instit, relaps), fpc = `*phase1*`)\nTwo-phase sparse-matrix design:\n calibrate(if_design, phase = 2, calfun = \"raking\", formula = ~if1 + \n    if2 + if3 + if4 + if5 + if6 + relaps * instit)\nPhase 1:\nIndependent Sampling design (with replacement)\nsvydesign(ids = ~1)\nPhase 2:\nStratified Independent Sampling design\ncalibrate(phase2, formula, population, calfun = calfun, ...)\n\n\n\n\n\n\n\n\n\nCharacteristic\n\n      \n        \n2-Phase Weighted\n\n      \n      \n        \n2-Phase Raked\n\n      \n      \n        \n2 Phase Imputed\n\n      \n      \n        \nFull Data\n\n      \n    \n\n\n\nlog(HR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \n\nlog(HR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \n\nlog(HR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \n\nlog(HR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n    \n\n\n\nhistol\n1.8\n1.4, 2.2\n\n\n2.1\n1.8, 2.4\n\n\n\n\n\n1.9\n1.6, 2.2\n\n\n\nage\n0.06\n0.02, 0.10\n0.006\n0.10\n0.07, 0.13\n\n\n0.10\n0.07, 0.13\n\n\n0.10\n0.06, 0.13\n\n\n\nI(stage &gt; 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    FALSE\n—\n—\n\n—\n—\n\n—\n—\n\n—\n—\n\n\n\n    TRUE\n1.4\n0.70, 2.0\n\n\n1.4\n0.91, 2.0\n\n\n1.4\n0.94, 1.9\n\n\n1.4\n0.90, 1.9\n\n\n\ntumdiam\n0.04\n0.00, 0.08\n0.038\n0.06\n0.03, 0.09\n\n\n0.06\n0.03, 0.09\n\n\n0.06\n0.03, 0.09\n\n\n\nhistol * age\n-0.12\n-0.22, -0.01\n0.027\n-0.16\n-0.23, -0.08\n\n\n\n\n\n-0.14\n-0.21, -0.08\n\n\n\nI(stage &gt; 2) * tumdiam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    TRUE * tumdiam\n-0.07\n-0.12, -0.01\n0.014\n-0.08\n-0.13, -0.04\n\n\n-0.08\n-0.12, -0.04\n\n\n-0.08\n-0.12, -0.04\n\n\n\nimphist\n\n\n\n\n\n\n2.1\n1.8, 2.4\n\n\n\n\n\n\n\nimphist * age\n\n\n\n\n\n\n-0.16\n-0.24, -0.08\n\n\n\n\n\n\n\n\n\n1 \nHR = Hazard Ratio, CI = Confidence Interval\n\n\n    \n\n\n\n\nAs expected we see that the raking and imputation - influence model have the lowest standard errors. Lumley notes that this is often the case when the imputation model is very good, further stating that the raking approach has the advantage of always being valid, while the calibration depends on correct modeling.\nSome history of the two phase calibration estimator\nHere Lumley gives a brief overview of the history of calibration and augmented inverse-probability model based estimators, which were developed in parallel and have a large amount of overlap.\nI’d like to raise some questions I found myself asking as I read this last section on using auxiliary information from phase 1.\n\nThe first concerns uncertainty estimates. Lumley doesn’t mention it, but the use of an imputation model should incorporate greater uncertainty into our final estimates. It doesn’t look like his software makes any effort to incorporate that uncertainty or how optimistic our final standard errors might be.\nThe use of influence functions remind me of, but are not exactly similar to dimensionality reduction methods, like principle components analysis I can’t help but wonder if there’s a way to reduce many of the influence functions to a few variables, or if some penalization methods would be required in the case of constructing many influence functions\nAs I mentioned earlier, it isn’t clear how we choose which variables should be regressed upon the influence function set-up. Why impute histology as opposed to tumor stage? Is it because it’s the most strongly associated with the outcome? How is someone supposed to know this in practice or a priori?\n\nOverall I’m glad that Lumley included this subject in his book as its very important but there were a lot of questions I feel still needed answering in this last section."
  },
  {
    "objectID": "ComplexSurveyNotes.html#exercises-7",
    "href": "ComplexSurveyNotes.html#exercises-7",
    "title": "Complex Survey Notes",
    "section": "Exercises",
    "text": "Exercises\n\nSuppose a phase one simple random sample of size n is taken from a population of size N, to measure a variable X with G categories. Write N_1, N_2, ..., N_G for the (unknown) number of individual in the population in each category, and n_1,...,n_g for the number in the phase one sample. The phase-two sample takes a fixed number m from each category. Show that \\pi_i^*, \\pi_{ij}^* for this design approach \\pi_i and \\pi_{ij} for a stratified sample from the population as n increases.\n\nI’ll start with defining the weights in stratified sample design for an arbitrary strata g. The sampling probability \\pi_i = \\frac{m}{N_g} and the co-sampling probability is \\pi_{ij} = \\frac{m}{N_g}(m-1)(N_g - 1). These values can be found from Chapter 2 question 10.\nFrom the beginning of chapter 8 we have that \\pi_{i}^* = \\pi_{i1} \\times \\pi_{i(2|1)} and the same is true for the co-inclusion probabilities. In which case we have \\pi_{i1} = \\frac{n}{N} and \\pi_{i(2|1)} = \\frac{m}{n_g}. Which gives us \n\\pi_{i*} = \\frac{n}{N}\\frac{m}{n_g}\n I’m not sure my notation is set up exactly right to answer Lumley’s question but I can reason my way there.\nAs \\n \\to N we’ll have more and more of the population in our first phase sample or in other words the first stage sampling probability will be 1 ; \\frac{N}{N} = 1. In which case we’re sampling from a strata with known strata size, N_g which is equivalent to the sampling probability we saw before.\nFor the co-inclusion probability we have a similar phenomenon where n \\to N results in a leading factor becoming 1.\n\n\\pi_{ij,1} = \\frac{n}{N}(n-1)(N-1) \\\\\n\\pi_{ij,(2|1)} = \\frac{m}{n_g}(m-1)(n_g-1) \\\\\n\\implies \\pi_{ij}^* = \\frac{n}{N}(n-1)(N-1)\\frac{m}{n_g}(m-1)(n_g-1) \\\\\n\\implies \\pi_{ij}^* \\stackrel{n\\to N}{=} 1 \\times (N-1)^2 \\frac{m}{N_g}(m-1)(N_g - 1) \\\\\n= \\frac{m}{N_g}(m-1)(N_g - 1) (N - 1)^2\n which is the same except for the (N-1)^2 constant. I’m wondering if this sampling probability assumes sampling with replacement… otherwise the conclusion probability should simplify — easily — to 1, the same as for \\pi_i, the only way you’d have any probability of sampling two things together being more than 1, is if you were sampling with replacement…\n\nConstruct a full two-phase data set for the Ille-et Vilaine case-control study.The additional phase-one observations are 430000-975 controls to make the number up to the population size. Fit the logistic regression model using the design produced by twophase() and compare the results to the weighted estimates in Figure 8.1.\n\n\nShow the codecases &lt;- cbind(esoph[rep(1:88, esoph$ncases), ], case = 1, weight = 1)\ncontrols &lt;- cbind(esoph[rep(1:88, esoph$ncontrols * 441), ], case = 0, weight = 1)\nesoph_full &lt;- rbind(cases, controls) %&gt;%\n  mutate(\n    ix = 1:n(),\n    in_second_phase = if_else(case == 1 | case == 0 & ix %% 441 == 1, TRUE,\n      FALSE\n    )\n  )\n\ntwop_design &lt;- twophase(list(~1, ~1),\n  strata = list(NULL, ~case),\n  subset = ~in_second_phase,\n  data = esoph_full\n)\n\nfit &lt;- svyglm(case ~ agegp + as.numeric(tobgp) + as.numeric(alcgp),\n  design = twop_design, family = quasibinomial()\n)\n\ntbl_merge(\n  tbls = list(\n    tbl_regression(unwtd), tbl_regression(wtd),\n    tbl_regression(fit)\n  ),\n  tab_spanner = c(\"**Unweighted**\", \"**Weighted**\", \"**Two Phase**\")\n)\n\n\n\n\n\n\n\n\nCharacteristic\n\n      \n        \nUnweighted\n\n      \n      \n        \nWeighted\n\n      \n      \n        \nTwo Phase\n\n      \n    \n\n\n\nlog(OR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \n\nlog(OR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \n\nlog(OR)\n\n1\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n    \n\n\n\nagegp\n\n\n\n\n\n\n\n\n\n\n\n    agegp.L\n3.8\n2.7, 5.6\n\n\n3.9\n2.5, 5.2\n\n\n3.9\n2.5, 5.2\n\n\n\n    agegp.Q\n-1.5\n-3.1, -0.51\n0.014\n-1.4\n-2.6, -0.16\n0.026\n-1.4\n-2.6, -0.17\n0.026\n\n\n    agegp.C\n0.08\n-0.73, 1.2\n0.9\n0.29\n-0.63, 1.2\n0.5\n0.29\n-0.63, 1.2\n0.5\n\n\n    agegp^4\n0.12\n-0.58, 0.74\n0.7\n0.25\n-0.40, 0.90\n0.4\n0.25\n-0.40, 0.90\n0.4\n\n\n    agegp^5\n-0.25\n-0.67, 0.17\n0.2\n-0.28\n-0.73, 0.17\n0.2\n-0.28\n-0.73, 0.17\n0.2\n\n\nas.numeric(tobgp)\n0.44\n0.25, 0.63\n\n\n0.45\n0.21, 0.69\n\n\n0.45\n0.21, 0.68\n\n\n\nas.numeric(alcgp)\n1.1\n0.87, 1.3\n\n\n1.0\n0.80, 1.3\n\n\n1.0\n0.80, 1.3\n\n\n\n\n\n1 \nOR = Odds Ratio, CI = Confidence Interval\n\n\n    \n\n\n\n\nWe can see from the table above that the two phase and weighted results are effectively the same. This makes sense, given the large size of the phase one sample.\n\nThis exercise uses the WA State crime data for 2004 as the population. The data consist of crime rates and population size for the police districts and sheriffs offices grouped by county.\n\n\nShow the codewa_crime_df &lt;- readxl::read_xlsx(\"data/WA_crime/1984-2011.xlsx\",\n  skip = 4\n) %&gt;%\n  filter(Year == \"2004\", Population &gt; 0) %&gt;%\n  mutate(\n    murder = `Murder Total`,\n    murder_and_crime = `Murder Total` + `Burglary Total`,\n    violent_crime = `Violent Crime Total`,\n    burglaries = `Burglary Total`,\n    property_crime = `Property Crime Total`,\n    state_pop = sum(Population),\n    County = stringr::str_to_lower(County),\n    num_counties = n_distinct(County),\n  ) %&gt;%\n  group_by(County) %&gt;%\n  mutate(num_agencies = n_distinct(Agency)) %&gt;%\n  ungroup() %&gt;%\n  select(\n    County, Agency, Population, murder_and_crime, murder, violent_crime,\n    property_crime, burglaries, num_counties, num_agencies\n  )\n\n\n\nTake a simple random sample of 20 police districts from the state and use all the data from the sampled counties. Estimate the total number of murders and burglaries in the state\n\nNot clear what Lumley means here, given that he first says to sample at the police district level but then says to use all the data at the county level…\nGiven this mix up I proceed assuming he meant that we should sample at the county level and then use all the data for those counties.\n\nShow the codeset.seed(35315)\ncounty_sample &lt;- wa_crime_df %&gt;%\n  distinct(County) %&gt;%\n  slice_sample(n = 20) %&gt;%\n  pull(County)\n\nwa_crime_df %&gt;%\n  filter(County %in% county_sample) %&gt;%\n  as_survey_design(\n    ids = c(County, Agency),\n    fpc = c(num_counties, num_agencies)\n  ) %&gt;%\n  summarize(\n    total = survey_total(murder_and_crime, vartype = \"ci\", deff = TRUE)\n  )\n\n# A tibble: 1 × 4\n   total total_low total_upp total_deff\n   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 49532.    23911.    75153.       2.60\n\n\n\nCalibrate the sample in the previous question using population as the auxiliary variable and estimate the total number of murders and burglaries in the state.\n\n\nShow the codesrs_design &lt;- wa_crime_df %&gt;%\n  filter(County %in% county_sample) %&gt;%\n  as_survey_design(\n    ids = c(County, Agency),\n    fpc = c(num_counties, num_agencies)\n  )\n\ncalibrated_srs_design &lt;- calibrate(srs_design,\n  formula = ~1,\n  population = sum(wa_crime_df$Population)\n)\n\nsvytotal(~murder_and_crime, calibrated_srs_design)\n\n                      total        SE\nmurder_and_crime 1599843753 232267644\n\n\nMy use of the calibrate() function continues to spit out ridiculous numbers. I’ll try to debug this later but for now all I can say is that my cursory look through the documentation for the function doesn’t show any egregious misuse.\n\nTake a simple random sample of 100 police districts as phase one of a two phase design, and assume that population is the only variable available at phase one. Divide the sample into 10 strata with roughly equal total population and sample two police districts from each stratum for phase two. Estimate the total number of murders and of burglaries in the state.\n\n\nShow the codeset.seed(33531)\np1_sample &lt;- wa_crime_df %&gt;%\n  distinct(Agency) %&gt;%\n  slice_sample(n = 100) %&gt;%\n  pull(Agency)\n\np1_pop &lt;- wa_crime_df %&gt;%\n  filter(Agency %in% p1_sample) %&gt;%\n  pull(Population)\np1_pop_quantiles &lt;- quantile(p1_pop, seq(from = 0, to = 1, length.out = 10))\n\np2_sample &lt;- wa_crime_df %&gt;%\n  filter(Agency %in% p1_sample) %&gt;%\n  mutate(strata = cut(Population,\n    breaks = p1_pop_quantiles,\n    include.lowest = TRUE\n  )) %&gt;%\n  select(Agency, strata) %&gt;%\n  group_by(strata) %&gt;%\n  slice_sample(n = 2) %&gt;%\n  ungroup() %&gt;%\n  pull(Agency)\n\ntwop_design &lt;- wa_crime_df %&gt;%\n  filter(Agency %in% p1_sample) %&gt;%\n  mutate(\n    strata = cut(Population, breaks = p1_pop_quantiles, include.lowest = TRUE),\n    in_phase_two = Agency %in% p2_sample,\n    fpc_one = 234,\n  ) %&gt;%\n  group_by(strata) %&gt;%\n  mutate(fpc_two = n()) %&gt;%\n  ungroup() %&gt;%\n  as_survey_twophase(\n    id = list(Agency, Agency),\n    strata = list(NULL, strata),\n    subset = in_phase_two,\n    fpc = list(fpc_one, fpc_two)\n  )\n\ntwop_design %&gt;%\n  summarize(\n    total = survey_total(murder_and_crime, deff = TRUE)\n  )\n\n# A tibble: 1 × 3\n   total total_se total_deff\n   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 45205.    9184.     0.0152\n\n\nWe see a much lower uncertainty around the total estimate of the number of murders and crimes relative to the previous design. This is especially exemplified in the design effect.\n\nCalibrate the two phase sample using the phase-one population size data. Estimate the total number of murders and of burglaries in the state.\n\n\nShow the codetwop_calibration &lt;- calibrate(twop_design, ~Population,\n  phase = \"2\"\n)\nsvytotal(~murder_and_crime, twop_calibration, deff = TRUE)\n\n                 total    SE   DEff\nmurder_and_crime 64008 10268 0.0128\n\n\nHere at least the number makes sense, though it looks like the standard error has increased relative to the standard two phase design. This might be because the population information is already encoded in the strata design of the second phase of sampling.\n\nThe sampling probabilities \\pi and \\pi^* in the NWTS two phase case-control study depend on the 2 x 2 table of relaps and instit. Suppose that the super population probabilities for the cells in the 2 x 2 table match those in Table 8.3.\n\n\nWrite R code to simulate realizations of Table 8.3 and to compute the second phase sampling probabilities \\pi_{i(2|1)} for a two-phase design with sample size 1300 and cell counts as equal as possible. That is, sample everyone in a cell that has fewer than 1300 people and then divide the remaining sample size evenly over the remaining cells.\n\nI see a lot of ways to conduct the second phase sampling that could be influential on the results. For example, do we try to ensure that the cell counts are as equal as possible, even at the expense of throwing away sample size? Or should we take as many samples as we can get even if it results in more unequal cell counts.\n\nShow the codeprobs &lt;- c(\n  \"Relapse_Unfav\" = 194 / 3915, \"Relapse_Fav\" = 475 / 3915,\n  \"Control_Unfav\" = 245 / 3915, \"Control_Fav\" = 3001 / 3915\n)\nprobs_df &lt;- as_tibble(t(probs)) %&gt;%\n  gather(everything(), key = \"Cell\", value = \"pi_one\")\nsims &lt;- t(rmultinom(1000, size = 3915, prob = probs)) %&gt;%\n  as_tibble() %&gt;%\n  mutate(sim_ix = 1:n()) %&gt;%\n  gather(everything(), -sim_ix, key = \"Cell\", value = \"Count\") %&gt;%\n  arrange(sim_ix, Count) %&gt;%\n  group_by(sim_ix) %&gt;%\n  mutate(\n    cell_rank = min_rank(Count),\n    initial_sample = sum((cell_rank == 1) * Count),\n    remaining_sample = (1300 - initial_sample) / 3,\n    twop_sampling_probability = case_when(\n      # sample everything from smallest cell with probability 1\n      cell_rank == 1 ~ 1,\n      remaining_sample &gt; Count ~ 1,\n      remaining_sample &lt; Count ~ remaining_sample / Count\n    )\n  ) %&gt;%\n  left_join(probs_df) %&gt;%\n  mutate(pi_star = pi_one * twop_sampling_probability)\n\n\n\nRun 1000 simulations, compute \\pi_i as the average of \\pi_i^* for a given cell over the simulations, and compare \\pi_i to the distribution of \\pi_i^*.\n\n\nShow the codepi_i &lt;- sims %&gt;%\n  group_by(Cell) %&gt;%\n  summarize(mean_p = mean(pi_star))\nsims %&gt;%\n  ggplot(aes(x = pi_star, fill = Cell)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = pi_i$mean_p[1]), linetype = 2, color = \"red\") +\n  geom_vline(aes(xintercept = pi_i$mean_p[2]), linetype = 2, color = \"red\") +\n  geom_vline(aes(xintercept = pi_i$mean_p[3]), linetype = 2, color = \"black\") +\n  geom_vline(aes(xintercept = pi_i$mean_p[4]), linetype = 2, color = \"red\") +\n  ggtitle(\"Distribution of Product Probabilities\",\n    subtitle = \"Dotted Lines Indicate Average of Cell Distribution\"\n  )\n\n\n\n\nAs constructed, the \\pi_i are the average of the \\pi_i^* so, naturally, these appear in the center of the distributions. What is perhaps more relieving is that the distributions either have low variance, when they have small probabilities of occurring, or are nicely normally distributed when they are, which, again, makes sense.\n\nThis exercise uses the WA state crime data for 2004 as the population and the data for 2003 as the auxiliary variables.\n\n\nTake a simple random sample of 20 police districts from the state and use all the data from the sampled counties. Estimate the ratio of violent crimes to property crimes.\n\n\nShow the codewa_crime_df &lt;- readxl::read_xlsx(\"data/WA_crime/1984-2011.xlsx\",\n  skip = 4\n) %&gt;%\n  filter(Year %in% c(\"2003\", \"2004\"), Population &gt; 0) %&gt;%\n  mutate(\n    murder = `Murder Total`,\n    murder_and_crime = `Murder Total` + `Burglary Total`,\n    violent_crime = `Violent Crime Total`,\n    burglaries = `Burglary Total`,\n    property_crime = `Property Crime Total`,\n    state_pop = sum(Population),\n    County = stringr::str_to_lower(County),\n    num_counties = n_distinct(County),\n  ) %&gt;%\n  group_by(County) %&gt;%\n  mutate(num_agencies = n_distinct(Agency)) %&gt;%\n  ungroup() %&gt;%\n  select(\n    Year, County, Agency, Population, murder_and_crime, murder, violent_crime,\n    property_crime, burglaries, num_counties, num_agencies\n  )\n\ndistrict_sample &lt;- wa_crime_df %&gt;%\n  filter(Year == \"2004\") %&gt;%\n  distinct(Agency) %&gt;%\n  slice_sample(n = 20) %&gt;%\n  pull(Agency)\n\nsrs_design &lt;- wa_crime_df %&gt;%\n  filter(Year == \"2004\") %&gt;%\n  mutate(fpc = n_distinct(Agency)) %&gt;%\n  filter(Agency %in% district_sample) %&gt;%\n  select(Year, County, Agency, Population, violent_crime, property_crime, fpc) %&gt;%\n  left_join(wa_crime_df %&gt;% filter(Year == \"2003\") %&gt;%\n    select(County, Agency, burglaries, Population) %&gt;%\n    rename_if(is.numeric, function(x) str_c(x, \"_2003\"))) %&gt;%\n  as_survey_design(\n    ids = \"Agency\",\n    fpc = fpc\n  )\n\nsrs_design %&gt;%\n  summarize(\n    survey_ratio(violent_crime, property_crime)\n  )\n\n# A tibble: 1 × 2\n    coef   `_se`\n   &lt;dbl&gt;   &lt;dbl&gt;\n1 0.0445 0.00768\n\n\n\nCalibrate the sample in the previous question using population and number of burglaries as the auxiliary variables, and estimate the ratio of violent crimes to property crimes.\n\n\nShow the codepop_totals &lt;- wa_crime_df %&gt;% \n  filter(Year == \"2003\") %&gt;% \n  summarize(\n    burglaries_2003 = sum(burglaries),\n    Population_2003 = sum(Population))\n\nsrs_calibration &lt;- calibrate(srs_design,\n                             ~ burglaries_2003 + ~Population_2003 - 1,\n  population = pop_totals\n)\nsvyratio(~violent_crime, ~property_crime, srs_calibration)\n\nRatio estimator: svyratio.survey.design2(~violent_crime, ~property_crime, srs_calibration)\nRatios=\n              property_crime\nviolent_crime      0.0437048\nSEs=\n              property_crime\nviolent_crime    0.008199953\n\n\nOur point estimate is about the same, but the standard error has increased ever so slightly.\n\nThe ratio of violent crimes to property crimes in the state in 2003 was 21078 / 290945 = 0.0724. Define an auxiliary variable infl = violent - 0.0724 x property, the influence function for the ratio, and use it to calibrate to the state data. Estimate the ratio of violent crimes to property crimes.\n\n\nShow the codesrs_design &lt;- update(srs_design,\n  infl = violent_crime - 0.0724 * property_crime\n)\n\npop_total &lt;- wa_crime_df %&gt;% \n  filter(Year == \"2003\") %&gt;% \n  mutate(infl = violent_crime - 0.0724 * property_crime) %&gt;% \n  summarize(\n    infl = sum(infl)\n  )\n\nsrs_calibration &lt;- calibrate(srs_design, ~infl -1,\n  population = pop_total\n)\nsvyratio(~violent_crime, ~property_crime, srs_calibration)\n\nRatio estimator: svyratio.survey.design2(~violent_crime, ~property_crime, srs_calibration)\nRatios=\n              property_crime\nviolent_crime     0.07456344\nSEs=\n              property_crime\nviolent_crime    0.001312338\n\n\nNow we see a pretty much spot on estimate and the lowest standard error of them all.\n\nTake a simple random sample of 100 police districts as phase one of a two-phase design, and assume that the 2003 crime data are available at phase one. Divide the sample into 10 strata with roughly equal total number of burglaries in 2003 and sample two police districts from each stratum for phase two. Estimate the total number of murders, and of burglaries, in the state.\n\n\nShow the codeset.seed(35315)\ndistrict_sample &lt;- wa_crime_df %&gt;%\n  filter(Year == \"2004\") %&gt;%\n  distinct(Agency) %&gt;%\n  slice_sample(n = 100) %&gt;%\n  pull(Agency)\n\nburglary_quantiles &lt;- wa_crime_df %&gt;%\n  filter(Year == \"2003\") %&gt;%\n  pull(burglaries) %&gt;%\n  quantile(., seq(from = 0, to = 1, length.out = 10))\n\nsecond_phase_sample &lt;- wa_crime_df %&gt;%\n  filter(Year == \"2004\", Agency %in% district_sample) %&gt;%\n  mutate(\n    strata = cut(burglaries, burglary_quantiles, include.lowest = TRUE)\n  ) %&gt;%\n  group_by(strata) %&gt;%\n  slice_sample(n = 2) %&gt;%\n  ungroup() %&gt;%\n  pull(Agency)\n\n\nwa_crime_df %&gt;%\n  filter(Year == \"2004\") %&gt;%\n  mutate(\n    fpc_one = n_distinct(Agency),\n    strata = cut(burglaries, burglary_quantiles, include.lowest = TRUE)\n  ) %&gt;%\n  filter(Agency %in% district_sample) %&gt;%\n  mutate(\n    in_second_phase = (Agency %in% second_phase_sample),\n    fpc_two = n(),\n  ) %&gt;%\n  as_survey_twophase(\n    id = list(Agency, Agency),\n    strata = list(NULL, strata),\n    subset = in_second_phase,\n    fpc = list(fpc_one, fpc_two)\n  ) %&gt;%\n  summarize(\n    num_burglaries = survey_total(burglaries),\n    num_murders = survey_total(murder)\n  )\n\n# A tibble: 1 × 4\n  num_burglaries num_burglaries_se num_murders num_murders_se\n           &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n1         650988           318526.        2808           773.\n\n\nThe results are very imprecise, especially for the number of burglaries for which we’re not able to compute a standard error.\n\nCalibrate the two-phase sample using the auxiliary variable infl. Estimate the total number of murders, and of burglaries, in the state.\n\n\nShow the codetwo_phase_crime_design &lt;- wa_crime_df %&gt;%\n  filter(Year == \"2004\") %&gt;%\n  mutate(\n    fpc_one = n_distinct(Agency),\n    strata = cut(burglaries, burglary_quantiles, include.lowest = TRUE),\n    infl = violent_crime - 0.0724 * property_crime,\n  ) %&gt;%\n  filter(Agency %in% district_sample) %&gt;%\n  mutate(\n    in_second_phase = (Agency %in% second_phase_sample),\n    fpc_two = n(),\n  ) %&gt;%\n  as_survey_twophase(\n    id = list(Agency, Agency),\n    strata = list(NULL, strata),\n    subset = in_second_phase,\n    fpc = list(fpc_one, fpc_two)\n  )\n\ntotal_infl &lt;- wa_crime_df %&gt;%\n  filter(Year == \"2004\") %&gt;%\n  summarize(infl = sum(violent_crime - 0.0724 * property_crime)) %&gt;%\n  pull(infl)\n\n\ncalibrate_tp_cd &lt;- calibrate(two_phase_crime_design, ~infl - 1,\n  population = total_infl,\n  phase = \"2\"\n)\nsvytotal(~ burglaries + ~murder, design = calibrate_tp_cd)\n\n              total        SE\nburglaries 56029.82 196962.95\nmurder       707.75    642.11\n\n\nStill highly variable answers, even with the calibration!\n\nWhat would the efficiency approximation from exercise 5.8 give as the loss of efficiency from using weights in a case-control design?\n\nTODO(apeterson)"
  },
  {
    "objectID": "ComplexSurveyNotes.html#item-non-response",
    "href": "ComplexSurveyNotes.html#item-non-response",
    "title": "Complex Survey Notes",
    "section": "Item Non-Response",
    "text": "Item Non-Response\nLumley opens this chapter by drawing a distinction between item and unit non-response. Item non-response refers to the case in which partial data is available from a sampled respondent or observation, but not all. In contrast, unit non-response occurs when no data at all is available from a sampled respondent or unit of analysis.\nLumley then lays out two classes of approach to handle item non-response that correspond to both design and modeling philosophies:\n\nDesign — Frame the non-response as part of the sampling mechanism in a two phase design.\nImputation — Model the non-response as a function of other measured covariates and estimate the missing values."
  },
  {
    "objectID": "ComplexSurveyNotes.html#two-phase-estimation-for-missing-data",
    "href": "ComplexSurveyNotes.html#two-phase-estimation-for-missing-data",
    "title": "Complex Survey Notes",
    "section": "Two Phase Estimation for Missing Data",
    "text": "Two Phase Estimation for Missing Data\nAs described above, if the pattern of missingness is relatively simple, such that the sample can be divided into two groups: (1) Those with complete data and (2) those with incomplete data, then the data can be modeled as a two phase sample in which the first phase individuals have incomplete data and the second phase have complete data.\nCalibration for item non-response\nKnowing the mechanism by which individuals’ data went missing is typically not feasible but Lumley recommends starting with a simple random sample weight and then calibrating those weights according to other available covariates.\n\nShow the codeset.seed(31631)\nsigmoid &lt;- binomial()$linkinv\npmar &lt;- with(apistrat, sigmoid(-7 + api99 / 100 - emer / 10))\npnar &lt;- with(apistrat, sigmoid( -7 + api00 / 100 - emer / 10))\nmar &lt;- rbinom(nrow(apistrat), 1, pmar)\nnar &lt;- rbinom(nrow(apistrat), 1, pnar)\n\nstratmar &lt;- apistrat\nstratmar$api00[mar == 1] &lt;- NA\nstratmar$w2 &lt;- nrow(apistrat) / sum(1 - mar) ## MCAR sampling weights\nstratnar &lt;- apistrat\nstratnar$api00[nar == 1] &lt;- NA\nstratnar$w2 &lt;- nrow(apistrat) / sum(1 - nar)\n\nmar_des &lt;- twophase(id = list(~1, ~1),\n                    strata = list(~stype, ~stype),\n                    subset = ~I(!is.na(api00)),\n                    weights = list(~pw, ~w2),\n                    # difference from original code\n                    method = 'approx',\n                    data = stratmar)\n\nnar_des &lt;- twophase(id = list(~1, ~1),\n                    strata = list(~stype, ~stype),\n                    subset = ~I(!is.na(api00)),\n                    weights = list(~pw, ~w2),\n                    method = \"approx\",\n                    data = stratnar)\n\ncalmar1 &lt;- calibrate(mar_des, phase = 2, calfun = \"raking\",\n                     ~api99 + emer + stype + enroll)\n\ncalnar1 &lt;- calibrate(nar_des, phase = 2, calfun = \"raking\",\n                     ~api99 + emer + stype + enroll)\n\ncalmar2 &lt;- calibrate(mar_des, phase = 2, calfun = \"raking\",\n                    ~ ns(api99, 3) + emer + stype + enroll)\n\ncalnar2 &lt;- calibrate(nar_des, phase = 2, calfun = \"raking\",\n                     ~ns(api99, 3) + emer + stype + enroll)\n\ndstrat &lt;- svydesign(id = ~1, strata = ~stype, weights = ~pw,\n                    data = apistrat, fpc = ~fpc)\ndstrat_mar &lt;- svydesign(id = ~1, strata = ~stype, weights = ~pw,\n                    data = stratmar, fpc = ~fpc)\ndstrat_nar &lt;- svydesign(id = ~1, strata = ~stype, weights = ~pw,\n                    data = stratnar, fpc = ~fpc)\n\n# This is included in the book code, but I focus on the glm output\n# for ease of illustration.\n# svymean(~api99 + api00, dstrat)\nfit_full &lt;- svyglm(api00 ~ emer + ell + meals, dstrat)\nfit_naive &lt;- svyglm(api00 ~ emer + ell + meals, dstrat_mar)\nfit_mar1 &lt;- svyglm(api00 ~ emer + ell + meals, calmar1)\nfit_mar2 &lt;- svyglm(api00 ~ emer + ell + meals, calmar2)\nfit_naive_n &lt;- svyglm(api00 ~ emer + ell + meals, dstrat_nar)\nfit_nar1 &lt;- svyglm(api00 ~ emer + ell + meals, calnar1)\nfit_nar2 &lt;- svyglm(api00 ~ emer + ell + meals, calnar2)\ntbl_merge(\n  tbls = list(\n    tbl_regression(fit_full, conf.int = FALSE),\n    tbl_regression(fit_naive, conf.int = FALSE),\n    tbl_regression(fit_mar1, conf.int = FALSE),\n    tbl_regression(fit_mar2, conf.int = FALSE),\n    tbl_regression(fit_naive_n, conf.int = FALSE),\n    tbl_regression(fit_nar1, conf.int = FALSE),\n    tbl_regression(fit_nar2, conf.int = FALSE)\n  ),\n  tab_spanner = c(\n   \"**Full Data**\",\n   \"**Naive - MAR**\",\n   \"**Linear - MAR**\",\n   \"**NonLinear - MAR**\",\n   \"**Naive - NAR**\",\n   \"**Linear - NAR**\",\n   \"**NonLinear - NAR**\"\n  )\n)\n\n\n\n\n\n\n\n\nCharacteristic\n\n      \n        \nFull Data\n\n      \n      \n        \nNaive - MAR\n\n      \n      \n        \nLinear - MAR\n\n      \n      \n        \nNonLinear - MAR\n\n      \n      \n        \nNaive - NAR\n\n      \n      \n        \nLinear - NAR\n\n      \n      \n        \nNonLinear - NAR\n\n      \n    \n\n\nBeta\n\n      \np-value\n\n      \nBeta\n\n      \np-value\n\n      \nBeta\n\n      \np-value\n\n      \nBeta\n\n      \np-value\n\n      \nBeta\n\n      \np-value\n\n      \nBeta\n\n      \np-value\n\n      \nBeta\n\n      \np-value\n\n    \n\n\n\nemer\n-1.8\n0.006\n-1.6\n0.016\n-1.6\n0.030\n-2.3\n0.004\n-1.5\n0.019\n-1.7\n0.021\n-2.2\n0.006\n\n\nell\n-0.56\n0.12\n-0.69\n0.083\n-0.54\n0.2\n-0.62\n0.14\n-0.53\n0.2\n-0.34\n0.4\n-0.47\n0.3\n\n\nmeals\n-2.7\n\n\n-2.2\n\n\n-2.5\n\n\n-2.4\n\n\n-2.5\n\n\n-2.7\n\n\n-2.7\n\n\n\n\n\n\n\nAbove I show one iteration of a missing at random analysis, though in the book Lumley shows the % bias in a simulation study of missing data. In his analysis he identifies a reduction in bias from using the (linear) calibration in both the missing at random data and not missing at random data. The flexible calibration does worse, he argues, because of the low sample size.\nModels for Response Probability\nIn addition to calibrating the sampling weights in the hypothetical two-phase sample, we can also model the second phase conditional probabilities using a (e.g.) logistic regression.\nLumley notes that this approach does not satisfy the same constraints that calibrating does - calibration guarantees estimated totals in phase one match their true counterpart, while the regression guarantees the same for the second phase.\nExample NHANES III bone density\nThis example looks at missing bone mineral density measurements from the NHANES study.\n\nShow the codesqlite &lt;- dbDriver(\"SQLite\")\ndbcon &lt;- dbConnect(sqlite, dbname = \"data/nhanes/imp.db\")\nnhanes &lt;- dbGetQuery(dbcon, \"select SDPPSU6, WTPFQX6, SDPSTRA6,\n                     HSAGEU, HSAGEIR, DMARETHN, HSSEX, DMARETHN, DMPMETRO,\n                     DMPCREGN, BMPWTMI, BMPWTIF, BDPFNDMI, BDPFNDIF\n                     from set1\")\ndbDisconnect(dbcon)\n\nnhanes &lt;- subset(nhanes, BDPFNDIF &gt; 0)\nnhanes$hasbone &lt;- 2 - nhanes$BDPFNDIF\nnhanes$age &lt;- with(nhanes, ifelse(HSAGEU == 1, HSAGEIR / 12, HSAGEIR))\nnhanes &lt;- subset(nhanes, age &gt; 20)\nnhanes$psu &lt;- with(nhanes, SDPPSU6 + 10 * SDPSTRA6)\nnhanes$agegp &lt;- with(nhanes, cut(age, c(20, 40, 60 , Inf)))\n\nmodel &lt;- glm(hasbone ~ (BMPWTMI + age) * \n               (HSSEX * DMARETHN + DMPMETRO * DMPCREGN), family = binomial,\n             data = nhanes)\nnhanes$po &lt;- mean(nhanes$hasbone)\nnhanes$pi2 &lt;- fitted(model)\ndesign0 &lt;- twophase(id = list(~psu, ~1), strata = list(~SDPSTRA6, NULL),\n                    weights = list(~WTPFQX6, ~I(1 / po)), \n                    subset = ~I(hasbone == 1),\n                    ## necessary for updated version of survey package\n                    method = \"approx\",\n                    data = nhanes)\ndesign1 &lt;- twophase(id = list(~psu, ~1), strata = list(~SDPSTRA6, NULL),\n                    weights = list(~WTPFQX6, ~I(1 / pi2)), \n                    subset = ~I(hasbone == 1),\n                    ## necessary for updated version of survey package\n                    method = \"approx\",\n                    data = nhanes)\n\ndesign2 &lt;- calibrate(design0, phase = 2, \n                     formula = ~(BMPWTMI + age) * \n                       (HSSEX * DMARETHN + DMPMETRO * DMPCREGN))\n\n\n\nShow the codetibble(\n  design = c(\"Constant Non-Response Probability\",\n             \"Modeled Non-Response Probability\",\n             \"Calibrated Non-Response\"),\n  estimate = c(svymean(~BDPFNDMI, design0),\n               svymean(~BDPFNDMI, design1),\n               svymean(~BDPFNDMI, design2)),\n  lower = c(confint(svymean(~BDPFNDMI, design0))[1],\n            confint(svymean(~BDPFNDMI, design1))[1],\n            confint(svymean(~BDPFNDMI, design2))[1]),\n  upper = c(confint(svymean(~BDPFNDMI, design0))[2],\n            confint(svymean(~BDPFNDMI, design1))[2],\n            confint(svymean(~BDPFNDMI, design2))[2]),\n) %&gt;%  \n  mutate(\n    fmt_estimate = str_c(round(estimate, 3),\n                         \"(\", round(lower, 3),\n                         \", \", round(upper, 3),\n                         \")\"),\n  ) %&gt;% \n  select(design, fmt_estimate) %&gt;% \n  spread(design,fmt_estimate) %&gt;% \n  gt::gt()\n\n\n\n\n\n\nCalibrated Non-Response\n      Constant Non-Response Probability\n      Modeled Non-Response Probability\n    \n\n0.817(0.811, 0.823)\n0.826(0.82, 0.831)\n0.818(0.812, 0.825)\n\n\n\n\n\nLumley points out that the impact of modeling the non response — a change in the point estimate of around .1 is larger than the standard error and so has the potential to impact the inference. Further, it isn’t clear what kind of definitive conclusion can be drawn from this adjustment but it seems likely that those who did not respond had a lower bone mineral density than those that did.\nEffect on Precision\nIn summarizing the impact of modeling or calibrating response probabilities on the estimate’s precision Lumley makes the following categorization about those variables used to calibrate or model non-response:\n\nAssociated with non-response but not with variables analyzed.\n\nDecrease precision, no impact on bias.\n\n\nAssociated with non-response & variables being analyzed.\n\nDecrease bias, may increase or decrease precision.\n\n\nAssociated only with variables analyzed and not with non-response.\n\n\nNo impact on bias, increase precision.\nDoubly Robust Estimators\nBoth of the previous methods that incorporated estimating the non-response into a two-phase sampling design hinged on getting the relationship between Y and non-response correct:\n\nThe two-phase estimator will be valid if the model for the response probabilities correctly models everything that affects both Y — the outcome variable — and non response.\nThe calibration estimator will be valid if the working model Y correctly models everything affects both Y and non-response.\n\nIt is also possible to separate the two models for non-response and the variable Y so that the estimator will be unbiased in large samples if either model is correct. Further, if the calibration is correct we can get increased precision. This approach is called a “Double Robust Estimator” and more can be found on it in (Bickel and Kwon 2001).\nI’m not entirely sure why Lumley included this section in his book as there’s no way I see to easily construct a doubly robust estimator from his survey R package."
  },
  {
    "objectID": "ComplexSurveyNotes.html#imputation-of-missing-data",
    "href": "ComplexSurveyNotes.html#imputation-of-missing-data",
    "title": "Complex Survey Notes",
    "section": "Imputation of Missing Data",
    "text": "Imputation of Missing Data\nLumley gives a brief description of imputation here which I’ll only note requires similar strong assumptions as the previous methods discussed — modelling the relationship between Y and non-response, but has the added benefit of being useful for other, more general analyses, as a complete dataset(s) is(are) provided instead of a single analysis’ specific weights.\nAs I alluded to above, typically multiple imputed data sets are provided to capture the added variability and uncertainty that the missing data adds to our analysis. I’ll note that this was added uncertainty was also not present in the previous methods described earlier. Lumley cites Rubin in identifying that the variability between m imputed data sets represents the added variance attributable to our missing data.\n\n\\hat{V}[T] = \\frac{1}{m} \\sum_{j=1}^{m} v_j + (\\frac{m + 1}{m}) \\frac{1}{m-1} \\sum_{j=1}^{m} (\\hat{T}_j - \\bar{T})^2\n\nDescribing Multiple Imputations in R\nI think the demo in the next section illustrates the point best, but the general idea Lumley gets at here is that each imputed data set needs to be given its own design object and analysis and then a special function needs to be called to ensure the results are combined appropriately.\nExample: NHANES Imputations\nThis example looks at the same bone mineral density data from NHANES as used previously in this chapter, only now we use the five imputed data sets provided by the CDC. A description of how these imputations were created can be found in (Schafer 2001). Recall, that creating the imputed data sets requires specialized knowledge, while using them requires only a knowledge of how to combine the imputed results together.\n\nShow the codeimpdata &lt;- imputationList(c(\"set1\", \"set2\", \"set3\", \"set4\", \"set5\"),\n                          dbtype = \"SQLite\", dbname = \"Data/nhanes/imp.db\")\nimpdata\n\nMI data with 5 datasets\nCall: imputationList(c(\"set1\", \"set2\", \"set3\", \"set4\", \"set5\"), dbtype = \"SQLite\", \n    dbname = \"Data/nhanes/imp.db\")\n\n\n\nShow the codedesigns &lt;- svydesign(id = ~SDPPSU6, strat = ~SDPSTRA6,\n                     weight = ~WTPFQX6, data = impdata, nest = TRUE)\n\ndesigns\n\nDB-backed Multiple (5) imputations: svydesign(id = ~SDPPSU6, strat = ~SDPSTRA6, weight = ~WTPFQX6, \n    data = impdata, nest = TRUE)\n\n\n\nShow the codedesigns &lt;- update(designs, age = ifelse(HSAGEU == 1, HSAGEIR / 12, HSAGEIR))\ndesigns &lt;- update(designs, agegp = cut(age, c(20, 40, 60, Inf, right = FALSE)))\nres &lt;- with(subset(designs, age &gt;= 20),\n            svyby(~BDPFNDMI, ~agegp + HSSEX, svymean))\nsummary(MIcombine(res))\n\nMultiple imputation results:\n      with(subset(designs, age &gt;= 20), svyby(~BDPFNDMI, ~agegp + HSSEX, \n    svymean))\n      MIcombine.default(res)\n             results          se    (lower    upper) missInfo\n(0,20].1   0.9649648 0.017813216 0.9296637 1.0002659     20 %\n(20,40].1  0.9317077 0.003545463 0.9246253 0.9387901     27 %\n(40,60].1  0.8362213 0.003859253 0.8285832 0.8438593     19 %\n(60,Inf].1 0.7652223 0.004013359 0.7573199 0.7731246     13 %\n(0,20].2   0.8671506 0.016246398 0.8344422 0.8998590     32 %\n(20,40].2  0.8505263 0.003199152 0.8442018 0.8568507     18 %\n(40,60].2  0.7776360 0.003620937 0.7705191 0.7847530     10 %\n(60,Inf].2 0.6428459 0.004189769 0.6342868 0.6514050     41 %\n\n\nThe summary(MIcombine(res)) call above uses the mitools R package methods for combining separate imputed estimates together to produce the output we see. Note in the output that the missInfo column describes the percentage of “missing information” - roughly the between imputations variance as a fraction of the total variance - akin to the design effect\nLumley doesn’t go into the substantive meaning of the results beyond comparing it to a previous published analysis. Likely because these will be used in the exercises at the end of the chapter.\nLumley’s next analysis uses replicate weights in a design-based logistic regression model. Lumley notes that the replicate weights are the same for all imputed models and that they were constructed using “Fay’s method” which makes better adjustments for when there are few observations present at the intersection of some observed covariate.\nI’m guessing that the replicate weights weren’t changed as the result of the imputation — if all original observations have imputed results. Still would be nice to have some commentary from Lumley on this point.\nThe code to perform the analysis is below. As before, Lumley offers no commentary on the results of the analysis.\n\nShow the codedesigns &lt;- update(designs,\n                  systolic = (PEP6G1MI + PEP6H1MI + PEP6I1MI) / 3)\nqres &lt;- with(subset(designs, age &gt;= 20),\n             svyby(~systolic + TCPMI, ~agegp + HSSEX, svyquantile, \n                   quantiles = c(0.5, 0.9), se = TRUE))\nsummary(MIcombine(qres), digits = 2)\n\nMultiple imputation results:\n      with(subset(designs, age &gt;= 20), svyby(~systolic + TCPMI, ~agegp + \n    HSSEX, svyquantile, quantiles = c(0.5, 0.9), se = TRUE))\n      MIcombine.default(qres)\n                        results    se (lower upper) missInfo\n(0,20].1:systolic.0.5       115  1.37    112    117     21 %\n(20,40].1:systolic.0.5      117  0.50    116    118      0 %\n(40,60].1:systolic.0.5      123  0.60    121    124     33 %\n(60,Inf].1:systolic.0.5     135  0.83    133    136      0 %\n(0,20].2:systolic.0.5       105  2.14    101    109     15 %\n(20,40].2:systolic.0.5      107  0.55    106    108     39 %\n(40,60].2:systolic.0.5      118  0.60    117    119     33 %\n(60,Inf].2:systolic.0.5     137  0.89    135    139     22 %\n(0,20].1:systolic.0.9       126  2.79    121    131      0 %\n(20,40].1:systolic.0.9      132  0.79    131    134     28 %\n(40,60].1:systolic.0.9      145  1.05    143    148     10 %\n(60,Inf].1:systolic.0.9     164  1.07    162    166      0 %\n(0,20].2:systolic.0.9       116  1.61    113    119      0 %\n(20,40].2:systolic.0.9      122  0.66    121    123      0 %\n(40,60].2:systolic.0.9      144  1.24    141    146     11 %\n(60,Inf].2:systolic.0.9     170  1.42    167    172     38 %\n(0,20].1:TCPMI.0.5          165  5.55    154    176      3 %\n(20,40].1:TCPMI.0.5         190  1.57    187    193     16 %\n(40,60].1:TCPMI.0.5         211  1.30    208    214      0 %\n(60,Inf].1:TCPMI.0.5        209  1.53    206    212     11 %\n(0,20].2:TCPMI.0.5          170  6.46    158    183     23 %\n(20,40].2:TCPMI.0.5         184  1.25    182    187     16 %\n(40,60].2:TCPMI.0.5         210  1.76    207    214     30 %\n(60,Inf].2:TCPMI.0.5        229  1.52    226    232     17 %\n(0,20].1:TCPMI.0.9          206 10.60    184    227     34 %\n(20,40].1:TCPMI.0.9         244  1.72    241    248      8 %\n(40,60].1:TCPMI.0.9         265  3.12    259    271      9 %\n(60,Inf].1:TCPMI.0.9        264  3.44    257    271      2 %\n(0,20].2:TCPMI.0.9          219 16.84    185    252     27 %\n(20,40].2:TCPMI.0.9         234  2.29    229    239     38 %\n(40,60].2:TCPMI.0.9         274  1.86    270    277      7 %\n(60,Inf].2:TCPMI.0.9        288  3.22    282    295     15 %\n\n\n\nShow the codesqlite &lt;- dbDriver(\"SQLite\")\ndbcon &lt;- dbConnect(sqlite, dbname = \"data/nhanes/imp.db\")\nimpload &lt;- function(varlist, conn) {\n  tables &lt;- paste(\"set\", 1:5, sep = \"\")\n  vars &lt;- paste(varlist, collapse = \",\")\n  query &lt;- paste(\"select\",vars,\"from @tab@\")\n  data &lt;- lapply(tables,\n                 function(table) dbGetQuery(conn, sub(\"@tab@\", table, query)))\n  mitools::imputationList(data)\n}\n\nregdata &lt;- impload(\n  c(\"SDPPSU6\", \"WTPFQX6\",\"SDPSTRA6\", \"HSAGEU\", \"HSAGEIR\", \"DMARETHN\",\n    \"BMPWTMI\", \"BMPHTMI\", \"DMPPIRMI\",\n    \"HAB1MI\", \"HAT28MI\", \"HSSEX\"), dbcon)\nwtnames &lt;- paste(paste(\"WTPQRP\", 1:52, sep = \"\"), collapse = \",\")\nregwts &lt;- dbGetQuery(dbcon, paste(\"select\", wtnames, \"from core\"))\ndesigns &lt;- svrepdesign(data = regdata, repweights = regwts,\n                       type = \"Fay\", rho = 0.3, weights = ~WTPFQX6, \n                       combined = TRUE)\ndbDisconnect(dbcon)\n\ndesigns &lt;- update(designs,\n                  age = ifelse(HSAGEU == 1, HSAGEIR / 12, HSAGEIR))\nadults &lt;- subset(designs, age &gt;= 20)\nadults &lt;- update(adults,\n                 race = factor(ifelse(DMARETHN == 4, 1, DMARETHN)),\n                 bmi =  BMPWTMI / (BMPHTMI / 100)^2,\n                 poverty = DMPPIRMI &lt;= 1,\n                 evgfp = factor(HAB1MI),\n                 activity = factor(HAT28MI, levels = c(3,1,2,-9)),\n                 agegp = cut(age, c(0, 20, 40, 60, Inf), right = FALSE)\n                 )\nadults &lt;- update(adults,\n                 overwt = ifelse(HSSEX == 1, bmi &gt;= 27.8, bmi &gt;= 27.3))\nmodels &lt;- with(adults,\n               svyglm(overwt ~ HSSEX + agegp + race + evgfp + activity + poverty,\n                      family = quasibinomial))\nsummary(MIcombine(models), digits = 3)\n\nMultiple imputation results:\n      with(adults, svyglm(overwt ~ HSSEX + agegp + race + evgfp + activity + \n    poverty, family = quasibinomial))\n      MIcombine.default(models)\n              results     se  (lower upper) missInfo\n(Intercept)   -1.5902 0.1122 -1.8106 -1.370     10 %\nHSSEX          0.0491 0.0507 -0.0504  0.149      5 %\nagegp[40,60)   0.6964 0.0549  0.5885  0.804      9 %\nagegp[60,Inf)  0.6065 0.0740  0.4614  0.752      4 %\nrace2          0.4481 0.0573  0.3358  0.560      2 %\nrace3          0.4103 0.0657  0.2815  0.539      3 %\nevgfp2         0.4281 0.0702  0.2904  0.566      4 %\nevgfp3         0.6852 0.0761  0.5359  0.834      6 %\nevgfp4         0.7669 0.0825  0.6052  0.929      3 %\nevgfp5         0.3757 0.1175  0.1453  0.606      6 %\nactivity1     -0.4014 0.0622 -0.5239 -0.279     14 %\nactivity2      0.2732 0.0564  0.1626  0.384      3 %\npovertyTRUE   -0.0160 0.0637 -0.1421  0.110     18 %"
  },
  {
    "objectID": "ComplexSurveyNotes.html#exercises-8",
    "href": "ComplexSurveyNotes.html#exercises-8",
    "title": "Complex Survey Notes",
    "section": "Exercises",
    "text": "Exercises\n\nFit regression models to examine how self-rated general health (HAB1MI) is related to age (HSAGEIR) and obesity (bmi as defined in Figure 9.7)\n\n\nUsing the NHANES III multiply imputed data, with replicate weights.\n\nI’ll use the same adults object defined in the previous example since it has all the appropriate variables loaded.\n\nShow the codemodels &lt;- with(adults,\n     svyglm(HAB1MI ~ age))\n\nsummary(MIcombine(models), digits = 3)\n\nMultiple imputation results:\n      with(adults, svyglm(HAB1MI ~ age))\n      MIcombine.default(models)\n            results       se (lower upper) missInfo\n(Intercept)  1.8457 0.032645 1.7817 1.9097      0 %\nage          0.0141 0.000731 0.0126 0.0155      0 %\n\n\nIn the above table we see a statistically significant positive relationship between age and self-rated general health, even after accounting for the missing data.\n\nUsing just the complete data before imputation.\n\n\nShow the codesqlite &lt;- dbDriver(\"SQLite\")\ndbcon &lt;- dbConnect(sqlite, dbname = \"data/nhanes/imp.db\")\nnhanes &lt;- dbGetQuery(dbcon, \"select SDPPSU6, WTPFQX6, SDPSTRA6, BMPWTMI, \n                     BMPHTMI, HAB1MI, HSAGEU, HSAGEIR from set1\n                     WHERE HAB1MI &gt; 0 AND BMPWTMI &gt; 0 AND BMPHTMI &gt; 0\")\ndbDisconnect(dbcon)\ndesign &lt;- svydesign(ids = ~SDPPSU6, weights = ~WTPFQX6, strata = ~SDPSTRA6,\n          data = nhanes, nest = TRUE)\ndesign &lt;- update(design,\n                 age = ifelse(HSAGEU == 1, HSAGEIR / 12, HSAGEIR),\n                 bmi =  BMPWTMI / (BMPHTMI / 100)^2,\n                 self_health = factor(HAB1MI))\n\nmodel &lt;- svyolr(self_health ~ age, design = design, subset = age &gt; 20)\nsummary(model)\n\nCall:\nsvyolr(self_health ~ age, design = design, subset = age &gt; 20)\n\nCoefficients:\n         Value  Std. Error  t value\nage 0.02408857 0.001442298 16.70153\n\nIntercepts:\n    Value   Std. Error t value\n1|2 -0.3025  0.0744    -4.0658\n2|3  1.1232  0.0733    15.3167\n3|4  2.7634  0.0791    34.9400\n4|5  4.5148  0.0775    58.2186\n\n\nWee see a similar statistically significant relationship as before but the effect is slightly attenuated.\n\nRepeat the logistic regression analysis in Figure 9.7 using linearization instead of replicate weights.\n\n\nShow the codesqlite &lt;- dbDriver(\"SQLite\")\ndbcon &lt;- dbConnect(sqlite, dbname = \"data/nhanes/imp.db\")\nimpload &lt;- function(varlist, conn) {\n  tables &lt;- paste(\"set\", 1:5, sep = \"\")\n  vars &lt;- paste(varlist, collapse = \",\")\n  query &lt;- paste(\"select\",vars,\"from @tab@\")\n  data &lt;- lapply(tables,\n                 function(table) dbGetQuery(conn, sub(\"@tab@\", table, query)))\n  mitools::imputationList(data)\n}\nregdata &lt;- impload(\n  c(\"SDPPSU6\", \"WTPFQX6\",\"SDPSTRA6\", \"HSAGEU\", \"HSAGEIR\", \"DMARETHN\", \n    \"DMPMETRO\", \"BMPWTMI\", \"BMPHTMI\", \"DMPPIRMI\", \"HAB1MI\", \n    \"HAT28MI\",\"BDPFNDMI\",\n    \"HSSEX\"), dbcon)\ndbDisconnect(dbcon)\n\ndesigns &lt;- svydesign(data = regdata, weights = ~WTPFQX6, \n                     strata = ~SDPSTRA6, id = ~SDPPSU6, nest = TRUE)\ndesigns &lt;- update(designs,\n                  age = ifelse(HSAGEU == 1, HSAGEIR / 12, HSAGEIR))\nadults &lt;- subset(designs, age &gt;= 20)\nadults &lt;- update(adults,\n                 race = factor(ifelse(DMARETHN == 4, 1, DMARETHN)),\n                 bmi =  BMPWTMI / (BMPHTMI / 100)^2,\n                 poverty = DMPPIRMI &lt;= 1,\n                 evgfp = factor(HAB1MI),\n                 activity = factor(HAT28MI, levels = c(3,1,2,-9)),\n                 agegp = cut(age, c(0, 20, 40, 60, Inf), right = FALSE)\n                 )\nadults &lt;- update(adults,\n                 overwt = ifelse(HSSEX == 1, bmi &gt;= 27.8, bmi &gt;= 27.3))\nmodels &lt;- with(adults,\n               svyglm(overwt ~ HSSEX + agegp + race + evgfp + activity + poverty,\n                      family = quasibinomial))\nsummary(MIcombine(models), digits = 3)\n\nMultiple imputation results:\n      with(adults, svyglm(overwt ~ HSSEX + agegp + race + evgfp + activity + \n    poverty, family = quasibinomial))\n      MIcombine.default(models)\n              results     se  (lower upper) missInfo\n(Intercept)   -1.5902 0.1283 -1.8420 -1.338      8 %\nHSSEX          0.0491 0.0557 -0.0601  0.158      4 %\nagegp[40,60)   0.6964 0.0606  0.5774  0.815      8 %\nagegp[60,Inf)  0.6065 0.0721  0.4651  0.748      4 %\nrace2          0.4481 0.0647  0.3212  0.575      2 %\nrace3          0.4103 0.0745  0.2642  0.556      2 %\nevgfp2         0.4281 0.0828  0.2657  0.590      3 %\nevgfp3         0.6852 0.0888  0.5111  0.859      4 %\nevgfp4         0.7669 0.0884  0.5936  0.940      3 %\nevgfp5         0.3757 0.1338  0.1134  0.638      4 %\nactivity1     -0.4014 0.0636 -0.5266 -0.276     13 %\nactivity2      0.2732 0.0702  0.1356  0.411      2 %\npovertyTRUE   -0.0160 0.0703 -0.1547  0.123     15 %\n\n\nThe results are effectively the same as before though the variances — and consequently the missing Information estimates — have changed slightly.\n\nIn the NHANES III imputation data, estimate mean, median, and 90th percentile of systolic blood pressure and total cholesterol broken down by rural/urban location and region of the country.\n\nNot clear to me which variables represent Systolic blood pressure, total cholesterol or the rural/urban location. When I check online documentation for the appropriate variables, I don’t see them in the data set that Lumley posted on his website. Leaving this question blank for now.\n\nFit regression models to examine how self-rated general health (HAB1MI) is related to obesity (bmi as defined in Figure 9.7)\n\n\nUsing just the complete data before imputation.\n\n\nShow the codesqlite &lt;- dbDriver(\"SQLite\")\ndbcon &lt;- dbConnect(sqlite, dbname = \"data/nhanes/imp.db\")\nnhanes &lt;- dbGetQuery(dbcon, \"select SDPPSU6, WTPFQX6, SDPSTRA6,\n                     BMPWTMI, HSAGEU, HSAGEIR, DMARETHN, BMPHTMI, HAB1MI,\n                     DMPMETRO from set1\n                     WHERE HAB1MI &gt;0 AND BMPWTMI &gt; 0 AND BMPHTMI &gt; 0\")\ndbDisconnect(dbcon)\n## I use the weights as is here, but they should be adjusted to account for\n## the missingness. Not clear to me if that's part of the imputed data or not\n## but I'm following Lumley's example.\ndesign &lt;- svydesign(ids = ~SDPPSU6, weights = ~WTPFQX6, strata = ~SDPSTRA6,\n          data = nhanes, nest = TRUE)\ndesign &lt;- update(design,\n                 age = ifelse(HSAGEU == 1, HSAGEIR / 12, HSAGEIR),\n                 bmi =  BMPWTMI / (BMPHTMI / 100)^2,\n                 urban = factor(DMPMETRO),\n                 race = factor(ifelse(DMARETHN == 4, 1, DMARETHN)),\n                 self_health = factor(HAB1MI))\nsummary(svyolr(self_health ~ bmi, design, subset = age &gt; 20))\n\nCall:\nsvyolr(self_health ~ bmi, design, subset = age &gt; 20)\n\nCoefficients:\n         Value  Std. Error  t value\nbmi 0.05506785 0.004228902 13.02179\n\nIntercepts:\n    Value   Std. Error t value\n1|2  0.1008  0.1353     0.7450\n2|3  1.5184  0.1369    11.0941\n3|4  3.1349  0.1441    21.7494\n4|5  4.8627  0.1386    35.0893\n\n\nI model the self reported health outcome as an ordinal outcome, for which we see a positive and significant association with BMI.\nLooking at a plot below this relationship can roughly be seen. We also see some BMI values &gt; 60 that look dubious. I’d probably want to check the imputation model or the data records for these if I were working with this data more seriously.\n\nShow the codesvyplot(self_health ~ bmi, design)\n\n\n\n\n\nCalibrating on age, sex, race, urban / rural location and region of the country.\n\nAgain, I’m not sure about which of the NHANES variables correspond to race, urban / rural location or region of the country. Also not clear how we should get the population data for these values.\n\n\nFor the incomplete data sets defined in Figure 9.1, fit a linear model to predict api00 from the other variables and impute from this regression model by taking the fitted value from the regression and adding a randomly selected residual.\n\nImpute a single complete data set and do the two analyses (mean and regression model) at the end of Figure 9.1. Compare the results to the true population values.\n\n\n\nIts worth noting that adding the right residual involves computing the standard deviation of the estimated variables. I show how to do this below.\n\nShow the codeimputation_model_one &lt;- lm(api00 ~ api99 + emer + stype, data = stratmar)\napi00_imp &lt;- predict(imputation_model_one, newdata = stratmar[is.na(stratmar$api00),])\nX_miss &lt;- model.matrix(~api99 + emer + stype, data = stratmar[is.na(stratmar$api00),])\nresid_std &lt;- diag(X_miss %*% \n                    vcov(imputation_model_one) %*% \n                    t(X_miss))\n\nimputed_apistrat &lt;- stratmar\nimputed_apistrat$api00_imp &lt;- imputed_apistrat$api00\nimputed_apistrat$api00_imp[is.na(imputed_apistrat$api00)] &lt;- api00_imp\nimputed_apistrat$pw &lt;- apistrat$pw\n\ndstrat_mar &lt;- svydesign(id = ~1, strata = ~stype, weights = ~pw,\n                    data = imputed_apistrat, fpc = ~fpc)\nsvymean(~api00_imp, dstrat_mar)\n\n            mean     SE\napi00_imp 664.69 9.6795\n\n\nFor the missing at random data set, the mean estimate is very close to the population value of 664.71. Though it is worth noting that the variance almost certainly underestimates the “true” variance of this estimator.\n\nShow the codeimputation_model_two &lt;- lm(api00 ~ api99 + emer + stype, data = stratnar)\napi00_imp &lt;- predict(imputation_model_two, newdata = stratnar[is.na(stratnar$api00),])\nX_miss &lt;- model.matrix(~api99 + emer + stype, data = stratnar[is.na(stratnar$api00),])\nresid_std &lt;- diag(X_miss %*% \n                    vcov(imputation_model_two) %*% \n                    t(X_miss))\n\nimputed_apistrat &lt;- stratnar\nimputed_apistrat$api00_imp &lt;- imputed_apistrat$api00\nimputed_apistrat$api00_imp[is.na(imputed_apistrat$api00)] &lt;- api00_imp + \n  rnorm(n = length(resid_std), sd = resid_std)\nimputed_apistrat$pw &lt;- apistrat$pw\n\ndstrat_nar &lt;- svydesign(id = ~1, strata = ~stype, weights = ~pw,\n                    data = imputed_apistrat, fpc = ~fpc)\nsvymean(~api00_imp, dstrat_nar)\n\n            mean    SE\napi00_imp 662.38 9.624\n\n\nHere we see a slight increase in bias (downward) from the true population value. Again, the variance is similar to before but certainly underestimated.\n\nCreate 10 multiply imputed data sets and repeat the two analyses. Compare the results to the true population values.\n\n\nShow the codeMI_apistrat &lt;- mitools::imputationList(lapply(1:10, function(ix) {\n  imputed_apistrat &lt;- stratnar\n  imputed_apistrat$api00_imp &lt;- imputed_apistrat$api00\n  imputed_apistrat$api00_imp[is.na(imputed_apistrat$api00)] &lt;- api00_imp + rnorm(n = length(resid_std), sd = resid_std)\n  imputed_apistrat$pw &lt;- apistrat$pw\n  return(imputed_apistrat)\n}))\n\ndstrat_nar_MI &lt;- svydesign(id = ~1, strata = ~stype, weights = ~pw,\n                    data = MI_apistrat, fpc = ~fpc)\n\nsummary(mitools::MIcombine(with(dstrat_nar_MI,\n     svymean(~api00_imp))))\n\nMultiple imputation results:\n      with(dstrat_nar_MI, svymean(~api00_imp))\n      MIcombine.default(with(dstrat_nar_MI, svymean(~api00_imp)))\n          results       se   (lower  upper) missInfo\napi00_imp 662.195 9.591989 643.3949 680.995      1 %\n\n\nFor the “Not Missing at Random” data set, we see a similar mean and standard error as before, which implies that the missing mechanism must not have been particularly impactful as determined by the imputed datasets.\n\nShow the codeapi00_imp &lt;- predict(imputation_model_one, newdata = stratmar[is.na(stratmar$api00),])\nX_miss &lt;- model.matrix(~api99 + emer + stype, data = stratmar[is.na(stratmar$api00),])\nresid_std &lt;- diag(X_miss %*% \n                    vcov(imputation_model_one) %*% \n                    t(X_miss))\nMI_apistrat &lt;- mitools::imputationList(lapply(1:10, function(ix) {\n  imputed_apistrat &lt;- stratmar\n  imputed_apistrat$api00_imp &lt;- imputed_apistrat$api00\n  imputed_apistrat$api00_imp[is.na(imputed_apistrat$api00)] &lt;- api00_imp + rnorm(n = length(resid_std), sd = resid_std)\n  imputed_apistrat$pw &lt;- apistrat$pw\n  return(imputed_apistrat)\n}))\n\ndstrat_mar_MI &lt;- svydesign(id = ~1, strata = ~stype, weights = ~pw,\n                    data = MI_apistrat, fpc = ~fpc)\n\nsummary(mitools::MIcombine(with(dstrat_mar_MI,\n     svymean(~api00_imp))))\n\nMultiple imputation results:\n      with(dstrat_mar_MI, svymean(~api00_imp))\n      MIcombine.default(with(dstrat_mar_MI, svymean(~api00_imp)))\n           results       se   (lower  upper) missInfo\napi00_imp 664.4089 9.760303 645.2788 683.539      1 %\n\n\nWe see a similar result as above, though again the mean estimate is slightly closer to the true population value."
  },
  {
    "objectID": "ComplexSurveyNotes.html#iptw-estimators",
    "href": "ComplexSurveyNotes.html#iptw-estimators",
    "title": "Complex Survey Notes",
    "section": "IPTW Estimators",
    "text": "IPTW Estimators\n\nThe Inverse Probability of Treatment Weighted (IPTW) estimator is the analog for potential-outcomes inference of the Horvitz-Thompson estimator.\n\nRandomized trials and calibration\nSimilar to how a designed sample has known sampling probabilities a randomized trial does as well. Typically \\pi_{i} = \\frac{1}{2} for assignment for each individual to the treatment or control group. In this case, no weighting is needed at all in the estimation, since the weights are the same for each individual.\nLumley then goes on to give a brief discussion of the relationship between calibration and regression when used in a randomized trial with some covariate X that is meaningfully associated with the outcome. A few brief notes here to summarize:\n\nFor a difference in means, the calibration and linear estimates are the same.\nWhen estimating a hazard, log-odds, or rate ratios they are not\n\nThe calibration estimator is still equivalent to the un-adjusted estimator.\nThe regression estimator is now different. I believe it would be a conditional average treatment effect estimate.\nLumley shows with simulation that the calibration estimate has higher precision then the un-adjusted estimate, and equivalently high precision to the adjusted.\n\n\n\nPersonally, I’d keep in mind that the conditional/adjusted estimate may be more interpretable than the un-adjusted - though the un-adjusted is often what is sought after by a journal, for example. Deciding which is more useful is a philosophical discussion, highly dependent on the question being asked.\nEstimated Weights for IPTW\nAnalogous to how weights (or values) had to be estimated for missing data in the previous chapter, sampling weights have to be estimated for observational data when examining the impact of an exposure on an outcome of interest.\nLumley uses an example from a paper (Tager et al. 1979) examining the impact of smoking habits on pulmonary function amongst a sample of 654 childrenn from Boston in the 1970’s. I can’t find the data accessible through Lumley’s book website but I did find the data set via the mplot package. However, this dataset does not have the siblings or family variables that Lumley uses in his analysis to adjust for the cluster sampling and family size. I proceed assuming a simple random sample without any cluster estimates.\nLumley plots lung function (forced expiratory volume or FEV) against age and childrens’ smoking status and identifies that although there’s a naive difference between smoking and non smoking children confounded by the fact that older children — who have higher FEV by virtue of age — are more likely to smoke than those who aren’t.\n\nShow the codefev &lt;- mplot::fev\nt.test(fev ~ smoke, data = fev)\n\n\n    Welch Two Sample t-test\n\ndata:  fev by smoke\nt = -7.1496, df = 83.273, p-value = 3.074e-10\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.9084253 -0.5130126\nsample estimates:\nmean in group 0 mean in group 1 \n       2.566143        3.276862 \n\n\n\nShow the codeboxplot(fev ~ smoke, data = fev, horizontal = TRUE,\n        xlab = \"FEV (L/S)\", \n        main = \"Forced Expiratory Volume and Smoking Status for 654 children\")\n\n\n\n\n\nShow the codeas_tibble(fev) %&gt;% \n  mutate(smoke = factor(smoke, labels = c(\"Non-Smoking\",\n                                          \"Smoking\"))) %&gt;% \n  ggplot(aes(x = age, y = fev, color = smoke)) +\n  geom_point() + \n  geom_jitter() +\n  theme(legend.title = element_blank())\n\n\n\n\nAn IPTW analysis proceeds by estimating the probability of exposure — smoking in this case — as a function of other covariates. I fit three models below following Lumley’s example as close as I can and setting up design objects with sampling probabilities equal to the estimated probability of smoking or not smoking according to the model and the observed smoking status.\nIt is worth highlighting here again, the analogy between sampling from a finite population of fixed individuals used in previous chapters, and how that corresponds to our example here, where we’re hypothetically sampling from a population according to smoking status and using that to estimate the impact of smoking on FEV.\nA further point, that I brought up in the previous chapter, is that we’re not incorporating any of our uncertainty about the probability of smoking into the model estimating the effect of smoking on FEV. Consequently the variance in the reported models is under-estimated.\n\nShow the codeolderkids &lt;- subset(fev, age &gt; 12)\nm1 &lt;- glm(smoke ~ age*sex, data = olderkids, family = binomial())\nm2 &lt;- glm(smoke ~ height*sex, data = olderkids, family = binomial())\nm3 &lt;- glm(smoke ~ age*sex + height*sex, data = olderkids, family = binomial())\nolderkids$pi1 &lt;- ifelse(olderkids$smoke == 1, fitted(m1), 1 - fitted(m1))\nolderkids$pi2 &lt;- ifelse(olderkids$smoke == 1, fitted(m2), 1 - fitted(m2))\nolderkids$pi3 &lt;- ifelse(olderkids$smoke == 1, fitted(m3), 1 - fitted(m3))\n\nd1 &lt;- svydesign(id = ~1, prob = ~pi1, data = olderkids)\nd2 &lt;- svydesign(id = ~1, prob = ~pi2, data = olderkids)\nd3 &lt;- svydesign(id = ~1, prob = ~pi3, data = olderkids)\nfit1 &lt;- svyglm(fev~smoke, design = d1)\nfit2 &lt;- svyglm(fev~smoke, design = d2)\nfit3 &lt;- svyglm(fev~smoke, design = d3)\n\ntbl_merge(\n  tbls = list(\n    tbl_regression(fit1),\n    tbl_regression(fit2),\n    tbl_regression(fit3)\n  ),\n  tab_spanner = c(\"**Model 1**\", \"**Model 2**\",\"**Model 3**\")\n)\n\n\n\n\n\n\n\n\nCharacteristic\n\n      \n        \nModel 1\n\n      \n      \n        \nModel 2\n\n      \n      \n        \nModel 3\n\n      \n    \n\n\nBeta\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \nBeta\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \nBeta\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n    \n\n\nsmoke\n-0.14\n-0.47, 0.20\n0.4\n-0.10\n-0.41, 0.21\n0.5\n-0.10\n-0.42, 0.23\n0.6\n\n\n\n1 \nCI = Confidence Interval\n\n\n    \n\n\n\n\nIn the table above, we now see that mean FEV value is now negatively associated with smoking, but none of the estimates are statistically significant.\nLumley shows that there are similar results for an un-weighted analysis and argues that the IPTW analysis is valuable for providing some justification for believing this effect more generally.\nDouble robustness\nLumley brings up the idea of a double robust estimator again, this time showing the estimates from a double robust estimation analysis whereby the covariates used in the probability or propensity model are included in the second stage model as well. This allows us to include even younger children in the final analysis and derive a similar conclusion as when they were excluded.\n\nShow the codeolderkids &lt;- subset(fev, age &gt; 10)\nm1 &lt;- glm(smoke ~ age*sex, data = olderkids, family = binomial())\nm2 &lt;- glm(smoke ~ height*sex, data = olderkids, family = binomial())\nm3 &lt;- glm(smoke ~ age*sex + height*sex, data = olderkids, family = binomial())\nolderkids$pi1 &lt;- ifelse(olderkids$smoke == 1, fitted(m1), 1 - fitted(m1))\nolderkids$pi2 &lt;- ifelse(olderkids$smoke == 1, fitted(m2), 1 - fitted(m2))\nolderkids$pi3 &lt;- ifelse(olderkids$smoke == 1, fitted(m3), 1 - fitted(m3))\nfit1 &lt;- svyglm(fev~smoke + height*sex + age*sex, design = d1)\nfit2 &lt;- svyglm(fev~smoke + height*sex + age*sex, design = d2)\nfit3 &lt;- svyglm(fev~smoke + height*sex + age*sex, design = d3)\n\ntbl_merge(\n  tbls = list(\n    tbl_regression(fit1),\n    tbl_regression(fit2),\n    tbl_regression(fit3)\n  ),\n  tab_spanner = c(\"**DR Model 1**\", \"**DR Model 2**\",\"**DR Model 3**\")\n)\n\n\n\n\n\n\n\n\nCharacteristic\n\n      \n        \nDR Model 1\n\n      \n      \n        \nDR Model 2\n\n      \n      \n        \nDR Model 3\n\n      \n    \n\n\nBeta\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \nBeta\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n      \nBeta\n\n      \n\n95% CI\n\n1\n\n      \np-value\n\n    \n\n\n\nsmoke\n-0.08\n-0.29, 0.12\n0.4\n-0.10\n-0.30, 0.10\n0.3\n-0.08\n-0.29, 0.12\n0.4\n\n\nheight\n0.03\n-0.03, 0.08\n0.3\n0.03\n-0.02, 0.09\n0.3\n0.03\n-0.03, 0.08\n0.3\n\n\nsex\n-7.5\n-13, -1.5\n0.015\n-7.3\n-13, -2.1\n0.007\n-7.1\n-13, -1.5\n0.014\n\n\nage\n0.01\n-0.05, 0.07\n0.8\n0.01\n-0.05, 0.07\n0.7\n0.01\n-0.05, 0.07\n0.8\n\n\nheight * sex\n0.12\n0.03, 0.20\n0.008\n0.11\n0.04, 0.19\n0.004\n0.11\n0.03, 0.19\n0.008\n\n\nsex * age\n0.03\n-0.08, 0.14\n0.6\n0.03\n-0.08, 0.14\n0.6\n0.03\n-0.08, 0.14\n0.6\n\n\n\n\n1 \nCI = Confidence Interval"
  },
  {
    "objectID": "ComplexSurveyNotes.html#marginal-structural-models",
    "href": "ComplexSurveyNotes.html#marginal-structural-models",
    "title": "Complex Survey Notes",
    "section": "Marginal Structural Models",
    "text": "Marginal Structural Models\nLumley begins this section with an important point:\n\nFor estimating effects of a single binary exposure there is not much to choose between IPTW estimators and regression estimators. Both will be valid if there are no unmeasured confounders, and any measured confounder can equally well be used in adjustment, in reweighting, or both.\n\nHowever, when considering repeated measures over time, IPTW estimators have a significant advantage in modeling the relationship between outcome and exposure more explicitly. After all, exposure measured at time point 1 may well impact the outcome at all subsequent time points.\nThankfully, using the IPTW approach, we can proceed similarly as before, estimating the potential outcome for each possible exposure pathway. The propensity scores or probabilities can then be multiplied to give the potential outcomes sequence and the sampling probability used will be for the sequence observed.\nExample: maternal stress and children’s illness\nI obtained the mscm dataset from Lumley’s website, who in turn obtained it from Patrick Heagerty’s website which includes documentation I would encourage looking over.\nThis data set comes from the The Mothers’ and Children’s Morbidity Study which examined how maternal stress might impact childhood illness. Stress and illness were measured for 30 days on 167 mother-child pairs on a 0-1 scale.\nI reproduce Lumley’s analysis in the code below examining the effect of a mother’s stress on her child’s illness.\nLumley begins by filtering the data to only include mother-child pairs with 28 non-missing observations and ~ 3 weeks of data.\nI try to reproduce Lumley’s plots below but I think he may have produced them before filtering or with alternative filtering in place, as we see a similar trend in both plots but different absolute estimates.\n\nShow the codemscm &lt;- read_table(\"data/mscm.txt\",col_names = c(\"id\",\"day\",\"stress\", paste(\"s\",1:6,sep=\"\"),\n                 \"illness\", paste(\"i\",1:6,sep=\"\"),\n                 \"married\",\"education\",\"employed\",\"chlth\",\"mhlth\",\"race\",\n                 \"csex\",\"hsize\",\"wk1illness\",\"wk1stress\"))\nmscm$nna &lt;- with(mscm, ave(stress + illness, id, FUN = function(x) sum(!is.na(x))))\nmscm &lt;- subset(mscm, nna == 28 & day &gt; 6 & day &lt; 29)\n\n\n\nShow the code# I try to reproduce Figure 10.5 here. While it's not the same as Lumley's\n# plot, we do see a similar trend.\nunwt &lt;- svydesign(id = ~id, data = mscm)\neda_fit &lt;- svyglm(i3 ~ s1 + s2 + s3 + s4 + s5 + s6 - 1, family = quasibinomial,\n               design = unwt)\nbroom::tidy(eda_fit, conf.int = TRUE) %&gt;% \n  mutate(time_offset = as.integer(str_replace(term,\"s\",\"\")) - 3) %&gt;% \n  ggplot(aes(x = time_offset, y = estimate)) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) + \n  xlab(\"Time Offset\") +\n  ylab(\"Log Odds Ratio\") + \n  geom_vline(aes(xintercept = 0), linetype = 2, color = 'red') +\n  ggtitle(\"Childhood Illness and Mother's Stress\",\n          subtitle = \"Log Odds Ratio of Illness vs. Stress Over Time\")\n\n\n\n\n\nShow the code# I try to reproduce Figure 10.6 here. Again, this isn't the same as Lumley's\n# plot but we see a similar trend. I'm guessing he might not've filtered \nunwt &lt;- svydesign(id = ~id, data = mscm)\nill_eda &lt;- svyglm(illness ~ i1 + i2 + i3 + i4 + i5 + i6 - 1, family = quasibinomial,\n               design = unwt)\nstress_eda &lt;- svyglm(stress ~ s1 + s2 + s3 + s4 + s5 + s6 - 1, family = quasibinomial,\n               design = unwt)\nbroom::tidy(ill_eda, conf.int = TRUE) %&gt;% \n  mutate(model = \"illness\") %&gt;% \n  rbind(.,tidy(stress_eda, conf.int = TRUE) %&gt;% \n          mutate(model = \"stress\")) %&gt;% \n  mutate(time_lag = as.integer(str_replace(term,\"i|s\",\"\"))) %&gt;% \n  ggplot(aes(x = time_lag, y = estimate, color = model)) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) + \n  geom_line() +\n  xlab(\"Time Lag\") +\n  ylab(\"Log Odds Ratio\") + \n  ggtitle(\"Log Odds Ratios of Childhood Illness and Mother's Stress \n          for different time lags.\") +\n  theme(legend.title = element_blank())\n\n\n\n\nTo analyze the data, Lumley produces stabilized weights by averaging two model’s respective probabilities of stress. A model (1) that includes the child’s current and previous state of illness (which might cause subsequent stress on the mother’s part), as well as the current day in the study and baselines stress and a second (2) that looks only at the baseline stress and change in mother’s stress over time.\nWhen Lumley talks about separating the exposure model from the outcome without having to worry about self-caused confounding — child’s illness inducing subsequent maternal stress — this is what he means. Using both probabilities increases the variability of the weights without inducing any potential confounding.\nThe two design objects include id as the sampling unit to account for within-person correlation. As a side note, I’m wondering how this approach compares to, say, a GEE or linear mixed effects model’s adjustment for within person correlation in measurement. Since we end up fitting regression models that are effectively GEE models I’m guessing those are roughly equivalent.\n\nShow the codemodel &lt;- glm(stress ~ illness + i1 + s1 + day + wk1stress, family = binomial(),\n             data = mscm, na.action = na.exclude)\nmscm$pstress &lt;- fitted(model)\nbasemodel &lt;- glm(stress ~ day + wk1stress, family = binomial, \n                 data = mscm, na.action = na.exclude)\nmscm$pbstress &lt;- fitted(basemodel)\nmscm$swit &lt;- with(mscm, ifelse(stress == 1, pbstress / pstress,\n                               ( 1 - pbstress) / (1-pstress)))\nmscm$swi &lt;- with(mscm, ave(swit, id, FUN = prod))\n\ndes &lt;- svydesign(id = ~id, weights = ~swi, data = mscm)\nunwt &lt;- svydesign(id = ~id, data = mscm)\n\n\nLumley then fits three outcome models. One uses the stabilized sampling weights, one uses no weights, and then a third uses no weights, but includes the baseline variable adjustment. The estimates and standard errors are in the table below.\n\nShow the codem_iptw &lt;- svyglm(illness ~ s1 + s2 + s3 + s4, design = des,\n                 family = quasibinomial)\nm_raw &lt;- svyglm(illness ~ s1 + s2 + s3 + s4, des = unwt,\n                family = quasibinomial)\n\nm_gee &lt;- svyglm(illness ~ s1+s2+s3+s4+wk1stress+wk1illness, des = unwt,\n                family = quasibinomial)\n\npred_data &lt;- data.frame(s1 = 0:1, s2 = 0:1, s3 = 0:1, s4 = 0:1,\n                        wk1stress = 0.16, wk1illness = 0.16)\n\npred_iptw &lt;- predict(m_iptw, pred_data, vcov = TRUE, type = \"response\")\npred_raw &lt;- predict(m_raw, pred_data, vcov = TRUE, type = \"response\")\npred_gee &lt;- predict(m_gee, pred_data, vcov = TRUE, type = \"response\")\n\n# Here Lumley's looking at the difference in log odds of a child being ill \n# if its mother was stressed the previous four days and had (I believe) average\n# baseline stress and illness, and if the mother did *not* stress the previous\n# four days, but still had the same baseline variables. i.e. Is the mother's\n# stress associated with a change in the odds of her child's illness.\nc_iptw &lt;- svycontrast(pred_iptw, c(-1, 1))\nc_raw &lt;- svycontrast(pred_raw, c(-1, 1))\nc_gee &lt;- svycontrast(pred_gee, c(-1, 1))\ntibble(model = c(\"IPTW\", \"Unadjusted\", \"Baseline Adjusted\"),\n       estimate = c(c_iptw, c_raw, c_gee),\n       std_err = c(attr(c_iptw,\"var\"), attr(c_raw,\"var\"),\n                   attr(c_gee,\"var\"))) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(decimals = 3) %&gt;% \n  gt::tab_header(\"Estimate of Childhood Illness from Mother's Stress\")\n\n\n\n\n\n\n\nEstimate of Childhood Illness from Mother's Stress\n    \n\nmodel\n      estimate\n      std_err\n    \n\n\n\nIPTW\n0.095\n0.015\n\n\nUnadjusted\n0.216\n0.009\n\n\nBaseline Adjusted\n0.159\n0.007\n\n\n\n\n\n\nLumley uses this data to demonstrate how the IPTW estimator is useful for adjusting for all variability in the exposure – mother’s stress — that might lead to the child’s illness.\nUnfortunately with such a small data set and a likely small effect size, we’re not able to determine that this result is statistically different from zero. Lumley notes that this is often the case for methods like these, as removing so much confounding from observational data results in very small subsequent effect typically.\nThis method is fairly sophisticated. Lumley cites the following papers that analyzed this data and I’d also encourage the interested reader to check out Hernan and Robins book on Causal Inference (Hernán and Robins 2010)."
  },
  {
    "objectID": "ComplexSurveyNotes.html#exercises-9",
    "href": "ComplexSurveyNotes.html#exercises-9",
    "title": "Complex Survey Notes",
    "section": "Exercises",
    "text": "Exercises\nThere are no exercises for this chapter."
  }
]